---
title: "Density Estimation"
author: 
- "Michaela Lukacova (dns525)"
institute: "University of Copenhagen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: themer-new.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content > h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
</style>

```{r, include=FALSE, eval = FALSE}
#rmarkdown::render('presentation.rmd')
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(CSwR)
style_mono_accent(
 base_color = "#4f7942")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(profvis)
library(knitr)
library(bench)
library(ggplot2)
library(tidyverse)
library(testthat)
library(Rcpp)

theme_set(theme_bw())
knitr::opts_chunk$set(fig.retina = 2)
theme_set(
  theme_bw(base_size = 18) +  # Set base font size and family
    theme(
      text = element_text(size = 15),           # Adjust text size
    )
)
```

```{r source, include=FALSE}
source("~/R/comp_stat/Smoothing_ML/AMISE_bw.R")
source("~/R/comp_stat/Smoothing_ML/cv_bw_l.R")
source("~/R/comp_stat/Smoothing_ML/cv_bw_l_fast.R")
source("~/R/comp_stat/Smoothing_ML/density_object.R")
source("~/R/comp_stat/Smoothing_ML/kern_bin.R")
source("~/R/comp_stat/Smoothing_ML/kern_dens_bin.R")
source("~/R/comp_stat/Smoothing_ML/naive_kern_dens.R")
source("~/R/comp_stat/Smoothing_ML/mse_dens.R")
#Rcpp::sourceCpp("~/R/comp_stat/Smoothing_ML/rcpp_kern_bin.cpp")
```

### Kernel Density Estimation

The kernel density estimators approximate an unknown density of data points with the function

$$ \hat{f}(x) = \frac{1}{hN} \sum_{j=1}^N K\left(\frac{x-x_j}{h}\right)$$

For a given kernel $K: \mathbb{R} \rightarrow \mathbb{R}$ and bandwidth parameter $h$.

In our implementations we will use the Epanechnikov kernel. 

$$K(x)=\frac{3}{4}\left(1-x^2\right) 1_{[-1,1]}(x)$$
---
### Naive density estimator

We implement a naive density estimator called `kern_dens1`, the function takes a vector of data points `x`, a bandwidth parameter `h`, a number of gridpoints `m` and a boolean `norm` to normalize the density estimate. The function returns a list with the gridpoints `x` and the density estimate `y`. The `norm`argument is used to normalize the density estimate, so that the gridpoints are equal to r's density function for comparison

```{r}
kern_dens1 <- function(x, h, m = 512, norm = TRUE) {
  
  rg <- range(x)
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)
  
  # we normalize now, so that the gridpoints are equal to r's density function
  if(norm){
    h <- sqrt(5) * h
  }
  
  for (i in seq_along(xx)) {
    
    condition <- abs(xx[i] - x) <= h
    y[i] <- sum((1 - (xx[i] - x)^2 / h^2) * condition)
    
  }
  
  y <- 3 * y / (4 * h * length(x))
  list(x = xx, y = y)
  
}
```

Note that we have already optimized the code a bit in this naive implementation by vectorizing and thinking about performing the operations in the smartest order possible. 

---
### Simulation of data

To test our naive density estimator, we simulate data by drawing 2 times $500$ points from a normal distribution with respectively mean -3 and mean 3, and standard deviation 2. This way we obtain a bimodal distribution. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
# Simulate data
n <- 500
set.seed(123)
x <- c(rnorm(n, mean = -3, sd = 2), rnorm(n, mean = 3, sd = 2))
x_dens <- function(x) 1/2 * dnorm(x, -3, 2) + 1/2 * dnorm(x,3,2)

p <- ggplot(tibble(x = x), aes(x = x)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.3, aes(x = x, y = ..density..)) +
  geom_function(fun = x_dens, alpha = 0.5, aes(color = "True Dens"))+
  scale_color_manual(values = c("True Dens" = "blue", 
                                "CV" = "hotpink", 
                                "AMISE" = "green3"))+
  labs(color = "Density types", title = "Simulated data", x = "x", y = "Density")

p
```

---
### S3 class

We have implemented an S3 class `density_object` to store the data and bandwidth parameters.

```{r}
test_obj <- my_density(x)
```

The object has a `plot` method which we use to check how the naive kernel density estimator approximates the simulated data.

```{r, fig.width=10, fig.height=4, fig.align='center'}
plot(test_obj, h = 0.5)
```
Looks pretty good. 

---
### Check of naive density estimator

We compare the results to the results obtained by using R's density function. This we can do by setting the `p` argument in the `plot` method to $2$. 

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
plot(test_obj, h = 0.5, p = 2)
```

They are practically the same!

---
### Check of naive density estimator

We further test that the estimates are correct by using the `test_dens` method. This method tests that the absolute distance between the R `density` function and our own function is less than $10^{-3}$ for 4 different $h$ values equally ditanced in the interval from $0.2$ to $1$. 

```{r}
test_dens(test_obj)
```

---
### Binned density estimator

After profiling and benchmarking the naive density function, we see that it is quite a lot slower than the `density` function in R. We implement a binned density estimator `kern_dens_bin` to speed up the density estimation. The binned density estimator is an alteration of the previous density estimator. It has the advantage of computational efficiency. The binned density estimator is computed as 

$$
\hat{f}(x) = \frac{1}{hN} \sum_{i=1}^B n_j K\left(\frac{x-c_j}{h}\right)
$$
The function bins the observations into $B$ equal length bins spanning the length of the $x$ observations. The number of observations in each bin is $n_j$. $c_j$ is the center of each bin. The computational efficiency is achieved by, instead of computing $K$ in each data point, we only compute $K$ in each centerpoint, and then multiply by the number of observations in that bin. The concern could be that we lose accuracy, but as we'll see, this is not a problem. 

```{r}
kern_dens_bin <- function(x, h, m = 512, B = 100, norm = TRUE, bin = kern_bin) {
  rg <- range(x)
  delta <- (rg[2] - rg[1]) / (B - 1)
  c <- seq(rg[1] + delta/2, rg[2] - delta/2, length.out = B)
  n <- bin(x, rg[1], rg[2], B)
  
  xx <- seq(rg[1] - 3 * h, rg[2] + 3 * h, length.out = m)
  y <- numeric(m)
  
  # we normalize now, so that the gridpoints are equal to r's density function
  if(norm){
    h <- sqrt(5) * h
  }
  
  for (i in seq_along(xx)) {
    condition <- abs(xx[i] - c)  <=  h
    y[i] <- sum(n * (1 - (xx[i] - c)^2 / h^2) * condition)
  }
  
  y <- 3 * y / (4 * h)
  list(x = xx, y = y)
}
```

---
### Check of binned density estimator

We compare the results to the results obtained by using R's density function. 

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
plot(test_obj, h = 0.5, binned = TRUE, B = 100)
```

The estimates are very close to beeing the same as when using R's density estimator. 

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
test_dens(test_obj, binned = TRUE, level = 1e-2)

plot_diff <- tibble(x = plot_data(test_obj, h = 0.5, binned = TRUE)$x,
                    "Diffs bin" = plot_data(test_obj, h = 0.5, binned = TRUE)$y - 
                      density(test_obj$x, kernel = "epanechnikov", 0.5)$y,
                    "Diffs naive" = plot_data(test_obj, h = 0.5)$y - 
                      density(test_obj$x, kernel = "epanechnikov", 0.5)$y)

ggplot(data = plot_diff, aes(x = x)) + 
  geom_line(aes(y = `Diffs bin`, color = "Binned")) +
  geom_line(aes(y = `Diffs naive`, color = "Naive")) +
  labs(title = "Differences between R's density function and our own", x = "x", y = "Differences") +
  scale_color_manual(values = c("Binned" = "purple", "Naive" = "hotpink"))
```
---
### Profiling

By profiling the `kern_dens_bin` function we see that the function is quite fast. All the time is spent in the `kern_bin` function, which is a function that computes the number of points in each bin. We implement this function in RCPP to speed up the computation.

```{r, message=FALSE, warning=FALSE, echo = FALSE}
y <- rnorm(10^8)
profvis::profvis({
  kern_dens_bin(y, 0.5)
})
```

---
### Optimizing

We vectorize the `kern_bin` function

```{r}
kern_bin_fast <- function(x, l, u, B) {
  delta <- (u - l) / (B - 1)
  is <- floor((x - l) / delta + 0.5) + 1
  w <- tabulate(is)
  w/sum(w)
}
```


And test that the two functions in fact compute the same

```{r}
sum(kern_bin(x, range(x)[1], range(x)[2], 100) != kern_bin_fast(x, range(x)[1], range(x)[2], 100)) %>% kable()
```

---
### Benchmarking 

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
# Generate random numbers
set.seed(19)
y <- rnorm(2^12)

dens_bench <- bench::press(
  k = 2^(7:12),
  {
    bench::mark(
      "Vectorized binned density" = kern_dens_bin(x = y[1:k], h = 0.2, bin = kern_bin_fast),
      "Binned density" = kern_dens_bin(y[1:k], 0.2),
      "Naive" = kern_dens1(y[1:k], 0.2),
      check = FALSE
    )
  }
)

dens_bench %>% 
  mutate(expr = as.character(expression), median = as.numeric(median)) %>% 
  ggplot(aes(k, median, color = expr)) + geom_point() + 
  scale_y_log10() +
  geom_line() + labs(y = "time (ms)")
```
Large improvements by using the binned density estimator, and a small improvement vectorizing it. 

---
### Now to a different question: How to select bandwidth?

We will explore two different approaches to bandwidth selection.
- AMISE plugin
- Cross-validation

The AMISE plugin method is based on minimizing the asymptotic mean integrated squared error. And can be seen as finding a suitable trade off between the integrated variance of our density estimator and the bias of the density estimator. In order for the method to be valid, we need to assumme $||f''_0||_2^2$ is finite.

The cross validation method does not rely on asymptotic results, but chooses the optimal bandwidth by minimizing a certain cross validation criterion. We will use the UCV criterion.

---
### Bandwidth selection: AMISE plugin

The Epanechnikov kernel is a square-integrable probability density with mean 0. It can be shown that for the Epanechnikov kernel:

$$
\sigma_K^2 = \frac{1}{5} \\
||K||_2^2 = \frac{3}{5}
$$

The AMISE is defined as

$$
AMISE(h) = \frac{||K||_2^2}{nh} + \frac{h^4 \sigma_{K}^4 ||f_0''||_2^2}{4} 
$$
$||f''_0||_2^2$ is the squared $L_2$-norm of the true unknown density. 
---
### Bandwidth selection: AMISE plugin

By minimizing $AMISE(h)$ the asymptotically optimal oracle bandwidth is 

$$
h_N = \left(\frac{||K||_2^2 }{||f''_0||_2^2 \sigma_K^4} \right)^{1/5}n^{-1/5}
$$

Inserting the values we have for the Epanechnikov kernel

$$
h_N = \left(\frac{15 }{||f''_0||_2^2 } \right)^{1/5}n^{-1/5}
$$
We have now arrived at a circular problem. In order to select bandwidth we need to know $f$, but to estimate $f$ we need the bandwidth. A solution to this problem is to estimate $h$ according to Silverman’s rule of thumb, obtain a pilot density estimate $\tilde{f}$ and compute the squared $L_2$-norm, use this estimate to find $h_N$, in order to arrive at our final kernel density estimate. 

---
### Bandwidth selection: AMISE plugin

Silverman’s rule of thumb for the Epanechnikov kernel is

$$
\hat{h}_n = (40\sqrt{\pi})^{1/5} \tilde{\sigma}n^{-1/5}
$$

Where $\tilde{\sigma} = \min\{\hat{\sigma}, \frac{\operatorname{IQR}}{1.34}\}$. 

For the Epanechnikov kernel ($H$) with bandwidth $h$ we can compute the squared $L_2$-norm as

$$
||\tilde{f}||^2_2 = \frac{1}{n^2 h^6} \sum_{i=1}^N\sum_{j=1}^N\int H''\left(\frac{x-x_i}{h}\right) H''\left(\frac{x-x_j}{h}\right) dx
$$
### Bandwidth selection: AMISE plugin

We have

$$
H''(x) = -\frac{3}{2} 1_{[-1,1]}(x)
$$
Note

$$
\left(\frac{x-x_i}{h}\right) \in [-1,1] \Leftrightarrow x \in [x_i-r,x_i+r]
$$

So
$$
||\tilde{f}||^2_2 = \frac{9}{4n^2 h^6} \sum_{i=1}^N\sum_{j=1}^N \int_{\max\{x_i - h, x_j -h\}}^{\min\{x_i + h, x_j + h\}} 1 dx = \frac{9}{4n^2 h^6} \sum_{i=1}^N\sum_{j=1}^N  \left(\max\{0,2h - |x_i-x_j|\} \right)
$$

The AMISE bandwidth based selection is implemented in the function `AMISE_bw`. And for our `density_object`, the AMISE_bw is:

```{r, echo = FALSE}
test_obj$amise_bw %>% kable()
```

---
### Optimal bandwidth by cross-validation 

An alternative to finding the optimal bandwidth by the AMISE plugin method is to use cross validation to find the optimal bandwidth. We will use a log likelihood based cross validation criterion, namely

$$
\mathcal{l}_{CV}(h) = \sum_{i=1}^N \log(\hat{f}_h^{-i}(x_i)) 
$$
Here $\hat{f}_h^{-i}$ is the kernel density estimate obtained by leaving out the group containing $x_i$ evaluated in $x_i$, that is
$$
\hat{f}_h^{-i}(x_i) = \frac{1}{hN_i} \sum_{j \in I^{-1}} K\left(\frac{x_i -x_j}{h}\right) = \frac{3}{4hN_i} \sum_{j \in I^{-1}} \left( 1-\left(\frac{x_i -x_j}{h}\right)^2\right) 1_{[-h,h]}(x_i - x_j)
$$
By maximizing this quantity, we can obtain a bandwidth estimate. We first implemented a naive method that for a sequence of bandwidths $(0.01,0.02,...,2)$ computes the log likelihood and picks the smallest bandwidth. We use 5 fold cross validation. This method is quite slow since the log likelihood needs to be computed for every data point for every bandwidth. 

```{r}
profvis::profvis({
  cv_bw_l(x)
})
```

---
### Optimizing the cross validation bandwidth selection

We see that this method takes up a lot of time, and the time is spent in the for loop. We see that the term $(x_i - x_j)^2$ is being computed over and over for each $h$. We optimize the cross validation band width selection algorithm such that this term is computed once, and then used for all $h$. We implement this in the function `cv_bw_l_fast`. 

Parallelizing and rcpp
```{r}

```


---
### Benchmarking 

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
# Generate random numbers
set.seed(19)
y <- rnorm(2^12)

dens_bench <- bench::press(
  k = 2^(7:12),
  {
    bench::mark(
      "Naive" = kern_dens_bin(x = y[1:k], h = 0.2, bin = kern_bin_rcpp),
      "Optimized" = kern_dens_bin(y[1:k], 0.2),
      "Parallelized" = kern_dens1(y[1:k], 0.2),
      check = FALSE
    )
  }
)

dens_bench %>% 
  mutate(expr = as.character(expression), median = as.numeric(median)) %>% 
  ggplot(aes(k, median, color = expr)) + geom_point() + 
  scale_y_log10() +
  geom_line() + labs(y = "time (ms)")
```

---
### Benchmarking bandwidth selection


---
### Comparing the two bandwidths

Our crossvalidation optimal bandwidth:

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
test_obj$cv_bw %>% kable()
test_obj$amise_bw %>% kable()
```

Plotting the two

```{r, message=FALSE, warning=FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
p +
  geom_line(data = plot_data(test_obj, h = test_obj$cv_bw, binned = TRUE),
            aes(x = x, y = y, color = "CV"), linetype = "dashed") +
  geom_line(data = plot_data(test_obj, h = test_obj$amise_bw, binned = TRUE),
            aes(x = x, y = y, color = "AMISE"), linetype = "dashed")
  
```
The kernel density with the AMISE bandwidth resembles the true density more. 

---
### Test of implementation using real data

We test our implementation on the faithful dataset. We estimate the waiting time to next eruption (in mins) of the Old Faithful geyser in Yellowstone National Park. 

```{r}
real_obj <- my_density(faithful$waiting)

tibble("CV bw" = real_obj$cv_bw, "AMISE bw" = real_obj$amise_bw) %>% kable()
```

Very different optimal bandwidths. 

```{r}
ggplot(faithful, aes(x = waiting)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.3, aes(x = waiting, y = ..density..)) +
  geom_line(data = plot_data(real_obj, h = real_obj$amise_bw),
            aes(x = x, y = y, color = "Naive"), linetype = "dashed") +
    geom_line(data = plot_data(real_obj, h = real_obj$amise_bw, binned = TRUE, B = 20),
            aes(x = x, y = y, color = "Binned"), linetype = "dashed") +
  scale_color_manual(values = c("Binned" = "blue", 
                                "Naive" = "hotpink"))+
  labs(color = "Density types", title = "Geyser data: AMISE BW", x = "Waiting time", y = "Density")

ggplot(faithful, aes(x = waiting)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.3, aes(x = waiting, y = ..density..)) +
  geom_line(data = plot_data(real_obj, h = real_obj$cv_bw),
            aes(x = x, y = y, color = "Naive"), linetype = "dashed") +
  geom_line(data = plot_data(real_obj, h = real_obj$cv_bw, binned = TRUE, B = 20),
            aes(x = x, y = y, color = "Binned"), linetype = "dashed") +
  scale_color_manual(values = c("Binned" = "blue", 
                                "Naive" = "hotpink"))+
  labs(color = "Density types", title = "Geyser data: CV BW", x = "Waiting time", y = "Density")

```
Here the cross validation bandwidth does terrible. We do the same for the eruption time

```{r}
real_obj <- my_density(faithful$eruptions)

tibble("CV bw" = real_obj$cv_bw, "AMISE bw" = real_obj$amise_bw) %>% kable()
```

More similar bandwidths. 

```{r}
ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.3, aes(x = eruptions, y = ..density..)) +
  geom_line(data = plot_data(real_obj, h = real_obj$amise_bw),
            aes(x = x, y = y, color = "Naive"), linetype = "dashed") +
    geom_line(data = plot_data(real_obj, h = real_obj$amise_bw, binned = TRUE, B = 20),
            aes(x = x, y = y, color = "Binned"), linetype = "dashed") +
  scale_color_manual(values = c("Binned" = "blue", 
                                "Naive" = "hotpink"))+
  labs(color = "Density types", title = "Geyser data: AMISE BW", x = "Waiting time", y = "Density")

ggplot(faithful, aes(x = eruptions)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.3, aes(x = eruptions, y = ..density..)) +
  geom_line(data = plot_data(real_obj, h = real_obj$cv_bw),
            aes(x = x, y = y, color = "Naive"), linetype = "dashed") +
  geom_line(data = plot_data(real_obj, h = real_obj$cv_bw, binned = TRUE, B = 20),
            aes(x = x, y = y, color = "Binned"), linetype = "dashed") +
  scale_color_manual(values = c("Binned" = "blue", 
                                "Naive" = "hotpink"))+
  labs(color = "Density types", title = "Geyser data: CV BW", x = "Waiting time", y = "Density")

```
Again I would say teh AMISE bandwidth does better. Furthermore we can safely conclude that the difference between the binned kernel estimator and the naive kernel estimator is very small, at least for this one particular real data set. 

To do:
- læse opgave formulering og sikre mig at jeg får besvaret tingene
- bygge en story line op, og vælge de bedste ting at tage med, som viser at jeg har besvaret de ting de går efter
- evt. bygge en pakke 
- lave parallelisering og rcpp, og undersøge cv bw og sikre mig at den gør det rigtige
- gøre præsentationen lækker
