---
title: "The EM-algorithm for the  t-Distribution"
author: "Michaela Lukacova (dns525)"
institute: "University of Copenhagen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: themer-new.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content > h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
</style>

```{r, include=FALSE, eval = FALSE}
#rmarkdown::render('presentation.rmd')
```

```{r xaringan-themer, include = FALSE, warning = FALSE}
library(xaringanthemer)
library(CSwR)
style_mono_accent(
 base_color = "#4f7942")
```

```{r packages, echo=FALSE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(bench)
library(tidyverse)
library(profvis)
library(bench)
library(Rcpp)
library(knitr)
library(testthat)
library(gridExtra)
theme_set(theme_bw())
knitr::opts_chunk$set(fig.retina = 2)
theme_set(
  theme_bw(base_size = 18) +  # Set base font size and family
    theme(
      text = element_text(size = 15),           # Adjust text size
    )
)
```

```{r source, echo = FALSE, warning=FALSE, message=FALSE}
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/simulate.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/MLE.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/EM.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/parameters.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/loglik.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/GD.R")
source("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/fischer_info.R")
Rcpp::sourceCpp("~/Desktop/Uni/5aar/CompStat/comp_stat/Michaela/EM_ML/source_code/e_step_m_step_cpp.cpp")
```

### Introduction and outline

The joint density of $Y = \left( X, W \right)$ is given by

$$f(x,w) = \frac{1}{\sqrt{\pi \nu  \sigma^2} 2 ^{(\nu +1)/2}\Gamma(\nu/2)}  w^{(\nu -1)/2} \exp\left(-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)\right)$$

- Why is the marginal density of $X$ a $t$-distribution?

- Maximizing the complete data log likelihood for iid. observations

- The EM algorithm for estimating $(\mu, \sigma^2)$

- Comparing with other optimization algorithm based on the marginal log-likelihood

- The Fisher information


---
### The marginal distribution of $X$ is the $t$-distribution

$$f(x) = \int f(x,w) dw =C \int \frac{1}{2^{(\nu+1)/2}} w^{(\nu -1)/2} \exp\left(-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)\right) dw$$

Where $C = \frac{1}{\sqrt{\pi \nu  \sigma^2} \Gamma(\nu/2)}$. Let $z = \frac{\nu+1}{2}$, implying that $z-1 = \frac{\nu-1}{2}$, and $t = \frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)$. 

$$f(x) =\int \frac{1}{2^{z}} w^{z - 1} \exp\left(-t \right) dw$$
Note that $w = 2t\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}$ and $dw = 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1} dt$.

$$f(x) =C \int \frac{1}{2^{z}} \left(2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)\right)^{-z}  t^{z-1}\exp\left(-t\right) dt  = C  \left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-z}\int t^{z-1} \exp\left(-t\right) dt$$

Recognizing the gamma function we finally obtain

$$f(x) = \frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\pi \nu  \sigma^2} \Gamma(\nu/2)} \left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{-\frac{\nu+1}{2}}$$

Which is recognized as the density of the $t$-distribution. 

---
### Maximizing the complete data log likelihood for iid. observations

The complete data log likelihood is

$$\mathcal{l}(Y) = \sum_{i=1}^n -\log\left(\sqrt{\pi \nu \sigma^2} 2 ^{(\nu +1)/2}\Gamma(\nu/2)\right) + \frac{\nu -1}{2}\log(w_i) -\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu \sigma^2}\right) \\
\propto \sum_{i=1}^n - \log(\sigma) + \frac{\nu -1}{2}\log(w_i) -\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu \sigma^2}\right) = g(\mu,\sigma)$$

Finding the derivatives of the log-likelihood with respect to $\mu$ and $\sigma^2$

$$\frac{\partial g}{\partial \mu} = \sum_{i=1}^n \frac{w_i(x_i-\mu)}{\nu \sigma^2} \\
\frac{\partial g}{\partial \sigma} = - \frac{n}{\sigma} + \sum_{i=1}^n \frac{w_i}{\nu \sigma^3}(x_i-\mu)^2$$


Setting the derivatives of the log-likelihood with respect to $\mu$ and $\sigma^2$  equal to zero results in the following estimators:

$$\hat{\mu} = \frac{\sum_{i=1}^n w_i x_i}{\sum_{i=1}^n w_i} \\
\hat{\sigma}^2 = \frac{\sum_{i=1}^n w_i (x_i - \hat{\mu})^2}{\nu n}$$

---
### Data generation

We have created a S3 class `parameters` with a `sim` function that generates data, a `mle` function that computes the full data MLE estimates and a `print` method.  

- Takes as input sample size $n$ and a vector of parameters $(\mu, \sigma^2, \nu)$

- Returns a list with two elements, $x$ and $w$

- The $w$'s are drawn from a $\chi^2$ distribution with $\nu$ degrees of freedom

- The $x$'s are drawn from a normal distribution with mean $\mu$ and variance $\sigma^2 \nu / w$

```{r}
set.seed(6783)
par0 <- parameters(1, 2, 1); par0
data0 <- sim(par0, n = 1000)
mle0 <- mle(par0, data0); mle0
```

---
### Testing the MLE estimators

We have implemented the MLE estimators of the complete data log likelihood as respectively `mle.mu` and `mle.sigma2`. We test them out for three different parameter sets, and different sample sizes. 

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
set.seed(974)
true_params <- data.frame(mu = c(1, 5, 0), sigma2 = c(3, 10, 3), nu = c(1,1,3))
mu_est <- numeric(6*3) # rows are parameter index, columns are sample size
sigma2_est <- numeric(6*3) # rows are parameter index, columns are sample size

sample_size = rep(c(20, 30, 40, 50, 100, 200), each = 3)
param_id = rep(1:3, times = 6)

param_labels <- c(
  "1" = "(1,3,1)",
  "2" = "(5,10,1)",
  "3" = "(0,3,3)"
)

for(i in 1:(6*3)){
  ss <- sample_size[i]
  par <- true_params[param_id[i],]
  data <- simulate(n = ss, par = par) 
  mu_est[i] <- mle.mu(data$x, data$w)
  sigma2_est[i] <- mle.sigma2(data$x, data$w, mu_est[i], par$nu)
}

data <- data.frame(
  sample_size = sample_size,
  param_id = param_id,  # 3 parameters, each with 6 sample sizes
  mu_mle = mu_est - true_params[param_id,1],
  sigma2_mle = sigma2_est - true_params[param_id,2]
)

# Reshape data to long format
data_long <- data %>%
  pivot_longer(cols = starts_with("mu_") | starts_with("sigma2_"), 
               names_to = c("Parameter", "Type"), 
               names_sep = "_", 
               values_to = "Value")

ggplot(data_long, aes(x = sample_size, y = Value, color = Type)) +
  geom_line(aes(linetype = Type), size = 1) +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed", aes(linewidth = 0.5)) +
  geom_point(size = 2) +
  facet_grid(Parameter ~ param_id, scales = "free_y", 
             labeller = labeller(param_id = param_labels)) +  # Separate rows for `mu` and `sigma`
  scale_color_manual(values = c("mle" = "blue")) +
  scale_linetype_manual(values = c("mle" = "solid")) +
  labs(x = "Sample Size", y = "Dist to true val", 
       title = "MLE for Parameters Across Sample Sizes") +
  theme_minimal() +
  theme(legend.position = "top", legend.title = element_blank())

```

---
### Implementing the EM algorithm

- Suppose we only observe the $X$'s.

- The EM algorithm can then be used to compute maximum likelihood estimates of $\theta = (\mu, \sigma^2)$. 

- The algorithm is generally a descent algorithm for the negative log likelihood. 

- Consists of two steps

  + The E-step: Computing the conditional expectation $Q(\theta | \theta ') = E_{\theta'} \left(\sum_{i=1}^{n}\log \left( f\left(x,w| \theta\right)\right) | X = x\right)$
  
  + The M-step: Maximizing the $Q$-function wrt. $\theta$ to obtain the new estimate $\theta'$


---
### Implementing the EM algorithm: The E-step

$$Q(\theta | \theta ') = E_{\theta'} \left(\sum_{i=1}^{n}\log \left( f\left(x,w| \theta\right)\right) | X = x\right) \\
= \sum_{i=1}^{n} -\log\left(\sqrt{\pi \nu \sigma^2} 2 ^{(\nu +1)/2}\Gamma(\nu/2)\right) + \frac{\nu -1}{2}E_{\theta'} \left(\log(W_i) | X = x \right) -E_{\theta'} \left(W_i | X = x \right)\frac{1}{2}\left(1+\frac{(x_i-\mu)^2}{\nu \sigma^2}\right)$$

We need to compute $E_{\theta'} \left(W_i | X = x \right)$ and $E_{\theta'} \left(\log(W_i) | X = x \right)$. 

By using various tricks we can write
$$f(x,w) = P  \frac{1}{\Gamma(k)\lambda^{k}} w^{k-1} \exp{- \frac{w}{\lambda}}$$
Where $k = \frac{\nu + 1}{2}$ and $\lambda = 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}$. We can recognize $f(x,w)$ for fixed $x$ as the gamma density multiplied by the constant $P$. Note that
$$f_{w|x = x'} (w) = \frac{f(x',w)}{f(x')} \propto f(x',w)$$
It follows that $W | X = x \sim \Gamma\left(\frac{\nu + 1}{2}, 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}\right)$

---
### Implementing the EM algorithm: The E-step

Thus

$$E_{\theta'} \left(W_i | X = x \right) = (\nu + 1)\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}$$
Using that $E(X) = k \lambda$ for random variable $X \sim \Gamma(k, \lambda)$. Furthermore we have $E(\log(X)) = \psi(k) + \log \lambda$, resulting in

$$E_{\theta'} \left(\log W_i | X = x \right) = \psi\left(\frac{\nu + 1}{2}\right) + \log \left( 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1} \right)$$

---
### Implementing the EM algorithm: The M-step

Finding the first and second derivative of the $Q$ function wrt. $\mu$

$$\frac{d}{d\mu} Q(\theta | \theta ') = \sum_{i=1}^n E_{\theta'} \left(W_i | X = x \right)\frac{(x_i-\mu)}{\nu \sigma^2} \\
    \frac{d^2}{d\mu^2} Q(\theta | \theta ') = \sum_{i=1}^n -  E_{\theta'} \left(W_i | X = x \right)\frac{1}{\nu \sigma^2} < 0$$
    
The second derivative is negative and the function is concave, the optimum can therefore be found by setting the first derivative equal to 0. 

$$\hat{\mu} = \frac{\sum_{i=1}^n  E_{\theta'} \left(W_i | X = x \right) x_i}{\sum_{i=1}^n  E_{\theta'} \left(W_i | X = x \right)}$$
    
The first and second derivative wrt. $\sigma$ are found to be

$$\frac{d}{d\sigma} Q(\theta | \theta ') = \sum_{i=1}^n - \frac{1}{\sigma} + E_{\theta'} \left(W_i | X = x \right)\frac{(x_i-\mu)^2}{\nu \sigma^3} \\
    \frac{d^2}{d\sigma^2} Q(\theta | \theta ') = \sum_{i=1}^n \frac{1}{\sigma^2} \left( 1 - 3 E_{\theta'} \left(W_i | X = x \right)\frac{(x_i-\mu)^2}{\nu \sigma^2} \right)$$

Setting the derivative to $0$:

$$\hat{\sigma}^2 = \frac{\sum_{i=1}^n E_{\theta'}\left(W_i | X = x \right) (x_i - \mu)^2}{n \nu}$$
---
### The EM algorithm

- The E-step is implemented as the `E.step` function

  + Takes as input the parameters $\theta$, data $X$ and $\nu$
  + Computes the conditional expectation of $W$ given $X$
  
- The M-step is implemented as the `M.step` function 

  + `M.step` takes as input the conditional expectation of $W$ given $X$, the data $X$ and $\nu$
  + Returns the estimates of $\mu$ and $\sigma^2$ that maximize the Q-function
  
- The two steps are combined by the function factory `em_factory` 

  + Takes as input the E-step and M-step functions, $\epsilon$, and $\nu$
  + Performs the EM algorithm until convergence
  + With the convergence criterion: $||\theta_n - \theta_{n-1}||^2_2 \leq \epsilon (||\theta ||^2_2 + \epsilon)$
  
We perform a naive test of the EM algorithm with start values $(1,1)$ and the data generated from the parameter set $(1,2,1)$.

```{r, echo = FALSE}
my_em <- em_factory(E.step, M.step, eps = 1e-6, nu = 1)
tibble("EM estimates" = my_em(c(1,1), X = data0$x), 
       "Full data MLE" = mle(par0, data0)) %>% 
  kable() %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

---
### EM: S3 class

We create an S3 class called `My_EM`, that stores the EM estimates, the tracer obejct, true parameters and other usefull quantities. The class has a `print` method

```{r, echo = TRUE}
EM0 <- EM(c(2,2), data0$x, par_true = par0$pars)
EM0
```

---
### EM: Rate of convergence

We can plot the convergence of the EM algorithm as a function of the number of iterations, and find the rate of convergence

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
ggplot(EM0$trace, aes(n, par_norm_diff)) +
  geom_point() +
  scale_y_log10()

log_rate_fit <- lm(log(par_norm_diff) ~ n,  data = EM0$trace)
exp(coefficients(log_rate_fit)["n"]) %>%
  kable(col.names = "log-rate of convergence", row.names = FALSE) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

---
### Heatmap of the log-likelihood

```{r, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
heat_map(obj = EM0, mle = mle0, x = data0$x)
```

---
### Different starting points

We try out different starting points for the EM algorithm:

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
tibble("c(1,1)" = my_em(c(1,1), X = data0$x), 
       "c(5,5)" = my_em(c(5,5), X = data0$x), 
       "c(-3,10)" = my_em(c(-3,10), X = data0$x), 
       "c(-20,200)" = my_em(c(-20,200), X = data0$x), 
       "Full data MLE" = mle(par0, data0)) %>% 
  kable(caption = "Different starting values",) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```


```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
EM1 <- EM(c(-20,50), data0$x, par_true = par0$pars)
heat_map(obj = EM1, mle = mle0, x = data0$x, grid_vals_m = c(-20,4), grid_vals_s = c(50,0.01))
```

---
### More convergence plots

And a plot method with different plot options:

```{r, message=FALSE, echo = TRUE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
grid.arrange(
  plot(EM0, mle_par = mle(par0, data0), plot_no = 2), 
  plot(EM0, mle_par = mle(par0, data0), W = data0$w, plot_no = 5),
  nrow = 1)
```

---
### Profiling

```{r, echo = FALSE}
set.seed(4027)
data1 <- simulate(10^6, c(1,2,3))

profvis({
  
  E.step <- function(X, par, nu){
    (nu + 1) / (1 + (X - par[1])^2 / (nu * par[2]))
  }
  
  ## M step
  
  M.step <- function(E_W, X, nu) {
    mu_k <- sum(E_W * X) / sum(E_W)
    sigma2_k <- sum(E_W * (X - mu_k)^2) / (length(X) * nu)
    return(c(mu_k, sigma2_k))
  }
  
  
  ## EM algorithm
  
  em_factory <- function(e_step, m_step, eps = 1e-6, nu) {
    force(e_step); force(m_step); force(eps); force(nu)
    function(par, X, epsilon = eps, cb = NULL, ...) {
      k <- 1
      repeat {
        if (!is.null(cb)) cb()
        k <- k + 1
        par0 <- par
        E_W <- e_step(X = X, par = par, nu = nu)
        par <- m_step(E_W = E_W, X = X, nu = nu)
        if (sum((par - par0)^2) <= epsilon * (sum(par^2) + epsilon))
          break
      }
      par  # Returns the parameter estimate
    }
  }
  
  my_em <- em_factory(E.step, M.step, eps = 1e-6, nu = 1)
  my_em(c(2,2), X = data1$x)
})

```


---
### Rcpp implementation

We have implemented the E-step and M-step in C++ using Rcpp. The functions are called `E_step_cpp` and `M_step_cpp`. 

Sanity check
```{r}
my_em_cpp <- em_factory(e_step = E_step_cpp, m_step = M_step_cpp, nu = 1)
my_em_cpp(c(2,2), X = data0$x) - my_em(c(2,2), X = data0$x) 
```

---
### Benchmarking

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
set.seed(4027)
data <- simulate(10^7, c(1,2,1))

em_bench <- bench::press(
  k = 10^(3:7),
  {
    bench::mark(
      "Rcpp" =  my_em_cpp(c(2,2), X = data$x[1:k]),
      "R" = my_em(c(2,2), X = data$x[1:k]),
      check = FALSE
    )
  }
)

em_bench %>% 
  mutate(expr = as.character(expression), median = as.numeric(median)) %>% 
  ggplot(aes(k, median, color = expr)) + geom_point() + scale_y_log10() +
  geom_line() + labs(y = "time (ms)")
```

- R code is already fast and vectorized
- The Rcpp implementation uses loops
- To increase speed one could try to use Rcpp Armadillo


---
### Gradiant Descent

As loss function we will use the negative loglikelihood divided by $\frac{1}{n}$

$$-\frac{1}{n}\mathcal{l}(\theta) = \frac{1}{n}\sum_{i=1}^n \log(\sigma) + \frac{\nu +1 }{2} \log \left( 1 + \frac{(x_i - \mu)^2}{\nu \sigma^2} \right)$$
The gradient is

$$\nabla - \mathcal{l}(\theta) =  \begin{pmatrix} \sum_{i=1}^n  \left(\nu +1 \right) \frac{(x_i - \mu)}{(x_i - \mu)^2 + \nu \sigma^2}  \\ \sum_{i=1}^n \frac{1}{\sigma} -  \frac{(\nu +1)}{  \nu \sigma^2 + (x_i - \mu)^2} \frac{(x_i - \mu)^2}{\sigma} \end{pmatrix}$$
 
We implement the gradient in the function `grad` and check that the gradient is close to $0$ in the full data MLE estimates.

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
grad_negloglik(mu = mle0[1], sigma = sqrt(mle0[2]), nu = par0$nu, x = data0$x) * 1 / length(data0$x)
```

---
### Gradient Descent

We implement gradient descent in the function `grad_desc` that takes as input the gradient function, the Hessian function, the data, and the starting values. The GD algorithm performs backtracking linesearch, and the learning rate is set to $1$.

```{r, message=FALSE, echo = TRUE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
grad_est <- grad_desc(par = c(2,2), x = data0$x, nu = par0$nu)
grad_est
```

Below we compare with the MLE estimates and the EM estimates

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
tibble("EM est" = my_em(c(1,1), X = data0$x), 
       "Full data MLE" = mle(par0, data0),
       "GD est" = grad_est,
       ) %>% 
  kable() %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

---
### Gradient Descent

We once again create an S3 object `GD` that stores the gradient descent estimates, the tracer object, true parameters and other useful quantities. The class has a `print` method

```{r, message=FALSE, echo = TRUE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
GD0 <- GD(par = c(2,2), x = data0$x, nu = par0$nu)
GD0
```

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
grid.arrange(plot(GD0),
             ggplot(GD0$trace, aes(n, par_norm_diff)) +
                geom_point() +
                scale_y_log10(),
             nrow = 1)
```

---
### GD heat map

```{r, message=FALSE, echo = TRUE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
heat_map(obj = GD0, mle = mle0, x = data0$x)
```

---
### Comparison: Convergence plots

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
plot(GD0)+
  geom_line(data = plot_data(EM0), aes(y = neg_loglik, x = .time, color = "EM"))+
  scale_color_manual(values = c("EM" = "red3", "GD" = "blue3"))
```

---
### Different parameter values

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
set.seed(4027)
par1 <- parameters(4,5,6); data1 <- sim(obj = par1, n = 10^5)
par2 <- parameters(-4,0.3,3); data2 <- sim(obj = par2, n = 200)
EM1 <- EM(c(1,1), data1$x, par_true = par1$pars); EM2 <- EM(c(1,1), data2$x, par_true = par2$pars)
GD1 <- GD(par = c(1,1), x = data1$x, nu = par1$nu); GD2 <- GD(par = c(1,1), x = data2$x, nu = par2$nu)

tibble("Parameter" = rep(c("mu", "sigma2"), 2),
       "MLE est" = c(mle(par1, data1), mle(par2, data2)), 
       "EM est" = c(EM1$est, EM2$est), 
       "GD est" = c(GD1$est, GD2$est)) %>% 
  kable() %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")

grid.arrange(
  plot(GD1) +
  geom_line(data = plot_data(EM1), aes(y = neg_loglik, x = .time, color = "EM"))+
  scale_color_manual(values = c("EM" = "red3", "GD" = "blue3"))+
  labs(title = "n = 10^5, True par = c(4,5,6)"), 
  plot(GD2) +
  geom_line(data = plot_data(EM2), aes(y = neg_loglik, x = .time, color = "EM"))+
  scale_color_manual(values = c("EM" = "red3", "GD" = "blue3"))+
    labs(title = "n = 200, True par = c(-4,0.3,3)"), nrow = 1)
```

---
### GD: Different starting values 

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
tibble("c(1,1)" = grad_desc(par = c(1,1), grad = grad_negloglik, H = neg_loglik, x = data0$x, nu = par0$nu), 
       "c(5,5)" = grad_desc(par = c(5,5), grad = grad_negloglik, H = neg_loglik, x = data0$x, nu = par0$nu), 
       "c(-3,10)" = grad_desc(par = c(-3,10), grad = grad_negloglik, H = neg_loglik, x = data0$x, nu = par0$nu), 
       "c(-20,200)" = grad_desc(par = c(-20,200), grad = grad_negloglik, H = neg_loglik, x = data0$x, nu = par0$nu), 
       "Full data MLE" = mle(par0, data0))   %>% 
  kable(caption = "Different starting values",) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```



```{r, echo = FALSE}
tibble("Grad in (-20, 200)" = grad_negloglik(-20, 200, 1, data0$x), "Grad in (1,1)" = grad_negloglik(1, 1, 1, data0$x)) %>% 
  kable(caption = "Grad. at diff. start vals",) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
GD2 <- GD(par = c(-20,10), x = data0$x, nu = par0$nu)
heat_map(obj = GD2, mle = mle0, x = data0$x, grid_vals_m = c(-20,4), grid_vals_s = c(0,32))
```

---
### Optimizing: Implementing momentum

We implement Gradient Descent with Polyak Momentum. This amounts to altering the update rule to

$$\theta_{n+1} = \theta_n - \gamma \nabla \mathcal{l}(\theta_n) + \mu (\theta_n - \theta_{n-1})$$
We use $\beta = 0.9$. We create a new object to sanity check the algorithm

```{r}
GD_mom <- GD(par = c(1,1), x = data0$x, nu = par0$nu, alg = grad_desc_mom)
GD_mom
```
Looks promising! The number of iterations decreased from 28 to 11. 
---
### Comparison: Benchmarking

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
set.seed(4027)
data <- sim(obj = par0, n = 10^6)

em_bench <- bench::press(
  k = 10^(3:6),
  {
    bench::mark(
      "GD" =  grad_desc(par = c(2,2), x = data$x[1:k], nu = par0$nu),
      "EM" = my_em(c(2,2), X = data$x[1:k]),
      "GD Momentum" = grad_desc_mom(c(2,2), x = data$x[1:k], nu = par0$nu),
      check = FALSE
    )
  }
)

em_bench %>% 
  mutate(expr = as.character(expression), median = as.numeric(median)) %>% 
  ggplot(aes(k, median, color = expr)) + geom_point() + scale_y_log10() +
  geom_line() + labs(y = "time (ms)")
```

---
### Empirical Fischer information

An estimate of the empirical Fischer information is computed as

$$\hat{\mathcal{I}} = \sum_{i = 1}^N \left(\nabla_{\theta} Q_i(\hat{\theta}| \hat{\theta}) - N^{-1} \nabla_{\theta} \mathcal{l}(\hat{\theta})  \right)\left(\nabla_{\theta} Q_i(\hat{\theta}| \hat{\theta}) - N^{-1} \nabla_{\theta} \mathcal{l}(\hat{\theta})  \right)^T$$

Where $\mathcal{l}(\hat{\theta}) = \sum_{i=1}^N \mathcal{l}_i(\hat{\theta})$. This is implemented in the function `fis_inf`. 

```{r, include = FALSE}
# sanity check
sum(abs(grad_Q(par0$mu, sqrt(par0$sigma2), par0$nu, data0$x) - 
          grad_loglik(par0$mu, sqrt(par0$sigma2), par0$nu, data0$x))) <= 10^-13 

```

```{r, echo = FALSE}
fisher <- fis_inf(EM0$est[1], sqrt(EM0$est[2]), par0$nu, data0$x)
fisher
```

The empirical Fisher Information paves way for the confidence interval

$$\hat{\theta}_i \pm 1.96 \sqrt{\hat{\mathcal{I}}^{-1}_{ii}}$$

```{r, echo = FALSE}
ci_mu <-EM0$est[1] + c(-1,1) * 1.96 * sqrt(solve(fisher)[1,1])
ci_sigma <- EM0$est[2] + c(-1,1) * 1.96 * sqrt(solve(fisher)[2,2])

tibble("Parameter" = c(expression(mu), expression(sigma ^2)), 
       "Lower" = c(ci_mu[1], ci_sigma[1]), 
       "Upper" = c(ci_mu[2], ci_sigma[2])) %>% 
  kable(caption = "Confidence interval",) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")

```

---
### Numerical Fischer information: 

By usinng `optimHess` we can compute the Hessian of the log-likelihood function and use this as an estimate of the Fisher information. 

```{r, echo = FALSE}
loglik2 <- function(par) loglik(mu = par[1], sigma = sqrt(par[2]), nu = par0$nu, x = data0$x)
grad2 <- function(par) grad_loglik(mu = par[1], sigma = sqrt(par[2]), nu = par0$nu, x = data0$x)
fisher2 <- -optimHess(EM0$est, loglik, grad2)
fisher2
```

Similarly we can use the function `jacobian` from the `numDeriv` package to compute the Jacobian of the gradient function, and use this as an estimate of the Fisher information. 

```{r, echo = FALSE}
library(numDeriv)
ihat <- -jacobian(grad2, EM0$est)
ihat
```

Leading to the confidence intervals

```{r, echo = FALSE}
ci_mu2 <-EM0$est[1] + c(-1,1) * 1.96 * sqrt(solve(fisher2)[1,1])
ci_sigma2 <- EM0$est[2] + c(-1,1) * 1.96 * sqrt(solve(fisher2)[2,2])

ci_mu3 <-EM0$est[1] + c(-1,1) * 1.96 * sqrt(solve(ihat)[1,1])
ci_sigma3 <- EM0$est[2] + c(-1,1) * 1.96 * sqrt(solve(ihat)[2,2])

tibble("Parameter" = c("Hessian mu", "Hessian sigma^2", "Jacobian mu", "Jacobian sigma^2"), 
       "Lower" = c(ci_mu2[1], ci_sigma2[1], ci_mu3[1], ci_sigma3[1]), 
       "Upper" = c(ci_mu2[2], ci_sigma2[2], ci_mu3[2], ci_sigma3[2])) %>% 
  kable(caption = "Confidence intervals",) %>% 
  kableExtra::kable_styling(full_width = FALSE, position = "center")
```

---
### Conclusion

- The implementation of the EM algorithm is quite fast and efficient.

- The EM and GD algorithm are quite robust in terms of starting values and true parameter values (for this problem at least).

- The EM algorithm outperforms the gradient descent algorithm in terms of speed and convergence.

- The GD algorithm can be optimized by using Polyak momentum, but is still not able to compete with the EM algorithm. 

- The confidence intervals computed using the Fisher information cover the true parameter values.

---
### Appendix

$$Q(\theta | \theta ') = E_{\theta'} \left(\sum_{i=1}^{n}\log \left( f\left(x,w| \theta\right)\right) | X = x\right) \\
= \sum_{i=1}^{n} -\log\left(\sqrt{\pi \nu \sigma^2} 2 ^{(\nu +1)/2}\Gamma(\nu/2)\right) + \frac{\nu -1}{2}E_{\theta'} \left(\log(W_i) | X = x \right) -E_{\theta'} \left(W_i | X = x \right)\frac{1}{2}\left(1+\frac{(x_i-\mu)^2}{\nu \sigma^2}\right)$$

We need to compute $E_{\theta'} \left(W_i | X = x \right)$ and $E_{\theta'} \left(\log(W_i) | X = x \right)$. Note that

$$f_{w|x = x'} (w) = \frac{f(x',w)}{f(x')} \propto f(x',w)$$

Note also 

$$f(x,w)  = \frac{1}{\sqrt{\pi \nu  \sigma^2} 2 ^{(\nu +1)/2}\Gamma(\nu/2)}  w^{(\nu -1)/2} \exp\left(-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)\right) \\
     =\frac{1}{\sqrt{\pi \nu  \sigma^2} 2 ^{k}\Gamma(\nu/2)} w^{k-1} \exp{- \frac{w}{\lambda}}$$

Where we have defined $k = \frac{\nu + 1}{2}$ and $\lambda = 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}$. 

---
### Appendix

Rewriting we obtain

$$f(x,w) = \frac{\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-(\frac{\nu + 1}{2})}}{\sqrt{\pi \nu  \sigma^2}\Gamma(\nu/2)} \frac{1}{\lambda^{k}} w^{k-1} \exp{- \frac{w}{\lambda}}$$

Note also that

$$\frac{\Gamma(\frac{\nu}{2})}{\Gamma(\frac{\nu}{2} + \frac{1}{2})} = 2^{1-\nu} \sqrt{\pi} \Gamma(\nu) \Leftrightarrow \\
    \Gamma(\frac{\nu}{2}) = 2^{1-\nu} \sqrt{\pi} \Gamma(\nu) \Gamma(k)$$
Substituting we obtain

$$f_{w|x=x'}(w) \propto f(x',w) = K  \frac{1}{\Gamma(k)\lambda^{k}} w^{k-1} \exp{- \frac{w}{\lambda}}$$

Where we have defined $K = \frac{\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-(\frac{\nu + 1}{2})}}{\pi \sqrt{ \nu  \sigma^2} 2^{1-\nu} \Gamma(\nu)}$. We can recognize $f(x,w)$ for fixed $x$ as the gamma density multiplied by the constant $K$. It follows that $W | X = x \sim \Gamma\left(\frac{\nu + 1}{2}, 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}\right)$. 

---
### Appendix

Thus

$$E_{\theta'} \left(W_i | X = x \right) = (\nu + 1)\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1}$$
Using that $E(X) = k \lambda$ for random variable $X \sim \Gamma(k, \lambda)$. Furthermore we have $E(\log(X)) = \psi(k) + \log \lambda$, resulting in

$$E_{\theta'} \left(\log W_i | X = x \right) = \psi\left(\frac{\nu + 1}{2}\right) + \log \left( 2\left(1+\frac{(x-\mu)^2}{\nu \sigma^2}\right)^{-1} \right)$$

