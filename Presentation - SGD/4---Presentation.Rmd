---
title: "Log-logistic Dose-response Curves"
author: 
- "Christian Rubjerg Hejstvig-Larsen (brf337)"
- "Dina Gyberg Jensen (vbz248)"
institute: "University of Copenhagen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content > h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
</style>
```{r, include=FALSE, eval = FALSE}
rmarkdown::render('presentation.rmd')
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(CSwR)
style_mono_accent(
 base_color = "#4f7942")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(bench)
library(tidyverse)
library(profvis)
library(bench)
theme_set(theme_bw() + theme(text = element_text(size = 13)))
```
###Introduction
Objective is to use stochastic optimization to estimate the parameters of a log-logistic dose-response model using non-linear least squares estimation. That is obtaining the parameters $\alpha, \beta, \gamma, \rho$ that minimize the loss function:
$$L(X,(\alpha,\beta,\gamma,\rho))=\frac{1}{N}\sum_{i=1}^N\left( y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)^2$$
<br>
Where the response is given by:
$$Y_i = f(x_i| \alpha, \beta,\gamma,\rho) + \varepsilon_i \hskip5mm \text{with}\hskip2mm \varepsilon_i \stackrel{iid}{\sim} N(0,\omega^2)$$
And the log-logistic dose-response model is given by:
$$f(x_i| \alpha, \beta,\gamma,\rho) = \gamma + \frac{\rho - \gamma}{1 + \exp(\beta \log(x_i) - \alpha)}$$

---
###Stochastic Gradient Descent
Start out implementing a standard version of the stochastic gradient descent algorithm (SGD). The gradient in a single point is given by
$$-2\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)  $$

<br>
Where $\nabla f(x_i| \alpha, \beta,\gamma,\rho)$ is given by
$$\nabla f(x_i| \alpha, \beta,\gamma,\rho) = \begin{pmatrix}
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\\
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\cdot\log(x_i)\\
  1-\frac{1}{1+\exp(\beta\log(x_i)-\alpha)} \\
  \frac{1}{1+\exp(\beta\log(x_i)-\alpha)}
\end{pmatrix}$$
So the update scheme becomes:
$$\theta_{t+1} = \theta_t +2 \gamma_t\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)$$
Where $\theta_t = (\alpha_t, \beta_t, \gamma_t, \rho_t)$ and $\gamma_t$ is the learning rate at time $t$.
---
###Implementation - Michs s√¶t rigti algoritme ind <3
The gradient function and $f$ are implemented seperately
```{r}

sgd <- function(
    par,
    grad, # Function of parameter and observation index
    gamma, # Decay schedule or a fixed learning rate
    maxiter = 150, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    epoch = vanilla,
    m = 1, # Batch size
    x,
    y,
    ...) {
  
  n <- length(x)
  gamma <- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)
  
  for (k in 1:maxiter) {
    
    if (!is.null(cb)) cb()
    samp <- sampler(n)
    par <- epoch(par = par, samp = samp, gamma = gamma[k], 
                 grad = grad, n = n, x = x, y = y, m = m)
    
  }
  par
}

vanilla <- function(par, samp, gamma, grad, n, x, y, ...){
  for (j in 1:n) {
    i <- samp[j]
    par <- par - gamma * grad(par, x[i], y[i])
  }
  return(par)
}
```

---
###Sampling
 
 - Implemented an S3 object `parameters` and a method `sim` to simulate data from the log-logistic dose-response model.
 
 - 2 different types of sampling methods
 
  - `grid_sample`: samples $X$ from a fixed grid of points $(e, e^2,..., e^{15})$.
  
  - `gauss_sample`: samples $\log(X)$ from $\mathcal{N}(0, \omega^2)$. Note $\omega$ may not be too large.

- The method also allows for scaling the data, which is useful for optimization.

```{r, eval = FALSE, message=FALSE}
sim(parameters(1, 1, 1, 1), N = 100, grid = TRUE, scale = TRUE)
```
 - True parameters have been set to `c(2,5,1,2)` and the data is simulated with `N = 5000`.
 
```{r, echo=FALSE, message=FALSE, warning=FALSE}
N <- 5000
param <- parameters(2,5,1,2)
data <- sim(param, N)
x <- data$x
y <- data$y

x_i <- data$x
y_i <- data$y
```

---
### Test of Algorithm 

```{r, echo=FALSE, warning=FALSE, message=FALSE}
### Source code

source("~/comp_stat/Stochastic_Optimization_ML/SGD.R")
source("~/comp_stat/Stochastic_Optimization_ML/sampler.R")
source("~/comp_stat/Stochastic_Optimization_ML/GD.R")
```

We do a naive test of the algorithm, and run the algorithm with 3 different starting values: the true values $(2,5,1,2)$, $(1,1,1,1)$ and random values $(2.63, 1.64, 1.00, 0.49)$. 

```{r, echo = FALSE}
set.seed(4027)
# Algorithm  for true values as starting values
sgd1 <- SGD(par0 = sv1, grad = grad, gamma = 0.005, x = x, y = y, true_par = param$par, maxit = 300)

# Algorithm for 2 times true values as starting values
sgd2 <- SGD(par0 = sv2, grad = grad, gamma = 0.005, x = x, y = y, true_par = param$par, maxit = 300)

# Algorithm for half of true values asstarting values
sgd3 <- SGD(par0 = sv3, grad = grad, gamma = 0.005, x = x, y = y, true_par = param$par, maxit = 300)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.width=10, fig.height=4, fig.align='center'}
grid.arrange(plot(sgd1, 1) + ggtitle("True vals"),
             plot(sgd2, 1)+ ggtitle("c(1,1,1,1)"), 
             plot(sgd3, 1) + ggtitle("Random vals"),
             nrow = 1)
```

We obtain quite different convergence schemes. 

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.width=10, fig.height=4, fig.align='center'}
plot(sgd1, 3) + 
  geom_line(aes(x = plot_data(sgd2)$.time, 
                y = plot_data(sgd2)$abs_dist_from_par), col = "orange") + 
  geom_line(aes(x = plot_data(sgd3)$.time, 
                y = plot_data(sgd3)$abs_dist_from_par), col = "red")

diff_start_vals_sgd <- tibble(
  "Par" = c("alpha (0.1)", "beta (1.0)", "gamma (2.2)", "rho (1.0)"),
  "True vals" = sgd1$est,
  "(1, 1, 1, 1)" = sgd2$est,
   "Random vals" = sgd3$est)

diff_start_vals_sgd %>% kable(caption = "Different starting values")
```

---
###Profiling the algorithm
```{r, echo = FALSE}
profvis({
f <- function(x, par){
  alpha <- par[1]
  beta <- par[2]
  gamma <- par[3]
  rho <- par[4]
  
  return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))
}



gradient <- function(par, i, x, y,...){
  alpha <- par[1]
  beta <- par[2]
  gamma <- par[3]
  rho <- par[4]
  
  x_i <- x[i]
  y_i <- y[i]
  
  expbetalogxalpha <- exp(beta * log(x_i) - alpha)
  
  identical_part <- - 2 * (y_i - f(x_i, par))
  
  grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)
  grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)
  grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))
  grad_rho <- mean(identical_part / (1 + expbetalogxalpha))
  
  return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))
}

sgd <- function(
    par0,
    grad,
    N, # Sample size
    gamma, # Decay schedule or a fixed learning rate
    epoch = NULL,
    maxiter = 100, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    ...) {
  
  if (is.function(gamma)) gamma <- gamma(1:maxiter)
  gamma <- rep_len(gamma, maxiter)
  
  par <- par0
  
  for (n in 1:maxiter) {
    if (!is.null(cb)) cb$tracer()
    
    samp <- sampler(N)
    
    if (is.null(epoch)){
       for (j in 1:N) {
         i <- samp[j]
        par <- par - gamma[n] * grad(par, i, ...)
       }
    } else {
      par <- epoch(par, samp, gamma[n], ...)
    }
  }
  par
}

sgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = sim_1$x, y = sim_1$y)
})
```
---
###Implementing the gradient function in RCPP
```{r}
library(Rcpp)

cppFunction('
NumericVector gradient_rcpp(NumericVector par, NumericVector indices, 
NumericVector x, NumericVector y) {
  // Extract parameters
  double alpha = par[0];
  double beta = par[1];
  double gamma = par[2];
  double rho = par[3];

  // Initialize gradients
  double grad_alpha = 0.0;
  double grad_beta = 0.0;
  double grad_gamma = 0.0;
  double grad_rho = 0.0;

  int n = indices.size(); // Number of indices

  // Loop over the indices
  for (int idx = 0; idx < n; ++idx) {
    int i = indices[idx] - 1;  // Convert from r to c++ indexing

    // Get individual data point
    double x_i = x[i];
    double y_i = y[i];

    // Calculating f(x_i, par)
    double f_x_i = gamma + (rho - gamma) / (1 + exp(beta * log(x_i) - alpha));

    // Exponential term
    double expbetalogxalpha = exp(beta * log(x_i) - alpha);

    // Identical part used in gradients
    double identical_part = -2 * (y_i - f_x_i);

    // Accumulate gradients for all indices
    grad_alpha += (identical_part * (rho - gamma) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_beta += -(identical_part * (rho - gamma) * log(x_i) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_gamma += identical_part * (1 - 1 / (1 + expbetalogxalpha));
    grad_rho += identical_part / (1 + expbetalogxalpha);
  }

  // Return the mean of accumulated gradients
  return NumericVector::create(grad_alpha / n, grad_beta / n, grad_gamma / n, grad_rho / n);
}
')

```

```{r}
gradient(c(3,3,3,3), c(1,2), sim_1$x, sim_1$y)
gradient_rcpp(c(3,3,3,3), c(1,2), sim_1$x, sim_1$y)
```

---
###Benchmarking
Benchmarking the RCPP implementation against the R implementation
```{r,echo=F, warning  = FALSE, message = FALSE, fig.width=10, fig.height=4, fig.align='center'}
sim1 <- sim(param1, 100)

bench_results <- bench::mark(
    R = sgd(c(3,3,3,3), grad = gradient, N = 100, gamma = 0.01, x = sim1$x, y = sim_1$y),
    Rcpp = sgd(c(3,3,3,3), grad = gradient_rcpp, N = 100, gamma = 0.01, x = sim1$x, y = sim_1$y),
    iterations = 100,
    check = F
)
plot(bench_results)
```  
<br>
```{r,echo=F, warning  = FALSE, message = FALSE, fig.width=10, fig.height=4, fig.align='center'}
knitr::kable(data.frame(expression = c('R', 'Rcpp'), bench_results[,2:9]))
```

---
```{r,echo=T, warning  = FALSE, message = FALSE, fig.width=10, fig.height=4, fig.align='center'}
sim1 <- sim(param1, 1000)

bench_results <- bench::mark(
    R = sgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = sim1$x, y = sim_1$y),
    Rcpp = sgd(c(3,3,3,3), grad = gradient_rcpp, N = 1000, gamma = 0.01, x = sim1$x, y = sim_1$y),
    iterations = 10,
    check = F
)
plot(bench_results)

knitr::kable(data.frame(expression = c('R', 'Rcpp'), bench_results[,2:9]))
```

---
### Decay Schedule

```{r, echo=FALSE, warning=FALSE, message=FALSE}
### Source code

source("~/comp_stat/Stochastic_Optimization_ML/SGD.R")
source("~/comp_stat/Stochastic_Optimization_ML/sampler.R")
source("~/comp_stat/Stochastic_Optimization_ML/GD.R")
```

We implement a flexible three-parameter power law family of decay schedules

$$
\gamma_n = \frac{\gamma_0 K}{K + n^{a}}
$$

We try a decay schedules with $a = 1$, $K= 0.4$ and $\gamma_0 = 1$. A different way to specify the decay schedule is to specify a desired learning rate $\gamma_1$ which should be reached at iteration $n_1$. These specifications then determine the parameter $K$. We specify two decay schedules with $\gamma_0 = 1$, $n_1 = 100$, $a=2$. And $\gamma_1 = 0.1$ and $\gamma_1 = 0.01$. 

```{r, warning = FALSE, message = FALSE, echo = FALSE}
decay1 <- decay_scheduler(gamma0 = 1, a = 1, K = 0.4)
decay2 <- decay_scheduler(gamma0 = 1, a = 2, n1 = 150, gamma1 = 0.1)
decay3 <- decay_scheduler(gamma0 = 1, a = 2, n1 = 150, gamma1 = 0.01)


sgd_decay1 <- SGD(par0 = sv2, grad = grad, gamma = decay1, x = x, y = y, true_par = param$par)
sgd_decay2 <- SGD(par0 = sv2, grad = grad, gamma = decay2, x = x, y = y, true_par = param$par)
sgd_decay3 <- SGD(par0 = sv2, grad = grad, gamma = decay3, x = x, y = y, true_par = param$par)
```

```{r, hide = TRUE, warning = FALSE, message = FALSE, echo = FALSE, fig.width=10, fig.height=4, fig.align='center'}
ggplot() +
  geom_line(aes(x = plot_data(sgd_decay1)$.time, 
                y = plot_data(sgd_decay1)$loss, color = "a = 1, K = 0.4"))+
  geom_line(aes(x = plot_data(sgd_decay2)$.time, 
                y = plot_data(sgd_decay2)$loss, color = "a = 2, gamma1 = 0.1")) +
  geom_line(aes(x = plot_data(sgd_decay3)$.time, 
                y = plot_data(sgd_decay3)$loss, color = "a = 2, gamma1 = 0.01")) +
  scale_y_log10() +
  labs(title = "Loss vs Time", x = "Time", y = "Loss")+
  scale_color_manual(values = c("a = 1, K = 0.4" = "red", 
                                "a = 2, gamma1 = 0.1" = "orange", 
                                "a = 2, gamma1 = 0.01" = "black"))+
  labs(color = "Parameter values")

ggplot() +
  geom_line(aes(x = plot_data(sgd_decay1)$.time, 
                y = plot_data(sgd_decay1)$abs_dist_from_par, color = "a = 1, K = 0.4"))+
  geom_line(aes(x = plot_data(sgd_decay2)$.time, 
                y = plot_data(sgd_decay2)$abs_dist_from_par, color = "a = 2, gamma1 = 0.1")) +
  geom_line(aes(x = plot_data(sgd_decay3)$.time, 
                y = plot_data(sgd_decay3)$abs_dist_from_par, color = "a = 2, gamma1 = 0.01")) +
  scale_y_log10() +
  labs(title = "Abs. dist. to par vs Time", x = "Time", y = "Abs. dist. to par")+
  scale_color_manual(values = c("a = 1, K = 0.4" = "red", 
                                "a = 2, gamma1 = 0.1" = "orange", 
                                "a = 2, gamma1 = 0.01" = "black"),
                         labels = c(
      expression(a == 1 ~ ", " ~ K == 0.4), 
      expression(a == 2 ~ ", " ~ gamma[1] == 0.1), 
      expression(a == 2 ~ ", " ~ gamma[1] == 0.01)
    ))+
  labs(color = "Parameter values")

```
