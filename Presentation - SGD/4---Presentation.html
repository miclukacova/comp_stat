<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Log-logistic Dose-response Curves</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Rubjerg Hejstvig-Larsen (brf337)" />
    <meta name="author" content="Dina Gyberg Jensen (vbz248)" />
    <meta name="date" content="2024-10-23" />
    <script src="4---Presentation_files/header-attrs-2.26/header-attrs.js"></script>
    <link href="4---Presentation_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
    <script src="4---Presentation_files/htmlwidgets-1.6.4/htmlwidgets.js"></script>
    <script src="4---Presentation_files/jquery-1.12.4/jquery.min.js"></script>
    <script src="4---Presentation_files/d3-3.5.6/d3.min.js"></script>
    <link href="4---Presentation_files/profvis-0.3.6.9000/profvis.css" rel="stylesheet" />
    <script src="4---Presentation_files/profvis-0.3.6.9000/profvis.js"></script>
    <script src="4---Presentation_files/profvis-0.3.6.9000/scroll.js"></script>
    <link href="4---Presentation_files/highlight-6.2.0/textmate.css" rel="stylesheet" />
    <script src="4---Presentation_files/highlight-6.2.0/highlight.js"></script>
    <script src="4---Presentation_files/profvis-binding-0.3.8/profvis.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Log-logistic Dose-response Curves
]
.author[
### Christian Rubjerg Hejstvig-Larsen (brf337)
]
.author[
### Dina Gyberg Jensen (vbz248)
]
.institute[
### University of Copenhagen
]
.date[
### 2024-10-23
]

---

&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content &gt; h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
&lt;/style&gt;





###Introduction
Objective is to use stochastic optimization to estimate the parameters of a log-logistic dose-response model using non-linear least squares estimation. That is obtaining the parameters `\(\alpha, \beta, \gamma, \rho\)` that minimize the loss function:
`$$L(X,(\alpha,\beta,\gamma,\rho))=\frac{1}{N}\sum_{i=1}^N\left( y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)^2$$`
&lt;br&gt;
Where the response is given by:
`$$Y_i = f(x_i| \alpha, \beta,\gamma,\rho) + \varepsilon_i \hskip5mm \text{with}\hskip2mm \varepsilon_i \stackrel{iid}{\sim} N(0,\omega^2)$$`
And the log-logistic dose-response model is given by:
`$$f(x_i| \alpha, \beta,\gamma,\rho) = \gamma + \frac{\rho - \gamma}{1 + \exp(\beta \log(x_i) - \alpha)}$$`

---
###Stochastic Gradient Descent
Start out implementing a standard version of the stochastic gradient descent algorithm (SGD). The gradient in a single point is given by
$$-2\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)  $$

&lt;br&gt;
Where `\(\nabla f(x_i| \alpha, \beta,\gamma,\rho)\)` is given by
`$$\nabla f(x_i| \alpha, \beta,\gamma,\rho) = \begin{pmatrix}
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\\
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\cdot\log(x_i)\\
  1-\frac{1}{1+\exp(\beta\log(x_i)-\alpha)} \\
  \frac{1}{1+\exp(\beta\log(x_i)-\alpha)}
\end{pmatrix}$$`
So the update scheme becomes:
`$$\theta_{t+1} = \theta_t +2 \gamma_t\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)$$`
Where `\(\theta_t = (\alpha_t, \beta_t, \gamma_t, \rho_t)\)` and `\(\gamma_t\)` is the learning rate at time `\(t\)`.
---
###Implementation 
The gradient function and `\(f\)` are implemented seperately

```r
sgd &lt;- function(
    par,
    grad, # Function of parameter and observation index
    gamma, # Decay schedule or a fixed learning rate
    maxiter = 150, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    epoch = vanilla,
    m = 1, # Batch size
    x,
    y,
    ...) {
  
  n &lt;- length(x)
  gamma &lt;- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)
  
  for (k in 1:maxiter) {
    
    if (!is.null(cb)) cb()
    samp &lt;- sampler(n)
    par &lt;- epoch(par = par, samp = samp, gamma = gamma[k], 
                 grad = grad, n = n, x = x, y = y, m = m)
    
  }
  par
}

vanilla &lt;- function(par, samp, gamma, grad, n, x, y, ...){
  for (j in 1:n) {
    i &lt;- samp[j]
    par &lt;- par - gamma * grad(par, x[i], y[i])
  }
  return(par)
}
```

---
###Sampling


 
 - Implemented an S3 object `parameters` and a method `sim` to simulate data from the log-logistic dose-response model.
 
 - 2 different types of sampling methods
 
  - `grid_sample`: samples `\(X\)` from a fixed grid of points `\((e, e^2,..., e^{15})\)`.
  
  - `gauss_sample`: samples `\(\log(X)\)` from `\(\mathcal{N}(0, \omega^2)\)`. Note `\(\omega\)` may not be too large.

- The method also allows for scaling the data, which is useful for optimization.


```r
sim(parameters(1, 1, 1, 1), N = 100, grid = TRUE, scale = TRUE)
```
 - True parameters have been set to `c(2,5,1,2)` and the data is simulated with `N = 5000`.
 


---
### Test of Algorithm 

We do a naive test of the algorithm, and run the algorithm with 3 different starting values: the true values `\((2,5,1,2)\)`, `\((1,1,1,1)\)` and random values `\((1.88, 0.31, 3.64, 2.29)\)`. Step size is `\(0.005\)`, maximal number of itereations `\(300\)`.  



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---
### Test of Algorithm 

We obtain quite different convergence schemes. 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

Table: Different starting values

|Par   | True vals| (1, 1, 1, 1)| Random vals|
|:-----|---------:|------------:|-----------:|
|α (2) | 2.0997154|     2.089024|   -2.082571|
|β (5) | 5.3782134|     5.345995|   -5.346636|
|γ (1) | 0.9709541|     1.006829|    1.933317|
|ρ (2) | 2.0427093|     1.985494|    1.049497|

---
###Profiling the algorithm
<div class="profvis html-widget html-fill-item" id="htmlwidget-d342331dd69a16b5bd3d" style="width:100%;height:600px;"></div>
<script type="application/json" data-for="htmlwidget-d342331dd69a16b5bd3d">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,7,7,8,8,8,8,9,9,9,10,10,10,11,11,11,12,12,12,13,13,13,14,14,14,15,15,15,16,16,16,16,17,17,17,18,18,18,19,19,19,20,20,20,21,21,21,21,22,22,22,22,23,23,23,23,24,24,24,25,25,25,26,26,26,27,27,28,28,28,29,29,29,30,31,31,31,31,32,32,32,33,33,33,33,34,35,35,35,36,36,36,37,37,38,38,38,39,39,39,40,40,40,41,41,41,42,42,42,43,43,43,44,44,44,45,45,45,46,46,46,47,47,47,48,48,48,48,49,49,49,50,50,50,51,51,51,51,52,52,52,52,53,53,53,54,54,54,55,55,56,56,57,57,57,58,58,58,59,59,59,60,60,60,60,61,61,61,61,62,62,62,63,63,63,63,64,64,64,65,65,65,66,66,66,67,67,67,68,68,68,68,69,69,69,69,70,70,70,71,71,71,72,72,72,73,73,73,74,74,74,74,75,75,75,76,76,76,77,77,77,78,78,78,79,79,79,80,80,80,81,81,81,82,82,82,83,83,83,84,84,84,85,85,85,86,86,86,86,87,87,87,88,88,88,89,89,89,90,90,90,91,91,91,91,92,92,92,93,93,94,94,94,94,95,95,95,95,96,96,96,97,97,97,98,98,98,99,99,99,99,100,100,100,101,101,101,102,102,103,103,103,104,104,104,105,105,105,106,107,107,107,107,108,108,108,109,109,109,110,110,110,111,111,111,112,112,112,113,113,113,114,114,114,114,115,115,115,116,116,116,117,117,117,118,118,118,119,119,119,120,120,121,121,121,122,122,122,123,123,123,124,124,124,125,125,125,125,126,126,126,127,127,127,127,128,128,128,129,129,129,130,130,130,131,131,131,132,132,132,132,133,133,133,134,134,134,134,135,135,135,136,136,136,137,137,137,138,138,138,138,139,139,139,140,140,140,140,141,141,141,142,142,142,143,143,143,143,144,144,144,145,145,145,145,146,146,146,147,147,147,148,148,148,149,149,149,149,149,150,150,150,151,151,151,151,152,152,152,152,153,153,153,153,154,155,155,155,155,156,156,156,157,157,157,158,158,158,159,159,159,160,160,160,161,161,161,162,162,162,163,163,163,164,164,164,165,165,165,165,166,166,166,167,167,167,167,168,168,168,168,169,169,169,170,170,170,170,171,171,171,172,172,172,173,173,173,174,174,174,175,175,175,176,176,176,177,177,177,178,178,178,179,179,179,180,180,181,181,181,182,182,182,183,183,183,184,185,185,185,185,186,186,186,187,187,188,188,188,189,189,190,191,191,191,191,192,192,192,193,193,193,194,194,195,195,195,195,196,196,196,197,197,198,198,198,199,199,199,200,200,200,201,201,201,201,202,202,202,203,204,204,204,205,205,205,206,206,206,206,207,207,207,208,208,208,208,209,209,209,209,210,210,210,211,211,211,212,212,212,213,213,213,214,214,214,215,215,215,215,216,216,216,217,217,217,217,218,218,218,219,219,219,219,220,220,220,220,221,221,221,222,222],"depth":[22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,43,42,41,40,39,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,1,4,3,2,1,3,2,1,4,3,2,1,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,5,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,1,4,3,2,1,3,2,1,2,1,3,2,1,2,1,1,4,3,2,1,3,2,1,3,2,1,2,1,4,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,2,1],"label":["putconst","cb$putcode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","findCenvVar","getInlineInfo","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpForBody","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpForBody","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","$","findCenvVar","getInlineInfo","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","make.codeBuf","genCode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","make.argContext","cmpPrim2","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","genCode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpPrim1","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","unique.default","findLocalsList","funEnv","make.functionContext","cmpfun","compiler:::tryCmpfun","i <- samp[j]","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","<GC>","mean","grad","sgd","<GC>","mean","grad","sgd","gamma <- par[3]","f","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","length","f <- function(x, par){","f","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","is.na","mean","grad","sgd","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","beta <- par[2]","grad","sgd","mean.default","grad","sgd","<GC>","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","gamma <- par[3]","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","<GC>","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","f","grad","sgd","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","gradient <- function(par, i, x, y,...){","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","beta <- par[2]","f","grad","sgd","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","i <- samp[j]","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","length","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","<GC>","mean","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","isTRUE","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","<GC>","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean.default","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","x_i <- x[i]","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","<GC>","isTRUE","mean.default","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","isTRUE","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","is.na","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","gamma <- par[3]","f","grad","sgd","mean.default","grad","sgd","<GC>","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean","grad","sgd","length","<GC>","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","length","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","<GC>","length","isTRUE","mean.default","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","is.na","x_i <- x[i]","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,null,1,1,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,null,1,1,null,null,1,1,null,1,1,1,null,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,null,1,1,null,1,1,null,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,null,1,1,1,1,1,1,1,1,1,null,null,1,1,null,null,1,1,null,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,null,1,1,null,null,null,null,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,null,1,1,null,1,1,null,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,67,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,67,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,24,57,67,null,null,58,67,26,58,67,null,58,67,28,58,67,26,58,67,null,58,67,26,58,67,29,58,67,null,null,58,67,26,58,67,27,58,67,null,58,67,28,58,67,null,28,58,67,null,28,58,67,5,24,58,67,null,58,67,null,58,67,null,58,67,58,67,27,58,67,24,58,67,null,2,24,58,67,null,58,67,null,null,58,67,null,27,58,67,null,58,67,58,67,26,58,67,null,58,67,29,58,67,26,58,67,null,58,67,22,58,67,26,58,67,null,58,67,29,58,67,29,58,67,28,28,58,67,15,58,67,null,58,67,null,26,58,67,null,null,58,67,27,58,67,16,58,67,58,67,58,67,29,58,67,null,58,67,null,58,67,null,null,58,67,24,24,58,67,29,58,67,8,24,58,67,null,58,67,27,58,67,31,58,67,29,58,67,null,null,58,67,null,null,58,67,null,58,67,27,58,67,29,58,67,27,58,67,null,null,58,67,13,58,67,null,58,67,28,58,67,27,58,67,27,58,67,27,58,67,26,58,67,29,58,67,null,58,67,29,58,67,28,58,67,4,24,58,67,28,58,67,29,58,67,null,58,67,null,58,67,null,null,58,67,null,58,67,57,67,29,29,58,67,26,26,58,67,26,58,67,29,58,67,null,58,67,26,26,58,67,29,58,67,26,58,67,58,67,24,58,67,null,58,67,28,58,67,null,null,null,58,67,null,58,67,27,58,67,29,58,67,27,58,67,null,58,67,27,58,67,null,28,58,67,31,58,67,null,58,67,26,58,67,null,58,67,27,58,67,58,67,null,58,67,26,58,67,null,58,67,29,58,67,26,26,58,67,29,58,67,8,24,58,67,26,58,67,null,58,67,27,58,67,28,58,67,null,null,58,67,27,58,67,null,null,58,67,null,58,67,null,58,67,31,58,67,null,null,58,67,29,58,67,null,26,58,67,27,58,67,31,58,67,27,27,58,67,null,58,67,8,24,58,67,19,58,67,null,58,67,29,58,67,null,null,null,58,67,26,58,67,26,26,58,67,null,null,58,67,null,null,58,67,null,null,null,58,67,29,58,67,26,58,67,29,58,67,28,58,67,28,58,67,24,58,67,26,58,67,26,58,67,null,58,67,5,24,58,67,null,58,67,null,27,58,67,26,26,58,67,27,58,67,28,28,58,67,26,58,67,26,58,67,28,58,67,28,58,67,null,58,67,null,58,67,null,58,67,29,58,67,26,58,67,58,67,26,58,67,29,58,67,27,58,67,null,null,26,58,67,29,58,67,58,67,28,58,67,58,67,null,26,26,58,67,26,58,67,null,58,67,null,null,null,null,58,67,29,58,67,58,67,null,58,67,28,58,67,29,58,67,null,null,58,67,null,58,67,null,19,58,67,null,58,67,null,null,58,67,29,58,67,8,24,58,67,null,null,58,67,28,58,67,null,58,67,22,58,67,null,58,67,28,58,67,27,27,58,67,null,58,67,null,null,58,67,26,58,67,null,null,58,67,29,29,58,67,24,58,67,58,67],"memalloc":[33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.32283020019531,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.43336486816406,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.62150573730469,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.72270202636719,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,33.85794067382812,34.03762817382812,34.03762817382812,34.03762817382812,34.03762817382812,34.03762817382812,34.03762817382812,34.33043670654297,34.33043670654297,34.58206176757812,34.58206176757812,34.58206176757812,34.58206176757812,34.78254699707031,34.78254699707031,34.78254699707031,34.98134613037109,34.98134613037109,34.98134613037109,35.19290924072266,35.19290924072266,35.19290924072266,33.31791687011719,33.31791687011719,33.31791687011719,33.49896240234375,33.49896240234375,33.49896240234375,33.73963928222656,33.73963928222656,33.73963928222656,33.98981475830078,33.98981475830078,33.98981475830078,34.17581176757812,34.17581176757812,34.17581176757812,34.17581176757812,34.34727478027344,34.34727478027344,34.34727478027344,34.56849670410156,34.56849670410156,34.56849670410156,34.82990264892578,34.82990264892578,34.82990264892578,35.11891937255859,35.11891937255859,35.11891937255859,33.24672698974609,33.24672698974609,33.24672698974609,33.24672698974609,33.24672698974609,33.24672698974609,33.24672698974609,33.24672698974609,33.42095184326172,33.42095184326172,33.42095184326172,33.42095184326172,33.62460327148438,33.62460327148438,33.62460327148438,33.74395751953125,33.74395751953125,33.74395751953125,33.90736389160156,33.90736389160156,33.90736389160156,34.12606048583984,34.12606048583984,34.32445526123047,34.32445526123047,34.32445526123047,34.50342559814453,34.50342559814453,34.50342559814453,34.69815063476562,34.92514038085938,34.92514038085938,34.92514038085938,34.92514038085938,35.13992309570312,35.13992309570312,35.13992309570312,33.31254577636719,33.31254577636719,33.31254577636719,33.31254577636719,33.55644226074219,33.7608642578125,33.7608642578125,33.7608642578125,33.98725128173828,33.98725128173828,33.98725128173828,34.25792694091797,34.25792694091797,34.47466278076172,34.47466278076172,34.47466278076172,34.69744110107422,34.69744110107422,34.69744110107422,34.90355682373047,34.90355682373047,34.90355682373047,35.12801361083984,35.12801361083984,35.12801361083984,33.32295989990234,33.32295989990234,33.32295989990234,33.531005859375,33.531005859375,33.531005859375,33.75962066650391,33.75962066650391,33.75962066650391,33.96520233154297,33.96520233154297,33.96520233154297,34.20114135742188,34.20114135742188,34.20114135742188,34.42235565185547,34.42235565185547,34.42235565185547,34.64701843261719,34.64701843261719,34.64701843261719,34.64701843261719,34.898193359375,34.898193359375,34.898193359375,35.09588623046875,35.09588623046875,35.09588623046875,35.23854064941406,35.23854064941406,35.23854064941406,35.23854064941406,33.45639801025391,33.45639801025391,33.45639801025391,33.45639801025391,33.68434906005859,33.68434906005859,33.68434906005859,33.87919616699219,33.87919616699219,33.87919616699219,34.09660339355469,34.09660339355469,34.34072113037109,34.34072113037109,34.60486602783203,34.60486602783203,34.60486602783203,34.82463073730469,34.82463073730469,34.82463073730469,35.03463745117188,35.03463745117188,35.03463745117188,35.23470306396484,35.23470306396484,35.23470306396484,35.23470306396484,33.45420837402344,33.45420837402344,33.45420837402344,33.45420837402344,33.67524719238281,33.67524719238281,33.67524719238281,33.89884185791016,33.89884185791016,33.89884185791016,33.89884185791016,34.16370391845703,34.16370391845703,34.16370391845703,34.37611389160156,34.37611389160156,34.37611389160156,34.63319396972656,34.63319396972656,34.63319396972656,34.84325408935547,34.84325408935547,34.84325408935547,35.11323547363281,35.11323547363281,35.11323547363281,35.11323547363281,33.31692504882812,33.31692504882812,33.31692504882812,33.31692504882812,33.55328369140625,33.55328369140625,33.55328369140625,33.78172302246094,33.78172302246094,33.78172302246094,34.02964782714844,34.02964782714844,34.02964782714844,34.27852630615234,34.27852630615234,34.27852630615234,34.52035522460938,34.52035522460938,34.52035522460938,34.52035522460938,34.74900817871094,34.74900817871094,34.74900817871094,34.95670318603516,34.95670318603516,34.95670318603516,35.17237854003906,35.17237854003906,35.17237854003906,33.34912872314453,33.34912872314453,33.34912872314453,33.56622314453125,33.56622314453125,33.56622314453125,33.73325347900391,33.73325347900391,33.73325347900391,33.96673583984375,33.96673583984375,33.96673583984375,34.19266510009766,34.19266510009766,34.19266510009766,34.471435546875,34.471435546875,34.471435546875,34.68348693847656,34.68348693847656,34.68348693847656,34.93943023681641,34.93943023681641,34.93943023681641,35.15528106689453,35.15528106689453,35.15528106689453,35.15528106689453,33.34717559814453,33.34717559814453,33.34717559814453,33.57575225830078,33.57575225830078,33.57575225830078,33.83575439453125,33.83575439453125,33.83575439453125,34.06413269042969,34.06413269042969,34.06413269042969,34.19226837158203,34.19226837158203,34.19226837158203,34.19226837158203,34.4072265625,34.4072265625,34.4072265625,34.68300628662109,34.68300628662109,34.88796997070312,34.88796997070312,34.88796997070312,34.88796997070312,35.12584686279297,35.12584686279297,35.12584686279297,35.12584686279297,33.30892944335938,33.30892944335938,33.30892944335938,33.5347900390625,33.5347900390625,33.5347900390625,33.76265716552734,33.76265716552734,33.76265716552734,34.00659942626953,34.00659942626953,34.00659942626953,34.00659942626953,34.23354339599609,34.23354339599609,34.23354339599609,34.45442962646484,34.45442962646484,34.45442962646484,34.6951904296875,34.6951904296875,34.92555236816406,34.92555236816406,34.92555236816406,35.15187835693359,35.15187835693359,35.15187835693359,33.33422088623047,33.33422088623047,33.33422088623047,33.55168914794922,33.7911376953125,33.7911376953125,33.7911376953125,33.7911376953125,34.03615570068359,34.03615570068359,34.03615570068359,34.2452392578125,34.2452392578125,34.2452392578125,34.48207855224609,34.48207855224609,34.48207855224609,34.67886352539062,34.67886352539062,34.67886352539062,34.90503692626953,34.90503692626953,34.90503692626953,35.12163543701172,35.12163543701172,35.12163543701172,33.31041717529297,33.31041717529297,33.31041717529297,33.31041717529297,33.60912322998047,33.60912322998047,33.60912322998047,33.83787536621094,33.83787536621094,33.83787536621094,34.05661773681641,34.05661773681641,34.05661773681641,34.27469635009766,34.27469635009766,34.27469635009766,34.52970123291016,34.52970123291016,34.52970123291016,34.75334167480469,34.75334167480469,34.98587799072266,34.98587799072266,34.98587799072266,35.21643829345703,35.21643829345703,35.21643829345703,33.38986968994141,33.38986968994141,33.38986968994141,33.62290191650391,33.62290191650391,33.62290191650391,33.85486602783203,33.85486602783203,33.85486602783203,33.85486602783203,34.0382080078125,34.0382080078125,34.0382080078125,34.21648406982422,34.21648406982422,34.21648406982422,34.21648406982422,34.40888214111328,34.40888214111328,34.40888214111328,34.65583801269531,34.65583801269531,34.65583801269531,34.85081481933594,34.85081481933594,34.85081481933594,35.01759338378906,35.01759338378906,35.01759338378906,35.21522521972656,35.21522521972656,35.21522521972656,35.21522521972656,33.50312805175781,33.50312805175781,33.50312805175781,33.77792358398438,33.77792358398438,33.77792358398438,33.77792358398438,34.04216003417969,34.04216003417969,34.04216003417969,34.34510803222656,34.34510803222656,34.34510803222656,34.57290649414062,34.57290649414062,34.57290649414062,34.8319091796875,34.8319091796875,34.8319091796875,34.8319091796875,35.07282257080078,35.07282257080078,35.07282257080078,35.29113006591797,35.29113006591797,35.29113006591797,35.29113006591797,33.48196411132812,33.48196411132812,33.48196411132812,33.72032928466797,33.72032928466797,33.72032928466797,33.94767761230469,33.94767761230469,33.94767761230469,33.94767761230469,34.20465087890625,34.20465087890625,34.20465087890625,34.43651580810547,34.43651580810547,34.43651580810547,34.43651580810547,34.66958618164062,34.66958618164062,34.66958618164062,34.88315582275391,34.88315582275391,34.88315582275391,35.10167694091797,35.10167694091797,35.10167694091797,35.29739379882812,35.29739379882812,35.29739379882812,35.29739379882812,35.29739379882812,33.54000854492188,33.54000854492188,33.54000854492188,33.79692077636719,33.79692077636719,33.79692077636719,33.79692077636719,34.00157165527344,34.00157165527344,34.00157165527344,34.00157165527344,34.25774383544922,34.25774383544922,34.25774383544922,34.25774383544922,34.48563385009766,34.73193359375,34.73193359375,34.73193359375,34.73193359375,34.98951721191406,34.98951721191406,34.98951721191406,35.20717620849609,35.20717620849609,35.20717620849609,33.38887786865234,33.38887786865234,33.38887786865234,33.61962890625,33.61962890625,33.61962890625,33.86270904541016,33.86270904541016,33.86270904541016,34.08145141601562,34.08145141601562,34.08145141601562,34.29066467285156,34.29066467285156,34.29066467285156,34.50409698486328,34.50409698486328,34.50409698486328,34.70965576171875,34.70965576171875,34.70965576171875,34.95552825927734,34.95552825927734,34.95552825927734,34.95552825927734,35.15762329101562,35.15762329101562,35.15762329101562,35.30996704101562,35.30996704101562,35.30996704101562,35.30996704101562,33.55601501464844,33.55601501464844,33.55601501464844,33.55601501464844,33.79695129394531,33.79695129394531,33.79695129394531,34.04426574707031,34.04426574707031,34.04426574707031,34.04426574707031,34.27391052246094,34.27391052246094,34.27391052246094,34.53723907470703,34.53723907470703,34.53723907470703,34.72828674316406,34.72828674316406,34.72828674316406,34.97788238525391,34.97788238525391,34.97788238525391,35.21028137207031,35.21028137207031,35.21028137207031,33.38264465332031,33.38264465332031,33.38264465332031,33.61366271972656,33.61366271972656,33.61366271972656,33.84972381591797,33.84972381591797,33.84972381591797,34.08705139160156,34.08705139160156,34.08705139160156,34.22106170654297,34.22106170654297,34.44146728515625,34.44146728515625,34.44146728515625,34.64678955078125,34.64678955078125,34.64678955078125,34.88020324707031,34.88020324707031,34.88020324707031,35.10196685791016,35.32245635986328,35.32245635986328,35.32245635986328,35.32245635986328,33.51129913330078,33.51129913330078,33.51129913330078,33.75687408447266,33.75687408447266,33.97594451904297,33.97594451904297,33.97594451904297,34.22984313964844,34.22984313964844,34.44394683837891,34.66128540039062,34.66128540039062,34.66128540039062,34.66128540039062,34.91262054443359,34.91262054443359,34.91262054443359,35.13668060302734,35.13668060302734,35.13668060302734,35.32865905761719,35.32865905761719,33.59620666503906,33.59620666503906,33.59620666503906,33.59620666503906,33.81689453125,33.81689453125,33.81689453125,34.05699157714844,34.05699157714844,34.28765106201172,34.28765106201172,34.28765106201172,34.51044464111328,34.51044464111328,34.51044464111328,34.75827789306641,34.75827789306641,34.75827789306641,34.97200775146484,34.97200775146484,34.97200775146484,34.97200775146484,35.22145843505859,35.22145843505859,35.22145843505859,33.40488433837891,33.64240264892578,33.64240264892578,33.64240264892578,33.86138153076172,33.86138153076172,33.86138153076172,34.09420776367188,34.09420776367188,34.09420776367188,34.09420776367188,34.35281372070312,34.35281372070312,34.35281372070312,34.59070587158203,34.59070587158203,34.59070587158203,34.59070587158203,34.84122467041016,34.84122467041016,34.84122467041016,34.84122467041016,35.06571960449219,35.06571960449219,35.06571960449219,35.27339172363281,35.27339172363281,35.27339172363281,33.32013702392578,33.32013702392578,33.32013702392578,33.62486267089844,33.62486267089844,33.62486267089844,33.85605621337891,33.85605621337891,33.85605621337891,34.05738067626953,34.05738067626953,34.05738067626953,34.05738067626953,34.25431060791016,34.25431060791016,34.25431060791016,34.528564453125,34.528564453125,34.528564453125,34.528564453125,34.72726440429688,34.72726440429688,34.72726440429688,34.95981597900391,34.95981597900391,34.95981597900391,34.95981597900391,35.22197723388672,35.22197723388672,35.22197723388672,35.22197723388672,33.40273284912109,33.40273284912109,33.40273284912109,33.62648010253906,33.62648010253906],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.11053466796875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.188140869140625,0,0,0,0,0,0,0,0,0,0,0,0,0.1011962890625,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1352386474609375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1796875,0,0,0,0,0,0.2928085327148438,0,0.2516250610351562,0,0,0,0.2004852294921875,0,0,0.1987991333007812,0,0,0.2115631103515625,0,0,-1.874992370605469,0,0,0.1810455322265625,0,0,0.2406768798828125,0,0,0.2501754760742188,0,0,0.1859970092773438,0,0,0,0.1714630126953125,0,0,0.221221923828125,0,0,0.2614059448242188,0,0,0.2890167236328125,0,0,-1.8721923828125,0,0,0,0,0,0,0,0.174224853515625,0,0,0,0.2036514282226562,0,0,0.119354248046875,0,0,0.1634063720703125,0,0,0.2186965942382812,0,0.198394775390625,0,0,0.1789703369140625,0,0,0.1947250366210938,0.22698974609375,0,0,0,0.21478271484375,0,0,-1.827377319335938,0,0,0,0.243896484375,0.2044219970703125,0,0,0.2263870239257812,0,0,0.2706756591796875,0,0.21673583984375,0,0,0.2227783203125,0,0,0.20611572265625,0,0,0.224456787109375,0,0,-1.8050537109375,0,0,0.2080459594726562,0,0,0.2286148071289062,0,0,0.2055816650390625,0,0,0.2359390258789062,0,0,0.2212142944335938,0,0,0.2246627807617188,0,0,0,0.2511749267578125,0,0,0.19769287109375,0,0,0.1426544189453125,0,0,0,-1.782142639160156,0,0,0,0.2279510498046875,0,0,0.1948471069335938,0,0,0.2174072265625,0,0.2441177368164062,0,0.2641448974609375,0,0,0.2197647094726562,0,0,0.2100067138671875,0,0,0.2000656127929688,0,0,0,-1.780494689941406,0,0,0,0.221038818359375,0,0,0.2235946655273438,0,0,0,0.264862060546875,0,0,0.2124099731445312,0,0,0.257080078125,0,0,0.2100601196289062,0,0,0.2699813842773438,0,0,0,-1.796310424804688,0,0,0,0.236358642578125,0,0,0.2284393310546875,0,0,0.2479248046875,0,0,0.2488784790039062,0,0,0.2418289184570312,0,0,0,0.2286529541015625,0,0,0.2076950073242188,0,0,0.2156753540039062,0,0,-1.823249816894531,0,0,0.2170944213867188,0,0,0.1670303344726562,0,0,0.2334823608398438,0,0,0.2259292602539062,0,0,0.2787704467773438,0,0,0.2120513916015625,0,0,0.2559432983398438,0,0,0.215850830078125,0,0,0,-1.80810546875,0,0,0.22857666015625,0,0,0.2600021362304688,0,0,0.2283782958984375,0,0,0.1281356811523438,0,0,0,0.2149581909179688,0,0,0.2757797241210938,0,0.2049636840820312,0,0,0,0.2378768920898438,0,0,0,-1.816917419433594,0,0,0.225860595703125,0,0,0.2278671264648438,0,0,0.2439422607421875,0,0,0,0.2269439697265625,0,0,0.22088623046875,0,0,0.2407608032226562,0,0.2303619384765625,0,0,0.2263259887695312,0,0,-1.817657470703125,0,0,0.21746826171875,0.2394485473632812,0,0,0,0.2450180053710938,0,0,0.2090835571289062,0,0,0.2368392944335938,0,0,0.1967849731445312,0,0,0.2261734008789062,0,0,0.2165985107421875,0,0,-1.81121826171875,0,0,0,0.2987060546875,0,0,0.2287521362304688,0,0,0.2187423706054688,0,0,0.21807861328125,0,0,0.2550048828125,0,0,0.2236404418945312,0,0.2325363159179688,0,0,0.230560302734375,0,0,-1.826568603515625,0,0,0.2330322265625,0,0,0.231964111328125,0,0,0,0.1833419799804688,0,0,0.1782760620117188,0,0,0,0.1923980712890625,0,0,0.2469558715820312,0,0,0.194976806640625,0,0,0.166778564453125,0,0,0.1976318359375,0,0,0,-1.71209716796875,0,0,0.2747955322265625,0,0,0,0.2642364501953125,0,0,0.302947998046875,0,0,0.2277984619140625,0,0,0.259002685546875,0,0,0,0.2409133911132812,0,0,0.2183074951171875,0,0,0,-1.809165954589844,0,0,0.2383651733398438,0,0,0.2273483276367188,0,0,0,0.2569732666015625,0,0,0.2318649291992188,0,0,0,0.2330703735351562,0,0,0.2135696411132812,0,0,0.2185211181640625,0,0,0.1957168579101562,0,0,0,0,-1.75738525390625,0,0,0.2569122314453125,0,0,0,0.20465087890625,0,0,0,0.2561721801757812,0,0,0,0.2278900146484375,0.2462997436523438,0,0,0,0.2575836181640625,0,0,0.2176589965820312,0,0,-1.81829833984375,0,0,0.2307510375976562,0,0,0.2430801391601562,0,0,0.2187423706054688,0,0,0.2092132568359375,0,0,0.2134323120117188,0,0,0.2055587768554688,0,0,0.2458724975585938,0,0,0,0.2020950317382812,0,0,0.15234375,0,0,0,-1.753952026367188,0,0,0,0.240936279296875,0,0,0.247314453125,0,0,0,0.229644775390625,0,0,0.2633285522460938,0,0,0.1910476684570312,0,0,0.2495956420898438,0,0,0.2323989868164062,0,0,-1.82763671875,0,0,0.23101806640625,0,0,0.2360610961914062,0,0,0.2373275756835938,0,0,0.1340103149414062,0,0.2204055786132812,0,0,0.205322265625,0,0,0.2334136962890625,0,0,0.2217636108398438,0.220489501953125,0,0,0,-1.8111572265625,0,0,0.245574951171875,0,0.2190704345703125,0,0,0.2538986206054688,0,0.2141036987304688,0.2173385620117188,0,0,0,0.2513351440429688,0,0,0.22406005859375,0,0,0.1919784545898438,0,-1.732452392578125,0,0,0,0.2206878662109375,0,0,0.2400970458984375,0,0.2306594848632812,0,0,0.2227935791015625,0,0,0.247833251953125,0,0,0.2137298583984375,0,0,0,0.24945068359375,0,0,-1.816574096679688,0.237518310546875,0,0,0.2189788818359375,0,0,0.2328262329101562,0,0,0,0.25860595703125,0,0,0.2378921508789062,0,0,0,0.250518798828125,0,0,0,0.2244949340820312,0,0,0.207672119140625,0,0,-1.953254699707031,0,0,0.3047256469726562,0,0,0.2311935424804688,0,0,0.201324462890625,0,0,0,0.196929931640625,0,0,0.2742538452148438,0,0,0,0.198699951171875,0,0,0.2325515747070312,0,0,0,0.2621612548828125,0,0,0,-1.819244384765625,0,0,0.2237472534179688,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"profvis({\nf <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))\n}\n\n\n\ngradient <- function(par, i, x, y,...){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  x_i <- x[i]\n  y_i <- y[i]\n  \n  expbetalogxalpha <- exp(beta * log(x_i) - alpha)\n  \n  identical_part <- - 2 * (y_i - f(x_i, par))\n  \n  grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))\n  grad_rho <- mean(identical_part / (1 + expbetalogxalpha))\n  \n  return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))\n}\n\nsgd <- function(\n    par0,\n    grad,\n    N, # Sample size\n    gamma, # Decay schedule or a fixed learning rate\n    epoch = NULL,\n    maxiter = 100, # Max epoch iterations\n    sampler = sample, # How data is resampled. Default is a random permutation\n    cb = NULL,\n    ...) {\n  \n  if (is.function(gamma)) gamma <- gamma(1:maxiter)\n  gamma <- rep_len(gamma, maxiter)\n  \n  par <- par0\n  \n  for (n in 1:maxiter) {\n    if (!is.null(cb)) cb$tracer()\n    \n    samp <- sampler(N)\n    \n    if (is.null(epoch)){\n       for (j in 1:N) {\n         i <- samp[j]\n        par <- par - gamma[n] * grad(par, i, ...)\n       }\n    } else {\n      par <- epoch(par, samp, gamma[n], ...)\n    }\n  }\n  par\n}\n\nsgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = x, y = y)\n})","normpath":"<expr>"}],"prof_output":"/var/folders/kq/0f1vjrnj2xn0q3t1270kq05r0000gn/T//RtmpkjXqLC/file219da7dd56d.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
---
###Implementing the gradient function in RCPP

```r
library(Rcpp)

cppFunction('
NumericVector gradient_rcpp(NumericVector par, NumericVector indices, 
NumericVector x, NumericVector y) {
  // Extract parameters
  double alpha = par[0];
  double beta = par[1];
  double gamma = par[2];
  double rho = par[3];

  // Initialize gradients
  double grad_alpha = 0.0;
  double grad_beta = 0.0;
  double grad_gamma = 0.0;
  double grad_rho = 0.0;

  int n = indices.size(); // Number of indices

  // Loop over the indices
  for (int idx = 0; idx &lt; n; ++idx) {
    int i = indices[idx] - 1;  // Convert from r to c++ indexing

    // Get individual data point
    double x_i = x[i];
    double y_i = y[i];

    // Calculating f(x_i, par)
    double f_x_i = gamma + (rho - gamma) / (1 + exp(beta * log(x_i) - alpha));

    // Exponential term
    double expbetalogxalpha = exp(beta * log(x_i) - alpha);

    // Identical part used in gradients
    double identical_part = -2 * (y_i - f_x_i);

    // Accumulate gradients for all indices
    grad_alpha += (identical_part * (rho - gamma) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_beta += -(identical_part * (rho - gamma) * log(x_i) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_gamma += identical_part * (1 - 1 / (1 + expbetalogxalpha));
    grad_rho += identical_part / (1 + expbetalogxalpha);
  }

  // Return the mean of accumulated gradients
  return NumericVector::create(grad_alpha / n, grad_beta / n, grad_gamma / n, grad_rho / n);
}
')
```


```r
gradient(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

```r
gradient_rcpp(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

---
###Benchmarking
Benchmarking the RCPP implementation against the R implementation
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

|expression |     min|  median|   itr.sec| mem_alloc|   gc.sec| n_itr| n_gc| total_time|
|:----------|-------:|-------:|---------:|---------:|--------:|-----:|----:|----------:|
|R          | 220.5ms| 254.6ms|  3.390574|   338.3KB|  8.44253|   100|  249|     29.49s|
|Rcpp       |  36.3ms|  46.6ms| 21.027343|    24.7MB| 13.03695|   100|   62|      4.76s|

---

```r
sim1 &lt;- data[1:1000,]

bench_results &lt;- bench::mark(
    R = sgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = sim1$x, y = sim1$y),
    Rcpp = sgd(c(3,3,3,3), grad = gradient_rcpp, N = 1000, gamma = 0.01, x = sim1$x, y = sim1$y),
    iterations = 10,
    check = F
)
plot(bench_results)
```

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

```r
knitr::kable(data.frame(expression = c('R', 'Rcpp'), bench_results[,2:9]))
```



|expression |      min|   median|   itr.sec| mem_alloc|    gc.sec| n_itr| n_gc| total_time|
|:----------|--------:|--------:|---------:|---------:|---------:|-----:|----:|----------:|
|R          |    2.34s|    2.52s| 0.3725349|    1.02MB|  4.544925|    10|  122|     26.84s|
|Rcpp       | 425.28ms| 443.26ms| 2.1482805|   244.4MB| 10.741402|    10|   50|      4.66s|

---
### Decay Schedule
*Der går noget galt her, bliver fikset i morgen:)*



We implement a flexible three-parameter power law family of decay schedules

$$
\gamma_n = \frac{\gamma_0 K}{K + n^{a}}
$$

We try a decay schedules with `\(a = 1\)`, `\(K= 0.4\)` and `\(\gamma_0 = 1\)`. A different way to specify the decay schedule is to specify a desired learning rate `\(\gamma_1\)` which should be reached at iteration `\(n_1\)`. These specifications then determine the parameter `\(K\)`. We specify two decay schedules both with `\(\gamma_0 = 1\)`, `\(n_1 = 100\)` and `\(\gamma_1 = 0.01\)`. But one with `\(a = 1\)` and one with `\(a = 2\)`. 



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---
### Decay Schedule

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;


---
### Gradient Descent



We have also implemented a gradient descent algorithm. We have created an equivalent class for this algorithm as well. As stopping criterion we use `\(|\theta_n - \theta_{n-1}| \leq \epsilon = 10^{-3}\)`. We use backtracking line search in the algorithm in order to find step length. We check that the algorithm works: 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

|Par   | (2.1, 5.1, 1.1, 2.1)| (1, 1, 1, 1)| Random vals|
|:-----|--------------------:|------------:|-----------:|
|α (2) |             2.096344|     1.019338|   2.0266860|
|β (5) |             5.101440|     1.087881|  -0.4196662|
|γ (1) |             1.082479|     1.039645|   3.5354424|
|ρ (2) |             2.070642|     1.888254|   1.3762556|


---
### Comparison



We compare the performance of the two algorithms.




|   | True|       GD|       SGD|
|:--|----:|--------:|---------:|
|α  |    2| 2.182967| 2.0783116|
|β  |    5| 5.206888| 5.3463379|
|γ  |    1| 1.112721| 0.9837493|
|ρ  |    2| 2.062844| 1.9757947|

---
### Comparison

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-25-2.png" style="display: block; margin: auto;" /&gt;


---
### Exploiting Grid Structure 



If we sample our `\(x\)`'s from a grid of `\(m\)` points we can exploit the fact that we only have `\(m\)` distinct `\(x\)`-values. We do not need to compute `\(f(x_i| \alpha, \beta,\gamma,\rho)\)` for each `\(i\)`. We can compute the values for each distinct `\(x\)`-value and then use these values to compute the loss function. In the same way we can save computations when computing the derivative of `\(f\)`. We have implemented a `gd_grid` function that exploits this structure. 


```r
gd_grid &lt;- function(
    par,
    t0 = 1e-2,
    maxit = 1200,
    cb = NULL,
    epsilon = 1e-3,
    beta = 0.8,
    alpha = 0.1,
    x,
    y,
    ...) {
  
  x_vals &lt;- unique(x)
  matches &lt;- match(x, x_vals)
  n &lt;- length(x)

  for (i in 1:maxit) {
    
    # Computing 
    fs &lt;- f(par, x_vals)[matches]
    nabla_fs &lt;- sapply(seq_along(x_vals), function(i) nabla_f(par, x_vals[i]))
    
    # Calculations of objective and gradient
    value &lt;- sum((y - fs)^2) 
    gr &lt;- - 2 / n * nabla_fs[,matches] %*% (y - fs)
    
    grad_norm &lt;- sum(gr^2)
    
    # Callback
    if (!is.null(cb)) cb()
    
    t &lt;- t0
    # Proposed descent step
    par_new &lt;- par - t * gr
    new_fs &lt;- f(par_new, x_vals)[matches]
    
    # Convergence criterion based on gradient norm
    if (all(abs(par_new - par) &lt;= epsilon)) break
    
    # Backtracking line search
    while (sum((y - new_fs)^2) &gt; value - alpha * t * grad_norm) {
      t &lt;- beta * t
      par_new &lt;- par - t * gr
      new_fs &lt;- f(par_new, x_vals)[matches]
    }
    par &lt;- par_new
  }
  
  if (i == maxit)  warning("Maximal number, ", maxit, ", of iterations reached")
  
  par
}
```

---
## Benchmarking the two GD versions

We sample data from a grid, we use the same true parametes `\((2,5,1,2)\)`, and sample the `\(x\)`'s from a grid consisting of `\(e^1, ..., e^15\)`. 



We check that the two algorithms return the same output:


```r
all(grad_desc(par = c(1,1,1,1), grad = grad_gd, H = H, x = x2, y = y2) -
  gd_grid(par = c(1,1,1,1), x = x2, y = y2)) &lt; 10^(-8)
```

```
## Warning in all(grad_desc(par = c(1, 1, 1, 1), grad = grad_gd, H = H, x = x2, :
## coercing argument of type 'double' to logical
```

```
## [1] TRUE
```

Comparing grid version with regular GD by benchmarking with 100 point

![](4---Presentation_files/figure-html/unnamed-chunk-30-1.png)&lt;!-- --&gt;

---

Comparing grid version with regular GD with 500 points. 

![](4---Presentation_files/figure-html/unnamed-chunk-31-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
