<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Log-logistic Dose-response Curves</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Rubjerg Hejstvig-Larsen (brf337)" />
    <meta name="author" content="Dina Gyberg Jensen (vbz248)" />
    <meta name="date" content="2024-10-24" />
    <script src="4---Presentation_files/header-attrs-2.26/header-attrs.js"></script>
    <link href="4---Presentation_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
    <script src="4---Presentation_files/htmlwidgets-1.6.4/htmlwidgets.js"></script>
    <script src="4---Presentation_files/jquery-1.12.4/jquery.min.js"></script>
    <script src="4---Presentation_files/d3-3.5.6/d3.min.js"></script>
    <link href="4---Presentation_files/profvis-0.3.6.9000/profvis.css" rel="stylesheet" />
    <script src="4---Presentation_files/profvis-0.3.6.9000/profvis.js"></script>
    <script src="4---Presentation_files/profvis-0.3.6.9000/scroll.js"></script>
    <link href="4---Presentation_files/highlight-6.2.0/textmate.css" rel="stylesheet" />
    <script src="4---Presentation_files/highlight-6.2.0/highlight.js"></script>
    <script src="4---Presentation_files/profvis-binding-0.3.8/profvis.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Log-logistic Dose-response Curves
]
.author[
### Christian Rubjerg Hejstvig-Larsen (brf337)
]
.author[
### Dina Gyberg Jensen (vbz248)
]
.institute[
### University of Copenhagen
]
.date[
### 2024-10-24
]

---

&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content &gt; h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
&lt;/style&gt;





###Introduction
Objective is to use stochastic optimization to estimate the parameters of a log-logistic dose-response model using non-linear least squares estimation. That is obtaining the parameters `\(\alpha, \beta, \gamma, \rho\)` that minimize the loss function:
`$$L(X,(\alpha,\beta,\gamma,\rho))=\frac{1}{N}\sum_{i=1}^N\left( y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)^2$$`
&lt;br&gt;
Where the response is given by:
`$$Y_i = f(x_i| \alpha, \beta,\gamma,\rho) + \varepsilon_i \hskip5mm \text{with}\hskip2mm \varepsilon_i \stackrel{iid}{\sim} N(0,\omega^2)$$`
And the log-logistic dose-response model is given by:
`$$f(x_i| \alpha, \beta,\gamma,\rho) = \gamma + \frac{\rho - \gamma}{1 + \exp(\beta \log(x_i) - \alpha)}$$`

---
###Stochastic Gradient Descent
Start out implementing a standard version of the stochastic gradient descent algorithm (SGD). The gradient in a single point is given by
$$-2\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)  $$

&lt;br&gt;
Where `\(\nabla f(x_i| \alpha, \beta,\gamma,\rho)\)` is given by
`$$\nabla f(x_i| \alpha, \beta,\gamma,\rho) = \begin{pmatrix}
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\\
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\cdot\log(x_i)\\
  1-\frac{1}{1+\exp(\beta\log(x_i)-\alpha)} \\
  \frac{1}{1+\exp(\beta\log(x_i)-\alpha)}
\end{pmatrix}$$`
So the update scheme becomes:
`$$\theta_{t+1} = \theta_t +2 \gamma_t\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)$$`
Where `\(\theta_t = (\alpha_t, \beta_t, \gamma_t, \rho_t)\)` and `\(\gamma_t\)` is the learning rate at time `\(t\)`.
---
###Implementation 
The gradient function and `\(f\)` are implemented seperately

```r
sgd &lt;- function(
    par,
    grad, # Function of parameter and observation index
    gamma, # Decay schedule or a fixed learning rate
    maxiter = 150, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    epoch = vanilla,
    m = 1, # Batch size
    x,
    y,
    ...) {
  
  n &lt;- length(x)
  gamma &lt;- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)
  
  for (k in 1:maxiter) {
    
    if (!is.null(cb)) cb()
    samp &lt;- sampler(n)
    par &lt;- epoch(par = par, samp = samp, gamma = gamma[k], 
                 grad = grad, n = n, x = x, y = y, m = m)
    
  }
  par
}

vanilla &lt;- function(par, samp, gamma, grad, n, x, y, ...){
  for (j in 1:n) {
    i &lt;- samp[j]
    par &lt;- par - gamma * grad(par, x[i], y[i])
  }
  return(par)
}
```

---
###Sampling


 
 - Implemented an S3 object `parameters` and a method `sim` to simulate data from the log-logistic dose-response model.
 
 - 2 different types of sampling methods
 
  - `grid_sample`: samples `\(X\)` from a fixed grid of points `\((e, e^2,..., e^{15})\)`.
  
  - `gauss_sample`: samples `\(\log(X)\)` from `\(\mathcal{N}(0, \omega^2)\)`. Note `\(\omega\)` may not be too large.

- The method also allows for scaling the data, which is useful for optimization.


```r
sim(parameters(1, 1, 1, 1), N = 100, grid = TRUE, scale = TRUE)
```
 - True parameters have been set to `c(2,5,1,2)` and the data is simulated with `N = 5000`.
 


---
### Test of Algorithm 

We do a naive test of the algorithm, and run the algorithm with 3 different starting values: the true values `\((2,5,1,2)\)`, `\((1,1,1,1)\)` and random values `\((1.88, 0.31, 3.64, 2.29)\)`. Step size is `\(0.005\)`, maximal number of itereations `\(300\)`.  



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---
### Test of Algorithm 

We obtain quite different convergence schemes. 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

Table: Different starting values

|Par   | True vals| (1, 1, 1, 1)| Random vals|
|:-----|---------:|------------:|-----------:|
|α (2) | 2.0997154|     2.089024|   -2.082571|
|β (5) | 5.3782134|     5.345995|   -5.346636|
|γ (1) | 0.9709541|     1.006829|    1.933317|
|ρ (2) | 2.0427093|     1.985494|    1.049497|

---
###Profiling the algorithm
<div class="profvis html-widget html-fill-item" id="htmlwidget-d342331dd69a16b5bd3d" style="width:100%;height:600px;"></div>
<script type="application/json" data-for="htmlwidget-d342331dd69a16b5bd3d">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,7,7,7,8,8,8,9,9,9,10,10,10,11,11,11,11,12,12,12,13,13,13,14,14,14,15,15,15,16,16,16,16,17,17,17,18,19,19,19,20,20,20,20,21,21,21,22,22,22,23,23,24,24,24,24,25,25,25,26,26,26,27,28,28,28,29,29,30,30,30,30,31,31,31,32,32,32,32,33,33,33,34,34,34,35,35,35,36,37,37,37,38,38,38,39,39,39,40,40,40,41,41,41,41,42,42,42,43,43,43,43,44,44,44,44,45,45,45,46,46,46,47,47,47,47,48,48,48,49,49,49,49,50,50,50,51,51,51,52,52,52,52,53,53,53,54,54,54,54,55,56,57,58,58,58,59,59,59,60,60,60,60,61,61,61,62,62,62,63,63,63,64,64,64,65,65,66,66,67,68,68,68,69,69,69,70,70,70,71,71,71,72,72,72,73,73,73,74,74,74,75,75,75,76,76,76,77,77,77,78,78,79,80,80,80,81,81,81,82,82,82,83,83,83,83,84,84,84,85,85,85,86,86,86,87,87,88,89,89,89,89,90,90,90,91,91,91,92,92,92,93,93,93,94,94,95,96,96,96,97,97,98,98,98,99,99,99,100,100,100,101,101,101,102,102,102,103,103,103,104,104,104,105,105,105,105,106,106,106,107,107,107,108,108,109,109,109,110,110,110,111,111,111,112,112,112,113,113,113,113,114,114,114,114,115,115,115,116,116,116,116,117,117,117,118,118,118,119,119,120,120,121,121,121,122,122,122,123,123,123,124,124,124,124,125,125,125,126,126,126,127,127,127,128,128,129,129,129,130,130,130,131,131,131,131,132,132,132,132,133,133,134,134,134,134,135,136,136,136,137,137,137,138,138,138,139,139,139,140,140,140,140,141,141,141,142,142,142,143,143,143,144,144,144,145,145,145,146,146,146,147,147,147,148,148,148,149,149,149,150,150,150,151,151,151,152,152,152,152,153,153,153,153,154,154,154,154,155,155,155,156,156,156,156,157,157,157,158,158,159,159,159,159,160,160,160,161,161,161,162,162,162,163,163,163,163,164,164,164,165,165,165,165,166,166,166,167,167,167,168,168,168,169,169,169,169,170,170,170,171,171,171,172,172,172,173,173,173,174,174,174,175,175,175,176,176,176,177,177,178,178,178,179,179,179,180,180,180,181,181,181,182,182,183,183,183,184,184,184,185,185,185,185,186,186,186,187,187,187,188,188,188,188,189,189,189,190,190,190,191,191,191,192,192,192,193,193,193,194,194,194,194,195,195,195,196,196,196,197,197,197,198,198,198,199,199,199,199,200,200,200,201,202,203,203,203,204,204,205,206,206,206,207,207,207,207,208,208,208,208,209,209,209,210,210,210,211,211,211,212,212,212,213,213,214,214,214,215,215,215,216,216,216,217,217,217,218,219,219,219,219,220,220,220,220,221,221,221,222,222,222,223,223,223,223,224,224,225,225,225,226,226,226,226,227,227,227,228,228,228,228,229,229,229,230,230,230,231,232,233,233,234,234,235,235,235,236,236,236,236],"depth":[28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,4,3,2,1,3,2,1,3,2,1,1,3,2,1,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,1,1,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,2,1,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,2,1,4,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,1,1,3,2,1,2,1,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,1,1,2,1,2,1,3,2,1,4,3,2,1],"label":["findCenvVar","getInlineInfo","tryInline","cmpCall","cmp","cmpCallExprFun","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpForBody","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","get","getCompilerOption","make.toplevelContext","cmpfun","compiler:::tryCmpfun","cmpSym","cmp","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpPrim1","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","parent.env","findCenvVar","getInlineInfo","isBaseVar","getFoldFun","constantFoldCall","constantFold","constantFoldCall","constantFold","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","genCode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpPrim1","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","is.factor","grep","is.ddsym","cmpSym","cmpBuiltinArgs","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","<GC>","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","f","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","is.na","mean.default","grad","sgd","<GC>","mean","grad","sgd","x_i <- x[i]","grad","sgd","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","isTRUE","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","length","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","<GC>","mean.default","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","is.numeric","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","mean","grad","sgd","sample.int","sampler","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","alpha <- par[1]","f","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","is.numeric","c","length","mean","grad","sgd","mean","grad","sgd","rho <- par[4]","f","grad","sgd","mean","grad","sgd","beta <- par[2]","grad","sgd","mean","grad","sgd","y_i <- y[i]","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","<GC>","length","c","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","is.na","mean.default","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","<GC>","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","length","isTRUE","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","is.numeric","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","alpha <- par[1]","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","i <- samp[j]","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","<GC>","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","y_i <- y[i]","grad","sgd","isTRUE","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","<GC>","mean.default","grad","sgd","is.na","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","isTRUE","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","<GC>","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","isTRUE","mean.default","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","<GC>","mean","grad","sgd","mean.default","grad","sgd","y_i <- y[i]","grad","sgd","mean.default","grad","sgd","rho <- par[4]","f","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","rho <- par[4]","f","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","mean.default","grad","sgd","alpha <- par[1]","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","c","is.numeric","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","length","beta <- par[2]","grad","sgd","isTRUE","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean","grad","sgd","mean","grad","sgd","<GC>","is.numeric","mean.default","grad","sgd","alpha <- par[1]","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","length","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","<GC>","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","grad","sgd","mean","grad","sgd","is.na","c","grad","sgd","<GC>","is.numeric","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","isTRUE","mean.default","grad","sgd"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,null,1,1,null,null,1,1,null,1,1,1,1,1,1,null,1,1,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,null,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,null,1,1,1,1,1,1,1,1,1,1,null,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,null,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,null,1,1,1,1,null,null,1,1,null,1,1,1,null,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,null,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,1,1,1,null,null,1,1,1,1,null,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,null,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,null,null,1,1,1,null,null,1,1],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,67,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,29,58,67,null,58,67,null,58,67,27,58,67,29,58,67,null,28,58,67,31,58,67,26,58,67,null,58,67,24,58,67,null,null,58,67,null,58,67,null,null,58,67,null,26,58,67,19,58,67,null,58,67,58,67,null,null,58,67,24,58,67,22,58,67,null,29,58,67,58,67,null,null,58,67,null,58,67,null,null,58,67,26,58,67,26,58,67,27,58,67,null,26,58,67,29,58,67,24,58,67,null,58,67,28,28,58,67,26,58,67,null,null,58,67,29,29,58,67,27,58,67,null,53,67,null,null,58,67,null,58,67,3,24,58,67,27,58,67,29,58,67,28,28,58,67,27,58,67,null,null,58,67,null,31,null,29,58,67,27,58,67,6,24,58,67,28,58,67,15,58,67,29,58,67,20,58,67,58,67,null,null,31,null,58,67,26,58,67,26,58,67,27,58,67,29,58,67,27,58,67,26,58,67,null,58,67,28,58,67,null,58,67,58,67,null,null,58,67,26,58,67,26,58,67,null,27,58,67,null,58,67,29,58,67,28,58,67,58,67,null,null,null,58,67,26,58,67,26,58,67,28,58,67,22,58,67,58,67,null,27,58,67,58,67,27,58,67,14,58,67,27,58,67,null,58,67,null,58,67,28,58,67,null,58,67,null,null,58,67,null,58,67,28,58,67,58,67,28,58,67,26,58,67,28,58,67,26,58,67,29,29,58,67,null,null,58,67,27,58,67,27,27,58,67,26,58,67,26,58,67,58,67,57,67,27,58,67,null,58,67,null,58,67,null,24,58,67,27,58,67,27,58,67,27,58,67,58,67,28,58,67,20,58,67,null,null,58,67,null,null,58,67,58,67,null,null,58,67,null,28,58,67,null,58,67,27,58,67,null,58,67,null,null,58,67,28,58,67,null,58,67,null,58,67,null,58,67,24,58,67,29,58,67,22,58,67,27,58,67,26,58,67,29,58,67,28,58,67,null,null,58,67,28,28,58,67,null,null,58,67,null,58,67,null,27,58,67,29,58,67,58,67,26,26,58,67,29,58,67,29,58,67,29,58,67,null,null,58,67,22,58,67,null,29,58,67,null,58,67,20,58,67,null,58,67,6,24,58,67,29,58,67,28,58,67,null,58,67,29,58,67,28,58,67,26,58,67,27,58,67,58,67,31,58,67,27,58,67,28,58,67,26,58,67,58,67,null,58,67,27,58,67,28,28,58,67,27,58,67,27,58,67,6,24,58,67,26,58,67,null,58,67,22,58,67,null,58,67,14,58,67,null,null,58,67,28,58,67,28,58,67,27,58,67,29,58,67,null,null,58,67,null,58,67,31,null,null,58,67,58,67,null,15,58,67,null,null,58,67,29,29,58,67,29,58,67,28,58,67,28,58,67,26,58,67,null,null,null,58,67,14,58,67,28,58,67,null,58,67,null,27,27,58,67,8,24,58,67,26,58,67,29,58,67,null,26,58,67,58,67,null,58,67,null,null,58,67,29,58,67,26,26,58,67,58,58,67,28,58,67,null,31,58,67,null,null,31,58,67,null,null,58,67],"memalloc":[33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.43772125244141,33.56294250488281,33.56294250488281,33.56294250488281,33.56294250488281,33.56294250488281,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.7314453125,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,33.87297821044922,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.05979919433594,34.25859069824219,34.25859069824219,34.25859069824219,34.46399688720703,34.46399688720703,34.46399688720703,34.71006011962891,34.71006011962891,34.71006011962891,34.93926239013672,34.93926239013672,34.93926239013672,35.16886138916016,35.16886138916016,35.16886138916016,33.29447937011719,33.29447937011719,33.29447937011719,33.29447937011719,33.53266906738281,33.53266906738281,33.53266906738281,33.76227569580078,33.76227569580078,33.76227569580078,34.02055358886719,34.02055358886719,34.02055358886719,34.30799102783203,34.30799102783203,34.30799102783203,34.54721069335938,34.54721069335938,34.54721069335938,34.54721069335938,34.78336334228516,34.78336334228516,34.78336334228516,35.00086975097656,35.24602508544922,35.24602508544922,35.24602508544922,33.29447174072266,33.29447174072266,33.29447174072266,33.29447174072266,33.36808776855469,33.36808776855469,33.36808776855469,33.55980682373047,33.55980682373047,33.55980682373047,33.73345184326172,33.73345184326172,33.95294189453125,33.95294189453125,33.95294189453125,33.95294189453125,34.16027069091797,34.16027069091797,34.16027069091797,34.41203308105469,34.41203308105469,34.41203308105469,34.64205932617188,34.88807678222656,34.88807678222656,34.88807678222656,35.10022735595703,35.10022735595703,35.26448059082031,35.26448059082031,35.26448059082031,35.26448059082031,33.51609039306641,33.51609039306641,33.51609039306641,33.72807312011719,33.72807312011719,33.72807312011719,33.72807312011719,34.02272033691406,34.02272033691406,34.02272033691406,34.25984954833984,34.25984954833984,34.25984954833984,34.52418518066406,34.52418518066406,34.52418518066406,34.76325225830078,34.96784973144531,34.96784973144531,34.96784973144531,35.19461822509766,35.19461822509766,35.19461822509766,33.37598419189453,33.37598419189453,33.37598419189453,33.55729675292969,33.55729675292969,33.55729675292969,33.74785614013672,33.74785614013672,33.74785614013672,33.74785614013672,33.96205139160156,33.96205139160156,33.96205139160156,34.17976379394531,34.17976379394531,34.17976379394531,34.17976379394531,34.36804962158203,34.36804962158203,34.36804962158203,34.36804962158203,34.55963897705078,34.55963897705078,34.55963897705078,34.73060607910156,34.73060607910156,34.73060607910156,34.97688293457031,34.97688293457031,34.97688293457031,34.97688293457031,35.2127685546875,35.2127685546875,35.2127685546875,33.36484527587891,33.36484527587891,33.36484527587891,33.36484527587891,33.55220031738281,33.55220031738281,33.55220031738281,33.74297332763672,33.74297332763672,33.74297332763672,33.95613861083984,33.95613861083984,33.95613861083984,33.95613861083984,34.19840240478516,34.19840240478516,34.19840240478516,34.43380737304688,34.43380737304688,34.43380737304688,34.43380737304688,34.69944000244141,34.96408843994141,35.19842529296875,33.38334655761719,33.38334655761719,33.38334655761719,33.60903167724609,33.60903167724609,33.60903167724609,33.8446044921875,33.8446044921875,33.8446044921875,33.8446044921875,34.09303283691406,34.09303283691406,34.09303283691406,34.33354949951172,34.33354949951172,34.33354949951172,34.57982635498047,34.57982635498047,34.57982635498047,34.83676910400391,34.83676910400391,34.83676910400391,35.07624816894531,35.07624816894531,35.28950500488281,35.28950500488281,33.48784637451172,33.72599029541016,33.72599029541016,33.72599029541016,33.98497009277344,33.98497009277344,33.98497009277344,34.29755401611328,34.29755401611328,34.29755401611328,34.59352874755859,34.59352874755859,34.59352874755859,34.83991241455078,34.83991241455078,34.83991241455078,35.07010650634766,35.07010650634766,35.07010650634766,35.28609466552734,35.28609466552734,35.28609466552734,33.48207855224609,33.48207855224609,33.48207855224609,33.70502471923828,33.70502471923828,33.70502471923828,33.92833709716797,33.92833709716797,33.92833709716797,34.14517211914062,34.14517211914062,34.39261627197266,34.61800384521484,34.61800384521484,34.61800384521484,34.85641479492188,34.85641479492188,34.85641479492188,35.12236785888672,35.12236785888672,35.12236785888672,35.31209564208984,35.31209564208984,35.31209564208984,35.31209564208984,33.52960968017578,33.52960968017578,33.52960968017578,33.728759765625,33.728759765625,33.728759765625,33.94570922851562,33.94570922851562,33.94570922851562,34.19080352783203,34.19080352783203,34.45304870605469,34.69284820556641,34.69284820556641,34.69284820556641,34.69284820556641,34.94880676269531,34.94880676269531,34.94880676269531,35.21984100341797,35.21984100341797,35.21984100341797,33.39031219482422,33.39031219482422,33.39031219482422,33.6217041015625,33.6217041015625,33.6217041015625,33.85797882080078,33.85797882080078,34.09758758544922,34.34906768798828,34.34906768798828,34.34906768798828,34.59416961669922,34.59416961669922,34.83021545410156,34.83021545410156,34.83021545410156,35.06212615966797,35.06212615966797,35.06212615966797,35.28534698486328,35.28534698486328,35.28534698486328,33.46275329589844,33.46275329589844,33.46275329589844,33.65901947021484,33.65901947021484,33.65901947021484,33.85462951660156,33.85462951660156,33.85462951660156,34.03395080566406,34.03395080566406,34.03395080566406,34.24432373046875,34.24432373046875,34.24432373046875,34.24432373046875,34.41375732421875,34.41375732421875,34.41375732421875,34.60786437988281,34.60786437988281,34.60786437988281,34.80947875976562,34.80947875976562,34.98920440673828,34.98920440673828,34.98920440673828,35.14295196533203,35.14295196533203,35.14295196533203,35.29096984863281,35.29096984863281,35.29096984863281,33.41699981689453,33.41699981689453,33.41699981689453,33.58664703369141,33.58664703369141,33.58664703369141,33.58664703369141,33.73339080810547,33.73339080810547,33.73339080810547,33.73339080810547,33.88815307617188,33.88815307617188,33.88815307617188,34.04802703857422,34.04802703857422,34.04802703857422,34.04802703857422,34.19535064697266,34.19535064697266,34.19535064697266,34.32673645019531,34.32673645019531,34.32673645019531,34.49565887451172,34.49565887451172,34.66777801513672,34.66777801513672,34.83183288574219,34.83183288574219,34.83183288574219,35.00920867919922,35.00920867919922,35.00920867919922,35.18819427490234,35.18819427490234,35.18819427490234,35.32712554931641,35.32712554931641,35.32712554931641,35.32712554931641,33.53544616699219,33.53544616699219,33.53544616699219,33.72726440429688,33.72726440429688,33.72726440429688,33.87832641601562,33.87832641601562,33.87832641601562,34.11031341552734,34.11031341552734,34.30972290039062,34.30972290039062,34.30972290039062,34.57053375244141,34.57053375244141,34.57053375244141,34.80821228027344,34.80821228027344,34.80821228027344,34.80821228027344,35.03836059570312,35.03836059570312,35.03836059570312,35.03836059570312,35.17211151123047,35.17211151123047,35.34336090087891,35.34336090087891,35.34336090087891,35.34336090087891,33.54225921630859,33.75707244873047,33.75707244873047,33.75707244873047,33.94443511962891,33.94443511962891,33.94443511962891,34.11279296875,34.11279296875,34.11279296875,34.30214691162109,34.30214691162109,34.30214691162109,34.42659759521484,34.42659759521484,34.42659759521484,34.42659759521484,34.57304382324219,34.57304382324219,34.57304382324219,34.73792266845703,34.73792266845703,34.73792266845703,34.94496154785156,34.94496154785156,34.94496154785156,35.15678405761719,35.15678405761719,35.15678405761719,35.33248901367188,35.33248901367188,35.33248901367188,33.46833038330078,33.46833038330078,33.46833038330078,33.67676544189453,33.67676544189453,33.67676544189453,33.86332702636719,33.86332702636719,33.86332702636719,34.07658386230469,34.07658386230469,34.07658386230469,34.22980499267578,34.22980499267578,34.22980499267578,34.44213104248047,34.44213104248047,34.44213104248047,34.654296875,34.654296875,34.654296875,34.654296875,34.80075073242188,34.80075073242188,34.80075073242188,34.80075073242188,34.97373962402344,34.97373962402344,34.97373962402344,34.97373962402344,35.16977691650391,35.16977691650391,35.16977691650391,33.38359832763672,33.38359832763672,33.38359832763672,33.38359832763672,33.555419921875,33.555419921875,33.555419921875,33.76025390625,33.76025390625,34.00113677978516,34.00113677978516,34.00113677978516,34.00113677978516,34.18779754638672,34.18779754638672,34.18779754638672,34.45513916015625,34.45513916015625,34.45513916015625,34.740966796875,34.740966796875,34.740966796875,34.93356323242188,34.93356323242188,34.93356323242188,34.93356323242188,35.17101287841797,35.17101287841797,35.17101287841797,35.35214233398438,35.35214233398438,35.35214233398438,35.35214233398438,33.50513458251953,33.50513458251953,33.50513458251953,33.68183898925781,33.68183898925781,33.68183898925781,33.87601470947266,33.87601470947266,33.87601470947266,34.08345031738281,34.08345031738281,34.08345031738281,34.08345031738281,34.27048492431641,34.27048492431641,34.27048492431641,34.51222229003906,34.51222229003906,34.51222229003906,34.69630432128906,34.69630432128906,34.69630432128906,34.89361572265625,34.89361572265625,34.89361572265625,35.10154724121094,35.10154724121094,35.10154724121094,35.30320739746094,35.30320739746094,35.30320739746094,33.47215270996094,33.47215270996094,33.47215270996094,33.64500427246094,33.64500427246094,33.84317016601562,33.84317016601562,33.84317016601562,34.03073120117188,34.03073120117188,34.03073120117188,34.23956298828125,34.23956298828125,34.23956298828125,34.46755218505859,34.46755218505859,34.46755218505859,34.7635498046875,34.7635498046875,34.96066284179688,34.96066284179688,34.96066284179688,35.24131011962891,35.24131011962891,35.24131011962891,33.42790222167969,33.42790222167969,33.42790222167969,33.42790222167969,33.61275482177734,33.61275482177734,33.61275482177734,33.80918121337891,33.80918121337891,33.80918121337891,34.06038665771484,34.06038665771484,34.06038665771484,34.06038665771484,34.26470184326172,34.26470184326172,34.26470184326172,34.47727966308594,34.47727966308594,34.47727966308594,34.67478942871094,34.67478942871094,34.67478942871094,34.90988922119141,34.90988922119141,34.90988922119141,35.08180999755859,35.08180999755859,35.08180999755859,35.28591918945312,35.28591918945312,35.28591918945312,35.28591918945312,33.43841552734375,33.43841552734375,33.43841552734375,33.60504150390625,33.60504150390625,33.60504150390625,33.87332153320312,33.87332153320312,33.87332153320312,34.05887603759766,34.05887603759766,34.05887603759766,34.33130645751953,34.33130645751953,34.33130645751953,34.33130645751953,34.53366851806641,34.53366851806641,34.53366851806641,34.79754638671875,35.00485992431641,35.22822570800781,35.22822570800781,35.22822570800781,33.41993713378906,33.41993713378906,33.6025390625,33.87371826171875,33.87371826171875,33.87371826171875,34.12945556640625,34.12945556640625,34.12945556640625,34.12945556640625,34.34015655517578,34.34015655517578,34.34015655517578,34.34015655517578,34.52189636230469,34.52189636230469,34.52189636230469,34.72845458984375,34.72845458984375,34.72845458984375,34.98473358154297,34.98473358154297,34.98473358154297,35.208984375,35.208984375,35.208984375,35.38349151611328,35.38349151611328,33.57424163818359,33.57424163818359,33.57424163818359,33.74943542480469,33.74943542480469,33.74943542480469,33.89574432373047,33.89574432373047,33.89574432373047,34.09585571289062,34.09585571289062,34.09585571289062,34.30036926269531,34.52208709716797,34.52208709716797,34.52208709716797,34.52208709716797,34.80947113037109,34.80947113037109,34.80947113037109,34.80947113037109,35.02337646484375,35.02337646484375,35.02337646484375,35.23235321044922,35.23235321044922,35.23235321044922,35.39979553222656,35.39979553222656,35.39979553222656,35.39979553222656,33.39571380615234,33.39571380615234,33.55930328369141,33.55930328369141,33.55930328369141,33.73834228515625,33.73834228515625,33.73834228515625,33.73834228515625,33.93495941162109,33.93495941162109,33.93495941162109,34.11194610595703,34.11194610595703,34.11194610595703,34.11194610595703,34.31467437744141,34.31467437744141,34.31467437744141,34.51040649414062,34.51040649414062,34.51040649414062,34.71409606933594,34.94831848144531,35.14517211914062,35.14517211914062,35.26832580566406,35.26832580566406,33.47872924804688,33.47872924804688,33.47872924804688,33.6627197265625,33.6627197265625,33.6627197265625,33.6627197265625],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1252212524414062,0,0,0,0,0.1685028076171875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1415328979492188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1868209838867188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.19879150390625,0,0,0.2054061889648438,0,0,0.246063232421875,0,0,0.2292022705078125,0,0,0.2295989990234375,0,0,-1.874382019042969,0,0,0,0.238189697265625,0,0,0.2296066284179688,0,0,0.2582778930664062,0,0,0.2874374389648438,0,0,0.2392196655273438,0,0,0,0.2361526489257812,0,0,0.2175064086914062,0.2451553344726562,0,0,-1.951553344726562,0,0,0,0.07361602783203125,0,0,0.1917190551757812,0,0,0.17364501953125,0,0.2194900512695312,0,0,0,0.2073287963867188,0,0,0.2517623901367188,0,0,0.2300262451171875,0.2460174560546875,0,0,0.2121505737304688,0,0.1642532348632812,0,0,0,-1.748390197753906,0,0,0.2119827270507812,0,0,0,0.294647216796875,0,0,0.2371292114257812,0,0,0.2643356323242188,0,0,0.2390670776367188,0.2045974731445312,0,0,0.2267684936523438,0,0,-1.818634033203125,0,0,0.1813125610351562,0,0,0.1905593872070312,0,0,0,0.2141952514648438,0,0,0.21771240234375,0,0,0,0.1882858276367188,0,0,0,0.19158935546875,0,0,0.1709671020507812,0,0,0.24627685546875,0,0,0,0.2358856201171875,0,0,-1.847923278808594,0,0,0,0.1873550415039062,0,0,0.1907730102539062,0,0,0.213165283203125,0,0,0,0.2422637939453125,0,0,0.2354049682617188,0,0,0,0.2656326293945312,0.2646484375,0.2343368530273438,-1.815078735351562,0,0,0.2256851196289062,0,0,0.2355728149414062,0,0,0,0.2484283447265625,0,0,0.2405166625976562,0,0,0.24627685546875,0,0,0.2569427490234375,0,0,0.2394790649414062,0,0.2132568359375,0,-1.801658630371094,0.2381439208984375,0,0,0.2589797973632812,0,0,0.3125839233398438,0,0,0.2959747314453125,0,0,0.2463836669921875,0,0,0.230194091796875,0,0,0.2159881591796875,0,0,-1.80401611328125,0,0,0.2229461669921875,0,0,0.2233123779296875,0,0,0.2168350219726562,0,0.2474441528320312,0.2253875732421875,0,0,0.2384109497070312,0,0,0.2659530639648438,0,0,0.189727783203125,0,0,0,-1.782485961914062,0,0,0.1991500854492188,0,0,0.216949462890625,0,0,0.2450942993164062,0,0.2622451782226562,0.2397994995117188,0,0,0,0.2559585571289062,0,0,0.2710342407226562,0,0,-1.82952880859375,0,0,0.2313919067382812,0,0,0.2362747192382812,0,0.2396087646484375,0.2514801025390625,0,0,0.2451019287109375,0,0.2360458374023438,0,0,0.2319107055664062,0,0,0.2232208251953125,0,0,-1.822593688964844,0,0,0.1962661743164062,0,0,0.1956100463867188,0,0,0.1793212890625,0,0,0.2103729248046875,0,0,0,0.16943359375,0,0,0.1941070556640625,0,0,0.2016143798828125,0,0.1797256469726562,0,0,0.15374755859375,0,0,0.1480178833007812,0,0,-1.873970031738281,0,0,0.169647216796875,0,0,0,0.1467437744140625,0,0,0,0.1547622680664062,0,0,0.1598739624023438,0,0,0,0.1473236083984375,0,0,0.1313858032226562,0,0,0.1689224243164062,0,0.172119140625,0,0.1640548706054688,0,0,0.1773757934570312,0,0,0.178985595703125,0,0,0.1389312744140625,0,0,0,-1.791679382324219,0,0,0.1918182373046875,0,0,0.15106201171875,0,0,0.2319869995117188,0,0.1994094848632812,0,0,0.2608108520507812,0,0,0.2376785278320312,0,0,0,0.2301483154296875,0,0,0,0.1337509155273438,0,0.1712493896484375,0,0,0,-1.801101684570312,0.214813232421875,0,0,0.1873626708984375,0,0,0.1683578491210938,0,0,0.1893539428710938,0,0,0.12445068359375,0,0,0,0.1464462280273438,0,0,0.1648788452148438,0,0,0.2070388793945312,0,0,0.211822509765625,0,0,0.1757049560546875,0,0,-1.864158630371094,0,0,0.20843505859375,0,0,0.1865615844726562,0,0,0.2132568359375,0,0,0.1532211303710938,0,0,0.2123260498046875,0,0,0.2121658325195312,0,0,0,0.146453857421875,0,0,0,0.1729888916015625,0,0,0,0.1960372924804688,0,0,-1.786178588867188,0,0,0,0.1718215942382812,0,0,0.204833984375,0,0.2408828735351562,0,0,0,0.1866607666015625,0,0,0.2673416137695312,0,0,0.28582763671875,0,0,0.192596435546875,0,0,0,0.2374496459960938,0,0,0.1811294555664062,0,0,0,-1.847007751464844,0,0,0.1767044067382812,0,0,0.1941757202148438,0,0,0.2074356079101562,0,0,0,0.1870346069335938,0,0,0.2417373657226562,0,0,0.18408203125,0,0,0.1973114013671875,0,0,0.2079315185546875,0,0,0.20166015625,0,0,-1.8310546875,0,0,0.1728515625,0,0.1981658935546875,0,0,0.18756103515625,0,0,0.208831787109375,0,0,0.2279891967773438,0,0,0.2959976196289062,0,0.197113037109375,0,0,0.2806472778320312,0,0,-1.813407897949219,0,0,0,0.1848526000976562,0,0,0.1964263916015625,0,0,0.2512054443359375,0,0,0,0.204315185546875,0,0,0.2125778198242188,0,0,0.197509765625,0,0,0.2350997924804688,0,0,0.1719207763671875,0,0,0.2041091918945312,0,0,0,-1.847503662109375,0,0,0.1666259765625,0,0,0.268280029296875,0,0,0.1855545043945312,0,0,0.272430419921875,0,0,0,0.202362060546875,0,0,0.2638778686523438,0.2073135375976562,0.2233657836914062,0,0,-1.80828857421875,0,0.1826019287109375,0.27117919921875,0,0,0.2557373046875,0,0,0,0.2107009887695312,0,0,0,0.1817398071289062,0,0,0.2065582275390625,0,0,0.2562789916992188,0,0,0.2242507934570312,0,0,0.1745071411132812,0,-1.809249877929688,0,0,0.1751937866210938,0,0,0.1463088989257812,0,0,0.2001113891601562,0,0,0.2045135498046875,0.2217178344726562,0,0,0,0.287384033203125,0,0,0,0.2139053344726562,0,0,0.2089767456054688,0,0,0.1674423217773438,0,0,0,-2.004081726074219,0,0.1635894775390625,0,0,0.1790390014648438,0,0,0,0.1966171264648438,0,0,0.1769866943359375,0,0,0,0.202728271484375,0,0,0.1957321166992188,0,0,0.2036895751953125,0.234222412109375,0.1968536376953125,0,0.1231536865234375,0,-1.789596557617188,0,0,0.183990478515625,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"profvis({\nf <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))\n}\n\n\n\ngradient <- function(par, i, x, y,...){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  x_i <- x[i]\n  y_i <- y[i]\n  \n  expbetalogxalpha <- exp(beta * log(x_i) - alpha)\n  \n  identical_part <- - 2 * (y_i - f(x_i, par))\n  \n  grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))\n  grad_rho <- mean(identical_part / (1 + expbetalogxalpha))\n  \n  return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))\n}\n\nsgd <- function(\n    par0,\n    grad,\n    N, # Sample size\n    gamma, # Decay schedule or a fixed learning rate\n    epoch = NULL,\n    maxiter = 100, # Max epoch iterations\n    sampler = sample, # How data is resampled. Default is a random permutation\n    cb = NULL,\n    ...) {\n  \n  if (is.function(gamma)) gamma <- gamma(1:maxiter)\n  gamma <- rep_len(gamma, maxiter)\n  \n  par <- par0\n  \n  for (n in 1:maxiter) {\n    if (!is.null(cb)) cb$tracer()\n    \n    samp <- sampler(N)\n    \n    if (is.null(epoch)){\n       for (j in 1:N) {\n         i <- samp[j]\n        par <- par - gamma[n] * grad(par, i, ...)\n       }\n    } else {\n      par <- epoch(par, samp, gamma[n], ...)\n    }\n  }\n  par\n}\n\nsgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = x, y = y)\n})","normpath":"<expr>"}],"prof_output":"/var/folders/kq/0f1vjrnj2xn0q3t1270kq05r0000gn/T//Rtmp1Myquj/file2c0880986c7.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
---
###Implementing the gradient function in RCPP

```r
library(Rcpp)

cppFunction('
NumericVector gradient_rcpp(NumericVector par, NumericVector indices, 
NumericVector x, NumericVector y) {
  // Extract parameters
  double alpha = par[0];
  double beta = par[1];
  double gamma = par[2];
  double rho = par[3];

  // Initialize gradients
  double grad_alpha = 0.0;
  double grad_beta = 0.0;
  double grad_gamma = 0.0;
  double grad_rho = 0.0;

  int n = indices.size(); // Number of indices

  // Loop over the indices
  for (int idx = 0; idx &lt; n; ++idx) {
    int i = indices[idx] - 1;  // Convert from r to c++ indexing

    // Get individual data point
    double x_i = x[i];
    double y_i = y[i];

    // Calculating f(x_i, par)
    double f_x_i = gamma + (rho - gamma) / (1 + exp(beta * log(x_i) - alpha));

    // Exponential term
    double expbetalogxalpha = exp(beta * log(x_i) - alpha);

    // Identical part used in gradients
    double identical_part = -2 * (y_i - f_x_i);

    // Accumulate gradients for all indices
    grad_alpha += (identical_part * (rho - gamma) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_beta += -(identical_part * (rho - gamma) * log(x_i) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_gamma += identical_part * (1 - 1 / (1 + expbetalogxalpha));
    grad_rho += identical_part / (1 + expbetalogxalpha);
  }

  // Return the mean of accumulated gradients
  return NumericVector::create(grad_alpha / n, grad_beta / n, grad_gamma / n, grad_rho / n);
}
')
```


```r
gradient(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

```r
gradient_rcpp(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

---
###Benchmarking
Benchmarking the RCPP implementation against the R implementation
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

|expression |     min|  median|  itr.sec| mem_alloc|    gc.sec| n_itr| n_gc| total_time|
|:----------|-------:|-------:|--------:|---------:|---------:|-----:|----:|----------:|
|R          | 211.4ms| 244.8ms|  3.64318|   338.3KB|  9.071519|   100|  249|     27.45s|
|Rcpp       |  31.1ms|  41.1ms| 24.63715|    24.7MB| 15.275033|   100|   62|      4.06s|

---

```r
sim1 &lt;- data[1:1000,]

bench_results &lt;- bench::mark(
    R = sgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = sim1$x, y = sim1$y),
    Rcpp = sgd(c(3,3,3,3), grad = gradient_rcpp, N = 1000, gamma = 0.01, x = sim1$x, y = sim1$y),
    iterations = 10,
    check = F
)
plot(bench_results)
```

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

```r
knitr::kable(data.frame(expression = c('R', 'Rcpp'), bench_results[,2:9]))
```



|expression |     min|  median|   itr.sec| mem_alloc|    gc.sec| n_itr| n_gc| total_time|
|:----------|-------:|-------:|---------:|---------:|---------:|-----:|----:|----------:|
|R          |   2.16s|   2.24s| 0.4383459|    1.02MB|  5.391655|    10|  123|     22.81s|
|Rcpp       | 377.8ms| 431.2ms| 2.2680104|   244.4MB| 11.340052|    10|   50|      4.41s|

---
### Decay Schedule
*Der går noget galt her, bliver fikset i morgen:)*



We implement a flexible three-parameter power law family of decay schedules

$$
\gamma_n = \frac{\gamma_0 K}{K + n^{a}}
$$

We try a decay schedules with `\(a = 1\)`, `\(K= 0.4\)` and `\(\gamma_0 = 1\)`. A different way to specify the decay schedule is to specify a desired learning rate `\(\gamma_1\)` which should be reached at iteration `\(n_1\)`. These specifications then determine the parameter `\(K\)`. We specify two decay schedules both with `\(\gamma_0 = 1\)`, `\(n_1 = 100\)` and `\(\gamma_1 = 0.01\)`. But one with `\(a = 1\)` and one with `\(a = 2\)`. 



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---
### Decay Schedule

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;


---
### Gradient Descent



We have also implemented a gradient descent algorithm. We have created an equivalent class for this algorithm as well. As stopping criterion we use `\(|\theta_n - \theta_{n-1}| \leq \epsilon = 10^{-3}\)`. We use backtracking line search in the algorithm in order to find step length. We check that the algorithm works: 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

|Par   | (2.1, 5.1, 1.1, 2.1)| (1, 1, 1, 1)| Random vals|
|:-----|--------------------:|------------:|-----------:|
|α (2) |             2.096344|     1.019338|   2.0266860|
|β (5) |             5.101440|     1.087881|  -0.4196662|
|γ (1) |             1.082479|     1.039645|   3.5354424|
|ρ (2) |             2.070642|     1.888254|   1.3762556|


---
### Comparison



We compare the performance of the two algorithms. We use the same data as before and start the algorithms off in the points `\((2.2, 5.2, 1.2, 2.2)\)`. 




|   | True|       GD|       SGD|
|:--|----:|--------:|---------:|
|α  |    2| 2.182967| 2.0783116|
|β  |    5| 5.206888| 5.3463379|
|γ  |    1| 1.112721| 0.9837493|
|ρ  |    2| 2.062844| 1.9757947|

---
### Comparison

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-25-1.png" style="display: block; margin: auto;" /&gt;&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-25-2.png" style="display: block; margin: auto;" /&gt;




---


###Mini-batch stochastic gradient descent
The idea is to calculate the gradient in a _batch_ of data points and update the parameters:

+ Sample `\(m\)` indices, `\(I_n = \{i_1, ...,i_m \}\)` from `\(\{1, ..., N\}\)`.
+ Compute `\(\rho_n = \frac{1}{m} \sum_{i \in I_n} \nabla L_\theta(x_i, y_i, \theta_n)\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

We sample a partition of `\(I_1 \cup I_2 \cup ... \cup I_{M} \subseteq \{1, ..., N\}\)` for `\(M = \lfloor N/m \rfloor\)`.


```r
batch &lt;- function(
    par,           # Parameter estimates
    samp,          # Sample of N indices
    gamma,         # Learning rate
    grad,          # Gradient function
    m = 50,        # Mini-batch size
    ...
){
  M &lt;- floor(length(samp) / m) 
  for (j in 0:(M - 1)) {
    i &lt;- samp[(j * m + 1):(j * m + m)]        # Sample m indices
    par &lt;- par - gamma * grad(par, i, ...)    # Update parameter estimates
  }
  return(par)
}
```












---

###Momentum
Version of batch gradient descent where we add _momemtum_ to the gradient through a convex combination of the current gradient and the previous gradient. Given `\(\theta_n\)` and a batch `\(I_n\)` with `\(|I_n| = m\)` we

+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta \rho_{n-1} + (1 - \beta) g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

The value of `\(\beta\)` determines the gradient memory and is a parameter that can be tuned. Default is set to `\(0.9\)`. Note that `\(\beta = 0\)` corresponds to batch stochastic gradient descent.


```r
momentum &lt;- function() {
  rho &lt;- 0        # Initialize rho outside the inner function to keep track of the previous gradient
  function(
    par,          # Parameter values
    samp,         # Sample of N indices
    gamma,        # Learning rate
    grad,         # Gradient function
    m = 50,       # Mini-batch size
    beta = 0.9,   # Momentum memory
    ...
  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      rho &lt;&lt;- beta * rho + (1 - beta) * grad(par, i, ...)   # Using '&lt;&lt;-' assigns the value to rho in the enclosing environment
      par &lt;- par - gamma * rho
    }
    par
  } 
}
```










---

###Adaptive learning rates
To mitigate tuning issues, we introduce the adam algorithm, an adaptive learning rate algorithm. The idea is to combine momemtum with a standardiziation of each coordinate direction of the descent direction. This is in practice done by dividing the learning rate by a running average of magnitude of previous gradients:
`$$v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n$$`
where we denote `\(\beta_2\)` the forgetting factor. The complete algorithm is as follows:

+ Initialize `\(\theta_0\)`, `\(\rho_0 = 0\)`, `\(v_0 = 0\)`
+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta_1 \rho_{n-1} + (1 - \beta_1) g_n\)`
+ Compute `\(v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \frac{\rho_n}{\sqrt{v_n} + \epsilon}\)`

where we add `\(\epsilon\)` to avoid division by zero (default is `\(\epsilon = 10^{-8}\)`). The interpretation of `\(\beta_1\)` is the same as in the momentum algorithm.

---


```r
adam &lt;- function() {
  rho &lt;- v &lt;- 0     # Initialize rho and v outside the inner function to keep track of the previous gradients
  function(
    par,            # Initial parameter values
    samp,           # Sample of N indices
    gamma,          # Learning rate
    grad,           # Gradient function
    m = 50,         # Mini-batch size
    beta1 = 0.9,    # First-moment memory
    beta2 = 0.9,    # Second-moment memory
    ...

  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      gr &lt;- grad(par, i, ...)
      rho &lt;&lt;- beta1 * rho + (1 - beta1) * gr
      v &lt;&lt;- beta2 * v + (1 - beta2) * gr^2
      par &lt;- par - gamma * (rho / (sqrt(v) + 1e-8))
    }
    par
  } 
}
```












---

###Comparing convergence of the algorithms

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-40-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-41-1.png" style="display: block; margin: auto;" /&gt;










---
### Exploiting Grid Structure 



If we sample our `\(x\)`'s from a grid of `\(m\)` points we can exploit the fact that we only have `\(m\)` distinct `\(x\)`-values. We do not need to compute `\(f(x_i| \alpha, \beta,\gamma,\rho)\)` for each `\(i\)`. We can compute the values for each distinct `\(x\)`-value and then use these values to compute the loss function. In the same way we can save computations when computing the derivative of `\(f\)`. We have implemented a `gd_grid` function that exploits this structure. 


```r
gd_grid &lt;- function(
    par,
    t0 = 1e-2,
    maxit = 1200,
    cb = NULL,
    epsilon = 1e-3,
    beta = 0.8,
    alpha = 0.1,
    x,
    y,
    ...) {
  
  x_vals &lt;- unique(x)
  matches &lt;- match(x, x_vals)
  n &lt;- length(x)

  for (i in 1:maxit) {
    
    # Computing 
    fs &lt;- f(par, x_vals)[matches]
    nabla_fs &lt;- sapply(seq_along(x_vals), function(i) nabla_f(par, x_vals[i]))
    
    # Calculations of objective and gradient
    value &lt;- sum((y - fs)^2) 
    gr &lt;- - 2 / n * nabla_fs[,matches] %*% (y - fs)
    
    grad_norm &lt;- sum(gr^2)
    
    # Callback
    if (!is.null(cb)) cb()
    
    t &lt;- t0
    # Proposed descent step
    par_new &lt;- par - t * gr
    new_fs &lt;- f(par_new, x_vals)[matches]
    
    # Convergence criterion based on gradient norm
    if (all(abs(par_new - par) &lt;= epsilon)) break
    
    # Backtracking line search
    while (sum((y - new_fs)^2) &gt; value - alpha * t * grad_norm) {
      t &lt;- beta * t
      par_new &lt;- par - t * gr
      new_fs &lt;- f(par_new, x_vals)[matches]
    }
    par &lt;- par_new
  }
  
  if (i == maxit)  warning("Maximal number, ", maxit, ", of iterations reached")
  
  par
}
```

---
## Benchmarking the two GD versions

We sample data from a grid, we use the same true parametes `\((2,5,1,2)\)`, and sample the `\(x\)`'s from a grid consisting of `\(e^1, ..., e^15\)`. 



We check that the two algorithms return the same output:


```r
all(grad_desc(par = c(1,1,1,1), grad = grad_gd, H = H, x = x2, y = y2) -
  gd_grid(par = c(1,1,1,1), x = x2, y = y2)) &lt; 10^(-8)
```

```
## [1] TRUE
```

Comparing grid version with regular GD by benchmarking with 100 point

![](4---Presentation_files/figure-html/unnamed-chunk-46-1.png)&lt;!-- --&gt;

---

Comparing grid version with regular GD with 500 points. 

![](4---Presentation_files/figure-html/unnamed-chunk-47-1.png)&lt;!-- --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
