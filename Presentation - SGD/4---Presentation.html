<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Log-logistic Dose-response Curves</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Rubjerg Hejstvig-Larsen (brf337)" />
    <meta name="author" content="Dina Gyberg Jensen (vbz248)" />
    <meta name="author" content="Michaela Lukacova (dns525)" />
    <meta name="date" content="2024-10-24" />
    <script src="4---Presentation_files/header-attrs-2.28/header-attrs.js"></script>
    <link href="4---Presentation_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
    <script src="4---Presentation_files/htmlwidgets-1.6.4/htmlwidgets.js"></script>
    <script src="4---Presentation_files/jquery-1.12.4/jquery.min.js"></script>
    <script src="4---Presentation_files/d3-3.5.6/d3.min.js"></script>
    <link href="4---Presentation_files/profvis-0.3.6.9000/profvis.css" rel="stylesheet" />
    <script src="4---Presentation_files/profvis-0.3.6.9000/profvis.js"></script>
    <script src="4---Presentation_files/profvis-0.3.6.9000/scroll.js"></script>
    <link href="4---Presentation_files/highlight-6.2.0/textmate.css" rel="stylesheet" />
    <script src="4---Presentation_files/highlight-6.2.0/highlight.js"></script>
    <script src="4---Presentation_files/profvis-binding-0.3.8/profvis.js"></script>
    <link rel="stylesheet" href="themer-new.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Log-logistic Dose-response Curves
]
.author[
### Christian Rubjerg Hejstvig-Larsen (brf337)
]
.author[
### Dina Gyberg Jensen (vbz248)
]
.author[
### Michaela Lukacova (dns525)
]
.institute[
### University of Copenhagen
]
.date[
### 2024-10-24
]

---

&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content &gt; h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
&lt;/style&gt;





###Introduction
Objective is to use stochastic optimization to estimate the parameters of a log-logistic dose-response model using non-linear least squares estimation. That is obtaining the parameters `\(\alpha, \beta, \gamma, \rho\)` that minimize the loss function:
`$$L(X,(\alpha,\beta,\gamma,\rho))=\frac{1}{N}\sum_{i=1}^N\left( y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)^2$$`
&lt;br&gt;
Where the response is given by:
`$$Y_i = f(x_i| \alpha, \beta,\gamma,\rho) + \varepsilon_i \hskip5mm \text{with}\hskip2mm \varepsilon_i \stackrel{iid}{\sim} N(0,\omega^2)$$`
And the log-logistic dose-response model is given by:
`$$f(x_i| \alpha, \beta,\gamma,\rho) = \gamma + \frac{\rho - \gamma}{1 + \exp(\beta \log(x_i) - \alpha)}$$`

---
###Stochastic Gradient Descent
Start out implementing a standard version of the stochastic gradient descent algorithm (SGD). The gradient in a single point is given by
$$-2\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)  $$

&lt;br&gt;
Where `\(\nabla f(x_i| \alpha, \beta,\gamma,\rho)\)` is given by
`$$\nabla f(x_i| \alpha, \beta,\gamma,\rho) = \begin{pmatrix}
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\\
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\cdot\log(x_i)\\
  1-\frac{1}{1+\exp(\beta\log(x_i)-\alpha)} \\
  \frac{1}{1+\exp(\beta\log(x_i)-\alpha)}
\end{pmatrix}$$`
So the update scheme becomes:
`$$\theta_{t+1} = \theta_t +2 \gamma_t\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)$$`
Where `\(\theta_t = (\alpha_t, \beta_t, \gamma_t, \rho_t)\)` and `\(\gamma_t\)` is the learning rate at time `\(t\)`.
---
###Implementation 
The gradient function and `\(f\)` are implemented seperately

``` r
sgd &lt;- function(
    par,
    grad, # Function of parameter and observation index
    gamma, # Decay schedule or a fixed learning rate
    maxiter = 150, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    epoch = vanilla,
    m = 1, # Batch size
    x,
    y,
    ...) {
  
  n &lt;- length(x)
  gamma &lt;- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)
  
  for (k in 1:maxiter) {
    
    if (!is.null(cb)) cb()
    samp &lt;- sampler(n)
    par &lt;- epoch(par = par, samp = samp, gamma = gamma[k], 
                 grad = grad, n = n, x = x, y = y, m = m)
    
  }
  par
}

vanilla &lt;- function(par, samp, gamma, grad, n, x, y, ...){
  for (j in 1:n) {
    i &lt;- samp[j]
    par &lt;- par - gamma * grad(par, x[i], y[i])
  }
  return(par)
}
```
---
###Implementation

 - Implemented a tracer using `CSwR`:

``` r
SGD_tracer &lt;- tracer(c("par", "k"), Delta = 0) 
```
 
 - S3 object `SGD`
 
  - Contains optimal parameters and trace
  
  - Plot, print and summary methods are implemented to ease comparisons and analysis.





``` r
SGD(par0 = param$par, grad = grad, gamma = 0.01, x = x, y = y, true_par = param$par)
```

```
## Optimal parameters:
## [1] 1.9438978 5.0176181 0.9668158 2.0812142
## True parametes:
## [1] 2 5 1 2
## Number of iterations:
## [1] 100
## Total time:
## [1] 2.960539
```

---
###Sampling


 
 - Implemented an S3 object `parameters` and a method `sim` to simulate data from the log-logistic dose-response model.
 
 - 2 different types of sampling methods
 
  - `grid_sample`: samples `\(X\)` from a fixed grid of points `\((e, e^2,..., e^{15})\)`.
  
  - `gauss_sample`: samples `\(\log(X)\)` from `\(\mathcal{N}(0, \omega^2)\)`. Note `\(\omega\)` may not be too large.

- The method also allows for scaling the data, which is useful for optimization.


``` r
sim(parameters(1, 1, 1, 1), N = 100, grid = TRUE, scale = TRUE)
```
 - True parameters have been set to `c(2,5,1,2)` and the data is simulated with `N = 5000`.
 


---
### Test of Algorithm 

We do a naive test of the algorithm, and run the algorithm with 3 different starting values: the true values `\((2,5,1,2)\)`, `\((1,1,1,1)\)` and random values `\((1.88, 0.31, 3.64, 2.29)\)`. Step size is `\(0.005\)`, maximal number of itereations `\(300\)`.  



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-11-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
### Test of Algorithm 

We obtain quite different convergence schemes. 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-12-1.png" width="720" style="display: block; margin: auto;" /&gt;

Table: Different starting values

|Par   | True vals| (1, 1, 1, 1)| Random vals|
|:-----|---------:|------------:|-----------:|
|α (2) | 2.0997154|     2.089024|   -2.082571|
|β (5) | 5.3782134|     5.345995|   -5.346636|
|γ (1) | 0.9709541|     1.006829|    1.933317|
|ρ (2) | 2.0427093|     1.985494|    1.049497|

---
###Profiling the algorithm
<div class="profvis html-widget html-fill-item" id="htmlwidget-d342331dd69a16b5bd3d" style="width:100%;height:600px;"></div>
<script type="application/json" data-for="htmlwidget-d342331dd69a16b5bd3d">{"x":{"message":{"prof":{"time":[1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,5,5,5,6,7,7,7,8,8,8,9,9,9,10,10,10,11,11,11,12,12,13,13,13,14,14,14,14,14,15,15,15,16,16,16,17,17,17,18,18,18,18,19,19,19,19,20,20,20,21,21,21,22,22,22,23,23,23,24,24,24,25,25,25,26,26,27,27,27,28,28,28,28,29,29,29,30,30,30,30,31,31,31,32,32,32,33,33,33,34,34,34,35,35,35,36,36,36,37,37,37,38,38,38,39,39,39,40,40,40,41,41,41,42,42,42,43,43,43,44,44,45,46,46,46,47,47,47,48,48,48,49,49,49,50,50,50,51,51,51,52,52,52,53,53,53,53,54,55,55,55,56,56,56,57,57,57,58,58,58,59,59,59,60,60,60,61,61,61,61,62,62,62,63,63,63,64,64,64,65,65,65,66,66,66,67,67,67,68,68,68,69,69,69,70,70,70,70,71,71,71,72,72,72,73,73,73,73,74,74,74,75,76,76,76,77,77,77,78,78,78,79,79,80,80,80,81,81,81,82,82,82,83,83,83,84,85,85,85,86,86,86,87,87,87,88,88,88,89,89,89,90,90,90,91,91,91,92,92,92,93,93,93,93,94,95,95,95,96,96,96,96,97,97,97,98,98,98,99,99,99,100,100,101,101,101,102,103,103,103,104,104,104,105,105,105,106,106,106,107,107,107,108,108,108,109,110,110,110,111,111,111,112,112,112,113,113,113,113,114,114,115,115,116,116,116,116,117,117,117,118,118,118,119,119,119,120,120,120,121,121,121,122,123,123,123,124,124,124,125,126,126,126,127,127,127,128,128,128,129,129,129,129,130,130,130,131,131,131,132,132,132,133,133,133,133,134,134,134,135,135,135,136,136,136,137,137,137,138,138,138,139,139,140,141,141,141,141,142,142,142,142,143,143,143,144,144,144,145,145,145,146,146,146,147,147,147,147,148,148,149,149,149,150,150,150,151,151,151,151,152,152,152,153,153,153,154,154,154,155,155,155,155],"depth":[2,1,10,9,8,7,6,5,4,3,2,1,43,42,41,40,39,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,5,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,4,3,2,1,2,1,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,2,1,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1],"label":["Rprof","profvis","FUN","lapply","FUN","lapply","findLocalsList1","findLocalsList","funEnv","make.functionContext","cmpfun","compiler:::tryCmpfun","cb$restorecurloc","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","cmpPrim2","h","tryInline","cmpCall","cmp","genCode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpPrim1","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","mean","grad","sgd","mean","grad","sgd","length","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad","sgd","mean.default","grad","sgd","<GC>","isTRUE","mean.default","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","sample.int","sampler","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","is.numeric","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","length","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean.default","grad","sgd","length","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","is.numeric","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","length","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","c","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","<GC>","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","length","mean","grad","sgd","gamma <- par[3]","grad","sgd","sgd","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","<GC>","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","mean","grad","sgd","beta <- par[2]","grad","sgd","i <- samp[j]","sgd","is.numeric","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","grad","sgd","mean","grad","sgd","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","isTRUE","mean.default","grad","sgd"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,1,1,1,1,1,null,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,null,1,1,null,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,null,1,1,null,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1],"linenum":[null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,26,58,67,28,58,67,null,29,58,67,28,58,67,null,58,67,27,58,67,28,58,67,58,67,null,58,67,null,null,null,58,67,27,58,67,27,58,67,28,58,67,27,27,58,67,null,null,58,67,29,58,67,29,58,67,null,53,67,31,58,67,29,58,67,null,58,67,58,67,27,58,67,null,null,58,67,26,58,67,8,24,58,67,27,58,67,null,58,67,27,58,67,26,58,67,26,58,67,29,58,67,29,58,67,26,58,67,26,58,67,28,58,67,24,58,67,27,58,67,29,58,67,58,67,null,26,58,67,29,58,67,28,58,67,null,58,67,27,58,67,28,58,67,null,58,67,27,27,58,67,null,28,58,67,28,58,67,26,58,67,24,58,67,28,58,67,null,58,67,null,null,58,67,26,58,67,29,58,67,27,58,67,26,58,67,null,58,67,null,58,67,28,58,67,28,58,67,27,27,58,67,31,58,67,29,58,67,27,27,58,67,null,58,67,null,28,58,67,28,58,67,28,58,67,58,67,null,58,67,29,58,67,26,58,67,28,58,67,null,28,58,67,26,58,67,26,58,67,27,58,67,29,58,67,26,58,67,null,58,67,null,58,67,null,null,58,67,null,27,58,67,null,null,58,67,26,58,67,31,58,67,28,58,67,58,67,28,58,67,31,28,58,67,29,58,67,26,58,67,null,58,67,null,58,67,28,58,67,67,29,58,67,27,58,67,27,58,67,null,26,58,67,58,67,58,67,28,28,58,67,29,58,67,28,58,67,null,58,67,26,58,67,28,58,67,null,29,58,67,16,58,67,67,null,58,67,24,58,67,26,58,67,27,27,58,67,null,58,67,29,58,67,null,58,67,null,null,58,67,29,58,67,26,58,67,24,58,67,28,58,67,15,58,67,57,67,null,8,24,58,67,29,29,58,67,28,58,67,27,58,67,26,58,67,26,58,67,null,null,58,67,58,67,27,58,67,29,58,67,8,24,58,67,27,58,67,28,58,67,24,58,67,null,null,58,67],"memalloc":[33.57740783691406,33.57740783691406,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,33.908935546875,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.22808074951172,34.57486724853516,34.57486724853516,34.57486724853516,34.87203979492188,34.87203979492188,34.87203979492188,35.15219879150391,35.47251129150391,35.47251129150391,35.47251129150391,33.69938659667969,33.69938659667969,33.69938659667969,34.02394104003906,34.02394104003906,34.02394104003906,34.32656097412109,34.32656097412109,34.32656097412109,34.62020111083984,34.62020111083984,34.62020111083984,34.94931030273438,34.94931030273438,35.25603485107422,35.25603485107422,35.25603485107422,35.56760406494141,35.56760406494141,35.56760406494141,35.56760406494141,35.56760406494141,33.87537384033203,33.87537384033203,33.87537384033203,34.23607635498047,34.23607635498047,34.23607635498047,34.56740570068359,34.56740570068359,34.56740570068359,34.86948394775391,34.86948394775391,34.86948394775391,34.86948394775391,35.16107177734375,35.16107177734375,35.16107177734375,35.16107177734375,35.43831634521484,35.43831634521484,35.43831634521484,33.71067810058594,33.71067810058594,33.71067810058594,34.01337432861328,34.01337432861328,34.01337432861328,34.29339599609375,34.29339599609375,34.29339599609375,34.59459686279297,34.59459686279297,34.59459686279297,34.93391418457031,34.93391418457031,34.93391418457031,35.19892120361328,35.19892120361328,35.47202301025391,35.47202301025391,35.47202301025391,33.74493408203125,33.74493408203125,33.74493408203125,33.74493408203125,34.08338928222656,34.08338928222656,34.08338928222656,34.39719390869141,34.39719390869141,34.39719390869141,34.39719390869141,34.69844055175781,34.69844055175781,34.69844055175781,35.15647888183594,35.15647888183594,35.15647888183594,35.48471832275391,35.48471832275391,35.48471832275391,33.72971343994141,33.72971343994141,33.72971343994141,34.19664764404297,34.19664764404297,34.19664764404297,34.51822662353516,34.51822662353516,34.51822662353516,34.82811737060547,34.82811737060547,34.82811737060547,35.14041900634766,35.14041900634766,35.14041900634766,35.38971710205078,35.38971710205078,35.38971710205078,33.86920166015625,33.86920166015625,33.86920166015625,34.15715789794922,34.15715789794922,34.15715789794922,34.60808563232422,34.60808563232422,34.60808563232422,34.89984893798828,34.89984893798828,34.89984893798828,35.17604827880859,35.17604827880859,35.50898742675781,33.7568359375,33.7568359375,33.7568359375,34.10645294189453,34.10645294189453,34.10645294189453,34.41351318359375,34.41351318359375,34.41351318359375,34.71741485595703,34.71741485595703,34.71741485595703,35.03340148925781,35.03340148925781,35.03340148925781,35.47663879394531,35.47663879394531,35.47663879394531,33.73686218261719,33.73686218261719,33.73686218261719,34.06269073486328,34.06269073486328,34.06269073486328,34.06269073486328,34.37709045410156,34.65821838378906,34.65821838378906,34.65821838378906,34.97190856933594,34.97190856933594,34.97190856933594,35.26498413085938,35.26498413085938,35.26498413085938,35.5675048828125,35.5675048828125,35.5675048828125,33.79670715332031,33.79670715332031,33.79670715332031,34.11056518554688,34.11056518554688,34.11056518554688,34.41297149658203,34.41297149658203,34.41297149658203,34.41297149658203,34.74571990966797,34.74571990966797,34.74571990966797,35.05077362060547,35.05077362060547,35.05077362060547,35.30185699462891,35.30185699462891,35.30185699462891,33.772705078125,33.772705078125,33.772705078125,34.13764953613281,34.13764953613281,34.13764953613281,34.43942260742188,34.43942260742188,34.43942260742188,34.72068786621094,34.72068786621094,34.72068786621094,35.03945922851562,35.03945922851562,35.03945922851562,35.36631011962891,35.36631011962891,35.36631011962891,35.36631011962891,33.64076995849609,33.64076995849609,33.64076995849609,33.92314910888672,33.92314910888672,33.92314910888672,34.2178955078125,34.2178955078125,34.2178955078125,34.2178955078125,34.551025390625,34.551025390625,34.551025390625,34.85249328613281,35.18878173828125,35.18878173828125,35.18878173828125,35.50655364990234,35.50655364990234,35.50655364990234,33.72925567626953,33.72925567626953,33.72925567626953,33.87347412109375,33.87347412109375,34.24069213867188,34.24069213867188,34.24069213867188,34.50482177734375,34.50482177734375,34.50482177734375,34.81422424316406,34.81422424316406,34.81422424316406,35.10536956787109,35.10536956787109,35.10536956787109,35.41596984863281,33.69049072265625,33.69049072265625,33.69049072265625,34.00518798828125,34.00518798828125,34.00518798828125,34.30355834960938,34.30355834960938,34.30355834960938,34.62294006347656,34.62294006347656,34.62294006347656,34.91988372802734,34.91988372802734,34.91988372802734,35.34477233886719,35.34477233886719,35.34477233886719,33.63861083984375,33.63861083984375,33.63861083984375,33.94097900390625,33.94097900390625,33.94097900390625,34.27449798583984,34.27449798583984,34.27449798583984,34.27449798583984,34.59104156494141,34.87344360351562,34.87344360351562,34.87344360351562,35.18782043457031,35.18782043457031,35.18782043457031,35.18782043457031,35.45148468017578,35.45148468017578,35.45148468017578,33.69820404052734,33.69820404052734,33.69820404052734,34.04428100585938,34.04428100585938,34.04428100585938,34.38079833984375,34.38079833984375,34.68909454345703,34.68909454345703,34.68909454345703,35.005859375,35.20066070556641,35.20066070556641,35.20066070556641,35.34117889404297,35.34117889404297,35.34117889404297,33.68647003173828,33.68647003173828,33.68647003173828,34.12777709960938,34.12777709960938,34.12777709960938,34.32691192626953,34.32691192626953,34.32691192626953,34.57323455810547,34.57323455810547,34.57323455810547,34.69217681884766,34.90513610839844,34.90513610839844,34.90513610839844,35.03775787353516,35.03775787353516,35.03775787353516,35.26463317871094,35.26463317871094,35.26463317871094,35.59869384765625,35.59869384765625,35.59869384765625,35.59869384765625,33.90419006347656,33.90419006347656,34.24823760986328,34.24823760986328,34.52892303466797,34.52892303466797,34.52892303466797,34.52892303466797,34.81635284423828,34.81635284423828,34.81635284423828,35.10036468505859,35.10036468505859,35.10036468505859,35.40666961669922,35.40666961669922,35.40666961669922,33.81571960449219,33.81571960449219,33.81571960449219,34.12211608886719,34.12211608886719,34.12211608886719,34.3975830078125,34.68538665771484,34.68538665771484,34.68538665771484,34.96685028076172,34.96685028076172,34.96685028076172,35.27170562744141,35.56726837158203,35.56726837158203,35.56726837158203,33.84677124023438,33.84677124023438,33.84677124023438,34.13459014892578,34.13459014892578,34.13459014892578,34.43955230712891,34.43955230712891,34.43955230712891,34.43955230712891,34.76136779785156,34.76136779785156,34.76136779785156,35.05526733398438,35.05526733398438,35.05526733398438,35.34096527099609,35.34096527099609,35.34096527099609,35.61030578613281,35.61030578613281,35.61030578613281,35.61030578613281,33.94387817382812,33.94387817382812,33.94387817382812,34.2415771484375,34.2415771484375,34.2415771484375,34.55948638916016,34.55948638916016,34.55948638916016,34.87181854248047,34.87181854248047,34.87181854248047,35.17278289794922,35.17278289794922,35.17278289794922,35.47796630859375,35.47796630859375,33.75724029541016,34.07762908935547,34.07762908935547,34.07762908935547,34.07762908935547,34.41246795654297,34.41246795654297,34.41246795654297,34.41246795654297,34.74346160888672,34.74346160888672,34.74346160888672,35.03018188476562,35.03018188476562,35.03018188476562,35.3294677734375,35.3294677734375,35.3294677734375,35.62045288085938,35.62045288085938,35.62045288085938,33.87699890136719,33.87699890136719,33.87699890136719,33.87699890136719,34.1951904296875,34.1951904296875,34.61195373535156,34.61195373535156,34.61195373535156,34.89937591552734,34.89937591552734,34.89937591552734,35.19756317138672,35.19756317138672,35.19756317138672,35.19756317138672,35.48047637939453,35.48047637939453,35.48047637939453,33.7525634765625,33.7525634765625,33.7525634765625,34.06714630126953,34.06714630126953,34.06714630126953,34.5460205078125,34.5460205078125,34.5460205078125,34.5460205078125],"meminc":[0,0,0.3315277099609375,0,0,0,0,0,0,0,0,0,0.3191452026367188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.3467864990234375,0,0,0.2971725463867188,0,0,0.2801589965820312,0.3203125,0,0,-1.773124694824219,0,0,0.324554443359375,0,0,0.3026199340820312,0,0,0.29364013671875,0,0,0.3291091918945312,0,0.3067245483398438,0,0,0.3115692138671875,0,0,0,0,-1.692230224609375,0,0,0.3607025146484375,0,0,0.331329345703125,0,0,0.3020782470703125,0,0,0,0.2915878295898438,0,0,0,0.2772445678710938,0,0,-1.727638244628906,0,0,0.3026962280273438,0,0,0.2800216674804688,0,0,0.3012008666992188,0,0,0.3393173217773438,0,0,0.2650070190429688,0,0.273101806640625,0,0,-1.727088928222656,0,0,0,0.3384552001953125,0,0,0.3138046264648438,0,0,0,0.3012466430664062,0,0,0.458038330078125,0,0,0.3282394409179688,0,0,-1.7550048828125,0,0,0.4669342041015625,0,0,0.3215789794921875,0,0,0.3098907470703125,0,0,0.3123016357421875,0,0,0.249298095703125,0,0,-1.520515441894531,0,0,0.2879562377929688,0,0,0.450927734375,0,0,0.2917633056640625,0,0,0.2761993408203125,0,0.3329391479492188,-1.752151489257812,0,0,0.3496170043945312,0,0,0.3070602416992188,0,0,0.3039016723632812,0,0,0.3159866333007812,0,0,0.4432373046875,0,0,-1.739776611328125,0,0,0.3258285522460938,0,0,0,0.3143997192382812,0.2811279296875,0,0,0.313690185546875,0,0,0.2930755615234375,0,0,0.302520751953125,0,0,-1.770797729492188,0,0,0.3138580322265625,0,0,0.3024063110351562,0,0,0,0.3327484130859375,0,0,0.3050537109375,0,0,0.2510833740234375,0,0,-1.529151916503906,0,0,0.3649444580078125,0,0,0.3017730712890625,0,0,0.2812652587890625,0,0,0.3187713623046875,0,0,0.3268508911132812,0,0,0,-1.725540161132812,0,0,0.282379150390625,0,0,0.2947463989257812,0,0,0,0.3331298828125,0,0,0.3014678955078125,0.3362884521484375,0,0,0.3177719116210938,0,0,-1.777297973632812,0,0,0.1442184448242188,0,0.367218017578125,0,0,0.264129638671875,0,0,0.3094024658203125,0,0,0.2911453247070312,0,0,0.3106002807617188,-1.725479125976562,0,0,0.314697265625,0,0,0.298370361328125,0,0,0.3193817138671875,0,0,0.2969436645507812,0,0,0.4248886108398438,0,0,-1.706161499023438,0,0,0.3023681640625,0,0,0.3335189819335938,0,0,0,0.3165435791015625,0.2824020385742188,0,0,0.3143768310546875,0,0,0,0.2636642456054688,0,0,-1.753280639648438,0,0,0.3460769653320312,0,0,0.336517333984375,0,0.3082962036132812,0,0,0.3167648315429688,0.1948013305664062,0,0,0.1405181884765625,0,0,-1.654708862304688,0,0,0.4413070678710938,0,0,0.1991348266601562,0,0,0.2463226318359375,0,0,0.1189422607421875,0.2129592895507812,0,0,0.1326217651367188,0,0,0.2268753051757812,0,0,0.3340606689453125,0,0,0,-1.694503784179688,0,0.3440475463867188,0,0.2806854248046875,0,0,0,0.2874298095703125,0,0,0.2840118408203125,0,0,0.306304931640625,0,0,-1.590950012207031,0,0,0.306396484375,0,0,0.2754669189453125,0.2878036499023438,0,0,0.281463623046875,0,0,0.3048553466796875,0.295562744140625,0,0,-1.720497131347656,0,0,0.2878189086914062,0,0,0.304962158203125,0,0,0,0.3218154907226562,0,0,0.2938995361328125,0,0,0.2856979370117188,0,0,0.2693405151367188,0,0,0,-1.666427612304688,0,0,0.297698974609375,0,0,0.3179092407226562,0,0,0.3123321533203125,0,0,0.30096435546875,0,0,0.3051834106445312,0,-1.720726013183594,0.3203887939453125,0,0,0,0.3348388671875,0,0,0,0.33099365234375,0,0,0.2867202758789062,0,0,0.299285888671875,0,0,0.290985107421875,0,0,-1.743453979492188,0,0,0,0.3181915283203125,0,0.4167633056640625,0,0,0.2874221801757812,0,0,0.298187255859375,0,0,0,0.2829132080078125,0,0,-1.727912902832031,0,0,0.3145828247070312,0,0,0.4788742065429688,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"profvis({\nf <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))\n}\n\n\n\ngradient <- function(par, i, x, y,...){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  x_i <- x[i]\n  y_i <- y[i]\n  \n  expbetalogxalpha <- exp(beta * log(x_i) - alpha)\n  \n  identical_part <- - 2 * (y_i - f(x_i, par))\n  \n  grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))\n  grad_rho <- mean(identical_part / (1 + expbetalogxalpha))\n  \n  return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))\n}\n\nsgd <- function(\n    par0,\n    grad,\n    N, # Sample size\n    gamma, # Decay schedule or a fixed learning rate\n    epoch = NULL,\n    maxiter = 100, # Max epoch iterations\n    sampler = sample, # How data is resampled. Default is a random permutation\n    cb = NULL,\n    ...) {\n  \n  if (is.function(gamma)) gamma <- gamma(1:maxiter)\n  gamma <- rep_len(gamma, maxiter)\n  \n  par <- par0\n  \n  for (n in 1:maxiter) {\n    if (!is.null(cb)) cb$tracer()\n    \n    samp <- sampler(N)\n    \n    if (is.null(epoch)){\n       for (j in 1:N) {\n         i <- samp[j]\n        par <- par - gamma[n] * grad(par, i, ...)\n       }\n    } else {\n      par <- epoch(par, samp, gamma[n], ...)\n    }\n  }\n  par\n}\n\nsgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = x, y = y)\n})","normpath":"<expr>"}],"prof_output":"C:\\Users\\birgi\\AppData\\Local\\Temp\\RtmpwDDUnU\\file48b066b62d7.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
---
###Implementing the gradient function in RCPP

``` r
library(Rcpp)

cppFunction('
NumericVector gradient_rcpp(NumericVector par, NumericVector indices, 
NumericVector x, NumericVector y) {
  // Extract parameters
  double alpha = par[0];
  double beta = par[1];
  double gamma = par[2];
  double rho = par[3];

  // Initialize gradients
  double grad_alpha = 0.0;
  double grad_beta = 0.0;
  double grad_gamma = 0.0;
  double grad_rho = 0.0;

  int n = indices.size(); // Number of indices

  // Loop over the indices
  for (int idx = 0; idx &lt; n; ++idx) {
    int i = indices[idx] - 1;  // Convert from r to c++ indexing

    // Get individual data point
    double x_i = x[i];
    double y_i = y[i];

    // Calculating f(x_i, par)
    double f_x_i = gamma + (rho - gamma) / (1 + exp(beta * log(x_i) - alpha));

    // Exponential term
    double expbetalogxalpha = exp(beta * log(x_i) - alpha);

    // Identical part used in gradients
    double identical_part = -2 * (y_i - f_x_i);

    // Accumulate gradients for all indices
    grad_alpha += (identical_part * (rho - gamma) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_beta += -(identical_part * (rho - gamma) * log(x_i) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_gamma += identical_part * (1 - 1 / (1 + expbetalogxalpha));
    grad_rho += identical_part / (1 + expbetalogxalpha);
  }

  // Return the mean of accumulated gradients
  return NumericVector::create(grad_alpha / n, grad_beta / n, grad_gamma / n, grad_rho / n);
}
')
```


``` r
gradient(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

``` r
gradient_rcpp(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

---
###Benchmarking
Benchmarking the RCPP implementation against the R implementation
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-16-1.png" width="864" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

|expression |     min|  median|   itr.sec| mem_alloc|    gc.sec| n_itr| n_gc| total_time|
|:----------|-------:|-------:|---------:|---------:|---------:|-----:|----:|----------:|
|R          | 229.8ms| 256.4ms|  3.690656|    89.1KB|  8.820667|   100|  239|     27.09s|
|Rcpp       |  21.5ms|  24.8ms| 38.875167|    94.4KB| 10.496295|   100|   27|      2.57s|


---
### Decay Schedule
*Der går noget galt her, bliver fikset i morgen:)*



We implement a flexible three-parameter power law family of decay schedules

$$
\gamma_n = \frac{\gamma_0 K}{K + n^{a}}
$$

We try a decay schedules with `\(a = 1\)`, `\(K= 0.4\)` and `\(\gamma_0 = 1\)`. A different way to specify the decay schedule is to specify a desired learning rate `\(\gamma_1\)` which should be reached at iteration `\(n_1\)`. These specifications then determine the parameter `\(K\)`. We specify two decay schedules both with `\(\gamma_0 = 1\)`, `\(n_1 = 100\)` and `\(\gamma_1 = 0.01\)`. But one with `\(a = 1\)` and one with `\(a = 2\)`. 



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-20-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
### Decay Schedule

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-21-1.png" width="720" style="display: block; margin: auto;" /&gt;


---
### Gradient Descent



We have also implemented a gradient descent algorithm. We have created an equivalent class for this algorithm as well. As stopping criterion we use `\(|\theta_n - \theta_{n-1}| \leq \epsilon = 10^{-3}\)`. We use backtracking line search in the algorithm in order to find step length. We check that the algorithm works: 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-23-1.png" width="720" style="display: block; margin: auto;" /&gt;

|Par   | (2.1, 5.1, 1.1, 2.1)| (1, 1, 1, 1)| Random vals|
|:-----|--------------------:|------------:|-----------:|
|α (2) |             2.085147|    0.8975873|   1.3695276|
|β (5) |             5.106368|    1.4161093|  -0.6854422|
|γ (1) |             1.024231|    0.6687783|   3.7305263|
|ρ (2) |             2.003127|    2.1524844|   1.0499579|


---
### Comparison



We compare the performance of the two algorithms. We use the same data as before and start the algorithms off in the points `\((2.2, 5.2, 1.2, 2.2)\)`. 




|   | True|       GD|       SGD|
|:--|----:|--------:|---------:|
|α  |    2| 2.167353| 2.0783116|
|β  |    5| 5.214136| 5.3463379|
|γ  |    1| 1.024089| 0.9837493|
|ρ  |    2| 1.992799| 1.9757947|

---
### Comparison

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-27-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-27-2.png" width="720" style="display: block; margin: auto;" /&gt;




---


###Mini-batch stochastic gradient descent
The idea is to calculate the gradient in a _batch_ of data points and update the parameters:

+ Sample `\(m\)` indices, `\(I_n = \{i_1, ...,i_m \}\)` from `\(\{1, ..., N\}\)`.
+ Compute `\(\rho_n = \frac{1}{m} \sum_{i \in I_n} \nabla L_\theta(x_i, y_i, \theta_n)\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

We sample a partition of `\(I_1 \cup I_2 \cup ... \cup I_{M} \subseteq \{1, ..., N\}\)` for `\(M = \lfloor N/m \rfloor\)`.


``` r
batch &lt;- function(
    par,           # Parameter estimates
    samp,          # Sample of N indices
    gamma,         # Learning rate
    grad,          # Gradient function
    m = 50,        # Mini-batch size
    ...
){
  M &lt;- floor(length(samp) / m) 
  for (j in 0:(M - 1)) {
    i &lt;- samp[(j * m + 1):(j * m + m)]        # Sample m indices
    par &lt;- par - gamma * grad(par, i, ...)    # Update parameter estimates
  } 
  return(par)
}
```












---

###Momentum
Version of batch gradient descent where we add _momemtum_ to the gradient through a convex combination of the current gradient and the previous gradient. Given `\(\theta_n\)` and a batch `\(I_n\)` with `\(|I_n| = m\)` we

+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta \rho_{n-1} + (1 - \beta) g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

The value of `\(\beta\)` determines the gradient memory and is a parameter that can be tuned. Default is set to `\(0.9\)`. Note that `\(\beta = 0\)` corresponds to batch stochastic gradient descent.


``` r
momentum &lt;- function() {
  rho &lt;- 0        # Initialize rho outside the inner function to keep track of the previous gradient
  function(
    par,          # Parameter values
    samp,         # Sample of N indices
    gamma,        # Learning rate
    grad,         # Gradient function
    m = 50,       # Mini-batch size
    beta = 0.9,   # Momentum memory
    ...
  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      rho &lt;&lt;- beta * rho + (1 - beta) * grad(par, i, ...)   # Using '&lt;&lt;-' assigns the value to rho in the enclosing environment
      par &lt;- par - gamma * rho
    }
    par
  } 
}
```










---

###Adaptive learning rates
To mitigate tuning issues, we introduce the adam algorithm, an adaptive learning rate algorithm. The idea is to combine momemtum with a standardiziation of each coordinate direction of the descent direction. This is in practice done by dividing the learning rate by a running average of magnitude of previous gradients:
`$$v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n$$`
where we denote `\(\beta_2\)` the forgetting factor. The complete algorithm is as follows:

+ Initialize `\(\theta_0\)`, `\(\rho_0 = 0\)`, `\(v_0 = 0\)`
+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta_1 \rho_{n-1} + (1 - \beta_1) g_n\)`
+ Compute `\(v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \frac{\rho_n}{\sqrt{v_n} + \epsilon}\)`

where we add `\(\epsilon\)` to avoid division by zero (default is `\(\epsilon = 10^{-8}\)`). The interpretation of `\(\beta_1\)` is the same as in the momentum algorithm.

---


``` r
adam &lt;- function() {
  rho &lt;- v &lt;- 0     # Initialize rho and v outside the inner function to keep track of the previous gradients
  function(
    par,            # Initial parameter values
    samp,           # Sample of N indices
    gamma,          # Learning rate
    grad,           # Gradient function
    m = 50,         # Mini-batch size
    beta1 = 0.9,    # First-moment memory
    beta2 = 0.9,    # Second-moment memory
    ...

  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      gr &lt;- grad(par, i, ...)
      rho &lt;&lt;- beta1 * rho + (1 - beta1) * gr
      v &lt;&lt;- beta2 * v + (1 - beta2) * gr^2
      par &lt;- par - gamma * (rho / (sqrt(v) + 1e-8))
    }
    par
  } 
}
```












---

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-42-1.png" width="1152" style="display: block; margin: auto;" /&gt;

---

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-43-1.png" width="1152" style="display: block; margin: auto;" /&gt;


---
### Exploiting Grid Structure 



If we sample our `\(x\)`'s from a grid of `\(m\)` points we can exploit the fact that we only have `\(m\)` distinct `\(x\)`-values. We do not need to compute `\(f(x_i| \alpha, \beta,\gamma,\rho)\)` for each `\(i\)`. We can compute the values for each distinct `\(x\)`-value and then use these values to compute the loss function. In the same way we can save computations when computing the derivative of `\(f\)`. We have implemented a `gd_grid` function that exploits this structure. 


``` r
gd_grid &lt;- function(
    par,
    t0 = 5e-2,
    maxit = 1200,
    cb = NULL,
    epsilon = 1e-3,
    beta = 0.8,
    alpha = 0.1,
    x,
    y,
    ...) {
  
  x_vals &lt;- unique(x)
  matches &lt;- match(x, x_vals)
  n &lt;- length(x)

  for (i in 1:maxit) {
    
    # Computing 
    fs &lt;- f(par, x_vals)[matches]
    nabla_fs &lt;- sapply(seq_along(x_vals), function(i) nabla_f(par, x_vals[i]))
    
    # Calculations of objective and gradient
    value &lt;- sum((y - fs)^2) 
    gr &lt;- - 2 / n * nabla_fs[,matches] %*% (y - fs)
    
    grad_norm &lt;- sum(gr^2)
    
    # Callback
    if (!is.null(cb)) cb()
    
    t &lt;- t0
    # Proposed descent step
    par_new &lt;- par - t * gr
    new_fs &lt;- f(par_new, x_vals)[matches]
    
    # Convergence criterion based on gradient norm
    if (all(abs(par_new - par) &lt;= epsilon)) break
    
    # Backtracking line search
    while (sum((y - new_fs)^2) &gt; value - alpha * t * grad_norm) {
      t &lt;- beta * t
      par_new &lt;- par - t * gr
      new_fs &lt;- f(par_new, x_vals)[matches]
    }
    par &lt;- par_new
  }
  
  if (i == maxit)  warning("Maximal number, ", maxit, ", of iterations reached")
  
  par
}
```

---
### Benchmarking the two GD versions

We sample data from a grid, we use the same true parametes `\((2,5,1,2)\)`, and sample the `\(x\)`'s from the grid. 



We check that the two algorithms return the same output:


``` r
all(grad_desc(par = c(1,1,1,1), grad = grad_gd, H = H, x = x2, y = y2) -
  gd_grid(par = c(1,1,1,1), x = x2, y = y2)) &lt; 10^(-8)
```

```
## [1] TRUE
```

Comparing grid version with regular GD by benchmarking with 100 point

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-48-1.png" width="1152" style="display: block; margin: auto;" /&gt;

---
###Comparison for N large

Convergence of the loss function against time














&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-54-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of `\(|H(\theta') - H(\theta)|\)` against time
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-55-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of the loss function against epochs
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-56-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of `\(|H(\theta') - H(\theta)|\)` against epochs
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-57-1.png" width="1152" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
