<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Log-logistic Dose-response Curves</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Rubjerg Hejstvig-Larsen (brf337)" />
    <meta name="author" content="Dina Gyberg Jensen (vbz248)" />
    <meta name="author" content="Michaela Lukacova (dns525)" />
    <meta name="date" content="2024-10-24" />
    <script src="4---Presentation_files/header-attrs-2.26/header-attrs.js"></script>
    <link href="4---Presentation_files/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
    <script src="4---Presentation_files/htmlwidgets-1.6.4/htmlwidgets.js"></script>
    <script src="4---Presentation_files/jquery-1.12.4/jquery.min.js"></script>
    <script src="4---Presentation_files/d3-3.5.6/d3.min.js"></script>
    <link href="4---Presentation_files/profvis-0.3.6.9000/profvis.css" rel="stylesheet" />
    <script src="4---Presentation_files/profvis-0.3.6.9000/profvis.js"></script>
    <script src="4---Presentation_files/profvis-0.3.6.9000/scroll.js"></script>
    <link href="4---Presentation_files/highlight-6.2.0/textmate.css" rel="stylesheet" />
    <script src="4---Presentation_files/highlight-6.2.0/highlight.js"></script>
    <script src="4---Presentation_files/profvis-binding-0.3.8/profvis.js"></script>
    <link rel="stylesheet" href="themer-new.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Log-logistic Dose-response Curves
]
.author[
### Christian Rubjerg Hejstvig-Larsen (brf337)
]
.author[
### Dina Gyberg Jensen (vbz248)
]
.author[
### Michaela Lukacova (dns525)
]
.institute[
### University of Copenhagen
]
.date[
### 2024-10-24
]

---

&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content &gt; h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
&lt;/style&gt;





###Introduction
Objective is to use stochastic optimization to estimate the parameters of a log-logistic dose-response model using non-linear least squares estimation. That is obtaining the parameters `\(\alpha, \beta, \gamma, \rho\)` that minimize the loss function:
`$$L(X,(\alpha,\beta,\gamma,\rho))=\frac{1}{N}\sum_{i=1}^N\left( y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)^2$$`
&lt;br&gt;
Where the response is given by:
`$$Y_i = f(x_i| \alpha, \beta,\gamma,\rho) + \varepsilon_i \hskip5mm \text{with}\hskip2mm \varepsilon_i \stackrel{iid}{\sim} N(0,\omega^2)$$`
And the log-logistic dose-response model is given by:
`$$f(x_i| \alpha, \beta,\gamma,\rho) = \gamma + \frac{\rho - \gamma}{1 + \exp(\beta \log(x_i) - \alpha)}$$`

---
###Stochastic Gradient Descent
Start out implementing a standard version of the stochastic gradient descent algorithm (SGD). The gradient in a single point is given by
$$-2\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)  $$

&lt;br&gt;
Where `\(\nabla f(x_i| \alpha, \beta,\gamma,\rho)\)` is given by
`$$\nabla f(x_i| \alpha, \beta,\gamma,\rho) = \begin{pmatrix}
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\\
  \frac{\rho-\gamma}{(1+\exp(\beta\log(x_i)-\alpha))^2}\cdot\exp(\beta\log(x_i)-\alpha)\cdot\log(x_i)\\
  1-\frac{1}{1+\exp(\beta\log(x_i)-\alpha)} \\
  \frac{1}{1+\exp(\beta\log(x_i)-\alpha)}
\end{pmatrix}$$`
So the update scheme becomes:
`$$\theta_{t+1} = \theta_t +2 \gamma_t\cdot\nabla f(x_i| \alpha, \beta,\gamma,\rho)\cdot\left(y_i - f(x_i| \alpha, \beta,\gamma,\rho)\right)$$`
Where `\(\theta_t = (\alpha_t, \beta_t, \gamma_t, \rho_t)\)` and `\(\gamma_t\)` is the learning rate at time `\(t\)`.
---
###Implementation 
The gradient function and `\(f\)` are implemented seperately

```r
sgd &lt;- function(
    par,
    grad, # Function of parameter and observation index
    gamma, # Decay schedule or a fixed learning rate
    maxiter = 150, # Max epoch iterations
    sampler = sample, # How data is resampled. Default is a random permutation
    cb = NULL,
    epoch = vanilla,
    m = 1, # Batch size
    x,
    y,
    ...) {
  
  n &lt;- length(x)
  gamma &lt;- if (is.function(gamma)) gamma(1:maxiter) else rep(gamma, maxiter)
  
  for (k in 1:maxiter) {
    
    if (!is.null(cb)) cb()
    samp &lt;- sampler(n)
    par &lt;- epoch(par = par, samp = samp, gamma = gamma[k], 
                 grad = grad, n = n, x = x, y = y, m = m)
    
  }
  par
}

vanilla &lt;- function(par, samp, gamma, grad, n, x, y, ...){
  for (j in 1:n) {
    i &lt;- samp[j]
    par &lt;- par - gamma * grad(par, x[i], y[i])
  }
  return(par)
}
```
---
###Implementation

 - Implemented a tracer using `CSwR`:

```r
SGD_tracer &lt;- tracer(c("par", "k"), Delta = 0) 
```
 
 - S3 object `SGD`
 
  - Contains optimal parameters and trace
  
  - Plot, print and summary methods are implemented to ease comparisons and analysis.





```r
SGD(par0 = param$par, grad = grad, gamma = 0.01, x = x, y = y, true_par = param$par)
```

```
## Optimal parameters:
## [1] 1.801554 5.237883 1.093163 2.064605
## True parametes:
## [1] 2 5 1 2
## Number of iterations:
## [1] 100
## Total time:
## [1] 2.220475
```

---
###Sampling


 
 - Implemented an S3 object `parameters` and a method `sim` to simulate data from the log-logistic dose-response model.
 
 - 2 different types of sampling methods
 
  - `grid_sample`: samples `\(X\)` from a fixed grid of points `\((e, e^2,..., e^{15})\)`.
  
  - `gauss_sample`: samples `\(\log(X)\)` from `\(\mathcal{N}(0, \omega^2)\)`. Note `\(\omega\)` may not be too large.

- The method also allows for scaling the data, which is useful for optimization.


```r
sim(parameters(1, 1, 1, 1), N = 100, grid = TRUE, scale = TRUE)
```
 - True parameters have been set to `c(2,5,1,2)` and the data is simulated with `N = 5000`.
 


---
### Test of Algorithm 

We do a naive test of the algorithm, and run the algorithm with 3 different starting values: the true values `\((2,5,1,2)\)`, `\((1,1,1,1)\)` and random values `\((1.88, 0.31, 3.64, 2.29)\)`. Step size is `\(0.005\)`, maximal number of itereations `\(300\)`.  



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-11-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
### Test of Algorithm 

We obtain quite different convergence schemes. 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-12-1.png" width="720" style="display: block; margin: auto;" /&gt;

Table: Different starting values

|Par   | True vals| (1, 1, 1, 1)| Random vals|
|:-----|---------:|------------:|-----------:|
|α (2) | 2.0997154|     2.089024|   -2.082571|
|β (5) | 5.3782134|     5.345995|   -5.346636|
|γ (1) | 0.9709541|     1.006829|    1.933317|
|ρ (2) | 2.0427093|     1.985494|    1.049497|

---
###Profiling the algorithm
<div class="profvis html-widget html-fill-item" id="htmlwidget-d342331dd69a16b5bd3d" style="width:100%;height:600px;"></div>
<script type="application/json" data-for="htmlwidget-d342331dd69a16b5bd3d">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,7,8,8,8,8,9,9,9,9,10,10,10,11,11,11,12,12,12,13,13,13,14,14,14,14,15,15,15,15,16,16,16,17,17,17,18,19,19,19,20,20,20,20,21,21,21,22,22,22,22,23,23,23,23,24,24,24,25,25,25,26,26,26,26,27,27,27,28,28,28,29,29,29,30,30,30,31,31,31,31,32,32,33,33,33,34,34,34,35,36,36,36,37,37,37,38,38,38,38,39,39,39,40,40,40,41,41,41,42,42,43,43,43,44,44,44,45,45,45,46,47,47,47,48,48,48,49,49,49,50,50,50,50,51,51,52,52,52,52,53,53,53,54,54,54,55,55,55,55,56,56,56,57,57,57,58,58,58,59,59,59,60,60,60,61,61,61,62,62,62,63,63,63,63,64,64,64,65,65,65,66,66,66,67,67,67,67,68,68,68,69,69,69,70,70,70,71,72,72,73,73,73,74,74,74,75,75,75,75,76,76,76,76,77,77,77,78,78,78,79,79,79,80,80,80,81,81,81,82,82,82,83,83,83,84,84,84,85,85,85,85,86,86,86,87,87,87,88,88,88,89,89,89,90,90,90,90,91,91,91,92,92,92,93,93,93,94,94,94,94,95,95,95,96,96,96,97,97,97,98,98,98,99,99,99,100,101,101,101,102,102,102,103,103,104,104,104,105,105,105,106,106,106,107,107,107,108,108,108,108,108,109,109,109,110,110,110,111,111,111,112,112,112,113,113,113,113,114,114,114,114,115,115,115,116,116,116,117,117,117,118,118,119,119,119,120,120,120,121,121,121,122,122,122,123,123,123,124,124,124,125,125,125,125,126,126,126,126,127,127,127,128,128,128,129,129,129,130,130,130,130,131,131,131,132,132,132,132,133,133,133,133,134,134,134,135,135,135,136,136,136,137,137,137,138,138,138,138,139,139,139,140,141,141,142,142,142,143,143,143,144,144,144,145,145,145,146,146,146,147,147,147,148,148,148,149,149,149,150,150,150,150,151,151,151,152,152,152,153,154,154,154,154,155,155,156,156,156,157,157,157,158,158,158,158,159,159,159,160,160,160,161,161,161,162,162,162,163,163,163,164,164,164,165,165,165,165,166,166,166,167,167,167,168,168,168,169,169,170,170,170,171,172,172,172,173,173,173,174,174,174,175,175,175,175,176,177,177,177,178,178,178,179,179,179,180,180,180,181,182,182,182,182],"depth":[27,26,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,25,24,23,22,21,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,4,3,2,1,2,1,4,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,1,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,5,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,4,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,1,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,1,4,3,2,1,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,3,2,1,4,3,2,1,3,2,1,3,2,1,3,2,1,2,1,3,2,1,1,3,2,1,3,2,1,3,2,1,4,3,2,1,1,3,2,1,3,2,1,3,2,1,3,2,1,1,4,3,2,1],"label":["findCenvVar","getInlineInfo","isBaseVar","getFoldFun","constantFoldCall","constantFold","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","cmpForBody","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","getInlineInfo","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","findFunDef","cmpCall","cmp","cmpPrim1","h","tryInline","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","exists","findCenvVar","getInlineInfo","isBaseVar","getFoldFun","constantFoldCall","constantFold","cmp","genCode","cmpCallArgs","cmpCallSymFun","cmpCall","cmp","cmpSymbolAssign","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","putconst","cb$putcode","cmpPrim2","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","h","tryInline","cmpCall","cmp","genCode","cmpfun","compiler:::tryCmpfun","x_i <- x[i]","grad","sgd","is.na","isTRUE","mean.default","grad","sgd","<GC>","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","isTRUE","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","c","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","mean","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","<GC>","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean.default","grad","sgd","is.na","mean","grad","sgd","mean","grad","sgd","<GC>","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean.default","grad","sgd","x_i <- x[i]","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","c","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","<GC>","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean.default","grad","sgd","return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))","f","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","grad","sgd","sample.int","sampler","sgd","<GC>","par <- par - gamma[n] * grad(par, i, ...)","sgd","length","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","<GC>","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","<GC>","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","is.numeric","mean","grad","sgd","f","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","<GC>","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","expbetalogxalpha <- exp(beta * log(x_i) - alpha)","grad","sgd","mean","grad","sgd","isTRUE","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean.default","grad","sgd","alpha <- par[1]","grad","sgd","mean","grad","sgd","mean","grad","sgd","mean","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","mean","grad","sgd","isTRUE","mean.default","grad","sgd","rho <- par[4]","grad","sgd","mean.default","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","<GC>","mean.default","grad","sgd","mean.default","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","mean","grad","sgd","mean","grad","sgd","length","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","isTRUE","mean.default","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","length","<GC>","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","identical_part <- - 2 * (y_i - f(x_i, par))","grad","sgd","isTRUE","mean.default","grad","sgd","mean.default","grad","sgd","mean.default","grad","sgd","mean","grad","sgd","par <- par - gamma[n] * grad(par, i, ...)","sgd","mean.default","grad","sgd","is.numeric","mean.default","grad","sgd","grad_rho <- mean(identical_part / (1 + expbetalogxalpha))","grad","sgd","mean","grad","sgd","<GC>","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","length","mean","grad","sgd","mean","grad","sgd","mean.default","grad","sgd","grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)","grad","sgd","length","isTRUE","mean.default","grad","sgd"],"filenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,1,1,1,null,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,null,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,1,1,null,1,1,null,1,1,null,null,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,null,1,1,1,null,1,1,null,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,null,1,1,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,1,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,1,1,1,null,null,1,1,null,1,1,null,1,1,1,1,1,1,1,1,null,1,1,1,1,1,null,null,1,1,null,1,1,null,1,1,1,1,1,1,1,null,1,1,null,null,1,1,1,1,1,1,1,1,null,1,1,1,null,1,1,1,1,1,1,null,1,1,1,1,1,null,null,null,1,1],"linenum":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,67,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,58,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,24,19,58,67,null,null,null,58,67,null,29,58,67,27,58,67,29,58,67,27,58,67,29,58,67,26,26,58,67,null,null,58,67,24,58,67,29,58,67,31,27,58,67,8,24,58,67,28,58,67,8,24,58,67,28,28,58,67,29,58,67,27,58,67,null,null,58,67,26,58,67,null,58,67,24,58,67,22,58,67,null,26,58,67,58,67,29,58,67,null,58,67,null,29,58,67,26,58,67,null,27,58,67,null,58,67,19,58,67,58,58,67,58,67,26,58,67,null,58,67,28,58,67,31,29,58,67,29,58,67,null,58,67,null,null,58,67,58,67,28,28,58,67,null,58,67,26,58,67,null,null,58,67,29,58,67,null,58,67,26,58,67,26,58,67,27,58,67,null,58,67,null,58,67,29,29,58,67,null,58,67,29,58,67,null,58,67,8,24,58,67,58,58,67,null,53,67,null,58,67,null,58,67,null,58,67,null,58,67,null,null,58,67,null,null,58,67,null,58,67,27,58,67,29,58,67,24,58,67,29,58,67,28,58,67,29,58,67,28,58,67,null,null,58,67,24,58,67,31,58,67,27,58,67,null,58,67,28,28,58,67,null,58,67,null,58,67,27,58,67,null,26,58,67,null,58,67,null,58,67,29,58,67,null,58,67,null,58,67,null,28,58,67,24,58,67,58,67,26,58,67,27,58,67,null,58,67,null,58,67,null,29,29,58,67,29,58,67,26,58,67,22,58,67,29,58,67,null,null,58,67,null,null,58,67,null,58,67,null,58,67,26,58,67,58,67,27,58,67,null,58,67,14,58,67,29,58,67,28,58,67,27,58,67,29,29,58,67,null,null,58,67,17,58,67,null,58,67,27,58,67,null,null,58,67,null,58,67,28,28,58,67,27,27,58,67,null,58,67,29,58,67,null,58,67,27,58,67,27,27,58,67,28,58,67,null,58,67,null,58,67,27,58,67,28,58,67,null,58,67,26,58,67,27,58,67,29,58,67,null,58,67,null,null,58,67,27,58,67,null,58,67,null,null,29,58,67,58,67,26,58,67,29,58,67,null,null,58,67,null,58,67,null,58,67,29,58,67,28,58,67,null,58,67,24,58,67,null,null,58,67,null,58,67,null,58,67,29,58,67,58,67,null,58,67,null,null,58,67,29,58,67,27,58,67,null,26,58,67,null,29,58,67,29,58,67,null,58,67,26,58,67,null,null,null,58,67],"memalloc":[33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.62796783447266,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,33.87561798095703,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.060302734375,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.23291778564453,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.41470336914062,34.63435363769531,34.63435363769531,34.63435363769531,35.11896514892578,35.35591125488281,35.35591125488281,35.35591125488281,35.35591125488281,33.49797821044922,33.49797821044922,33.49797821044922,33.49797821044922,33.77888488769531,33.77888488769531,33.77888488769531,34.04466247558594,34.04466247558594,34.04466247558594,34.30326080322266,34.30326080322266,34.30326080322266,34.53962707519531,34.53962707519531,34.53962707519531,34.74961090087891,34.74961090087891,34.74961090087891,34.74961090087891,35.21501159667969,35.21501159667969,35.21501159667969,35.21501159667969,35.43865966796875,35.43865966796875,35.43865966796875,33.83045959472656,33.83045959472656,33.83045959472656,34.08413696289062,34.34585571289062,34.34585571289062,34.34585571289062,34.67638397216797,34.67638397216797,34.67638397216797,34.67638397216797,34.90041351318359,34.90041351318359,34.90041351318359,35.14799499511719,35.14799499511719,35.14799499511719,35.14799499511719,35.38849639892578,35.38849639892578,35.38849639892578,35.38849639892578,33.80680084228516,33.80680084228516,33.80680084228516,34.01361083984375,34.01361083984375,34.01361083984375,34.29008483886719,34.29008483886719,34.29008483886719,34.29008483886719,34.55658721923828,34.55658721923828,34.55658721923828,34.91978454589844,34.91978454589844,34.91978454589844,35.0977783203125,35.0977783203125,35.0977783203125,35.31607818603516,35.31607818603516,35.31607818603516,35.50715637207031,35.50715637207031,35.50715637207031,35.50715637207031,33.69917297363281,33.69917297363281,33.95872497558594,33.95872497558594,33.95872497558594,34.26942443847656,34.26942443847656,34.26942443847656,34.50841522216797,34.94361877441406,34.94361877441406,34.94361877441406,35.16904449462891,35.16904449462891,35.16904449462891,35.50339508056641,35.50339508056641,35.50339508056641,35.50339508056641,33.67694091796875,33.67694091796875,33.67694091796875,33.90200042724609,33.90200042724609,33.90200042724609,34.15740203857422,34.15740203857422,34.15740203857422,34.36762237548828,34.36762237548828,34.56228637695312,34.56228637695312,34.56228637695312,34.7611083984375,34.7611083984375,34.7611083984375,34.96128845214844,34.96128845214844,34.96128845214844,35.18575286865234,35.43296051025391,35.43296051025391,35.43296051025391,33.71968841552734,33.71968841552734,33.71968841552734,33.92835998535156,33.92835998535156,33.92835998535156,34.26454925537109,34.26454925537109,34.26454925537109,34.26454925537109,34.47612762451172,34.47612762451172,34.70637512207031,34.70637512207031,34.70637512207031,34.70637512207031,35.04867553710938,35.04867553710938,35.04867553710938,35.38694763183594,35.38694763183594,35.38694763183594,33.53623199462891,33.53623199462891,33.53623199462891,33.53623199462891,33.79859924316406,33.79859924316406,33.79859924316406,34.01692199707031,34.01692199707031,34.01692199707031,34.30664825439453,34.30664825439453,34.30664825439453,34.67841339111328,34.67841339111328,34.67841339111328,35.01367950439453,35.01367950439453,35.01367950439453,35.28975677490234,35.28975677490234,35.28975677490234,35.50223541259766,35.50223541259766,35.50223541259766,33.65288543701172,33.65288543701172,33.65288543701172,33.65288543701172,34.10459899902344,34.10459899902344,34.10459899902344,34.31996154785156,34.31996154785156,34.31996154785156,34.54022979736328,34.54022979736328,34.54022979736328,34.81100463867188,34.81100463867188,34.81100463867188,34.81100463867188,35.0550537109375,35.0550537109375,35.0550537109375,35.35084533691406,35.35084533691406,35.35084533691406,35.52846527099609,35.52846527099609,35.52846527099609,33.7431640625,34.01548767089844,34.01548767089844,34.22486114501953,34.22486114501953,34.22486114501953,34.60834503173828,34.60834503173828,34.60834503173828,35.03301239013672,35.03301239013672,35.03301239013672,35.03301239013672,35.4415283203125,35.4415283203125,35.4415283203125,35.4415283203125,33.70055389404297,33.70055389404297,33.70055389404297,33.91720581054688,33.91720581054688,33.91720581054688,34.19568634033203,34.19568634033203,34.19568634033203,34.41886901855469,34.41886901855469,34.41886901855469,34.62831115722656,34.62831115722656,34.62831115722656,34.96298217773438,34.96298217773438,34.96298217773438,35.22821807861328,35.22821807861328,35.22821807861328,35.42269134521484,35.42269134521484,35.42269134521484,35.54093170166016,35.54093170166016,35.54093170166016,35.54093170166016,33.74630737304688,33.74630737304688,33.74630737304688,33.95220184326172,33.95220184326172,33.95220184326172,34.19287109375,34.19287109375,34.19287109375,34.38151550292969,34.38151550292969,34.38151550292969,34.61290740966797,34.61290740966797,34.61290740966797,34.61290740966797,34.89716339111328,34.89716339111328,34.89716339111328,35.17588043212891,35.17588043212891,35.17588043212891,35.32606506347656,35.32606506347656,35.32606506347656,35.55728912353516,35.55728912353516,35.55728912353516,35.55728912353516,33.82485961914062,33.82485961914062,33.82485961914062,34.10753631591797,34.10753631591797,34.10753631591797,34.24907684326172,34.24907684326172,34.24907684326172,34.72750854492188,34.72750854492188,34.72750854492188,35.00868225097656,35.00868225097656,35.00868225097656,35.24079132080078,35.45639801025391,35.45639801025391,35.45639801025391,33.73392486572266,33.73392486572266,33.73392486572266,33.95368194580078,33.95368194580078,34.30351257324219,34.30351257324219,34.30351257324219,34.54083251953125,34.54083251953125,34.54083251953125,35.00288391113281,35.00288391113281,35.00288391113281,35.35096740722656,35.35096740722656,35.35096740722656,35.55973815917969,35.55973815917969,35.55973815917969,35.55973815917969,35.55973815917969,33.75196075439453,33.75196075439453,33.75196075439453,34.03701019287109,34.03701019287109,34.03701019287109,34.35978698730469,34.35978698730469,34.35978698730469,34.63931274414062,34.63931274414062,34.63931274414062,34.96630096435547,34.96630096435547,34.96630096435547,34.96630096435547,35.24449920654297,35.24449920654297,35.24449920654297,35.24449920654297,35.50796508789062,35.50796508789062,35.50796508789062,33.86684417724609,33.86684417724609,33.86684417724609,34.28936767578125,34.28936767578125,34.28936767578125,34.53004455566406,34.53004455566406,34.76205444335938,34.76205444335938,34.76205444335938,34.96891021728516,34.96891021728516,34.96891021728516,35.30298614501953,35.30298614501953,35.30298614501953,35.55246734619141,35.55246734619141,35.55246734619141,33.68114471435547,33.68114471435547,33.68114471435547,34.11790466308594,34.11790466308594,34.11790466308594,34.33182525634766,34.33182525634766,34.33182525634766,34.33182525634766,34.55648803710938,34.55648803710938,34.55648803710938,34.55648803710938,34.80037689208984,34.80037689208984,34.80037689208984,35.0428466796875,35.0428466796875,35.0428466796875,35.4942626953125,35.4942626953125,35.4942626953125,33.49798583984375,33.49798583984375,33.49798583984375,33.49798583984375,33.625244140625,33.625244140625,33.625244140625,34.03186798095703,34.03186798095703,34.03186798095703,34.03186798095703,34.45395660400391,34.45395660400391,34.45395660400391,34.45395660400391,34.60471343994141,34.60471343994141,34.60471343994141,34.7989501953125,34.7989501953125,34.7989501953125,34.96833038330078,34.96833038330078,34.96833038330078,35.14255523681641,35.14255523681641,35.14255523681641,35.38985443115234,35.38985443115234,35.38985443115234,35.38985443115234,33.55343627929688,33.55343627929688,33.55343627929688,33.77707672119141,34.17462158203125,34.17462158203125,34.44624328613281,34.44624328613281,34.44624328613281,34.68941497802734,34.68941497802734,34.68941497802734,35.02373504638672,35.02373504638672,35.02373504638672,35.29237365722656,35.29237365722656,35.29237365722656,35.43057250976562,35.43057250976562,35.43057250976562,33.65213012695312,33.65213012695312,33.65213012695312,33.92375183105469,33.92375183105469,33.92375183105469,34.18048095703125,34.18048095703125,34.18048095703125,34.47321319580078,34.47321319580078,34.47321319580078,34.47321319580078,34.74871063232422,34.74871063232422,34.74871063232422,35.08888244628906,35.08888244628906,35.08888244628906,35.33634185791016,35.50714874267578,35.50714874267578,35.50714874267578,35.50714874267578,33.77862548828125,33.77862548828125,34.19640350341797,34.19640350341797,34.19640350341797,34.59592437744141,34.59592437744141,34.59592437744141,34.78760528564453,34.78760528564453,34.78760528564453,34.78760528564453,34.99446868896484,34.99446868896484,34.99446868896484,35.21536254882812,35.21536254882812,35.21536254882812,35.42012786865234,35.42012786865234,35.42012786865234,33.77748870849609,33.77748870849609,33.77748870849609,34.02923583984375,34.02923583984375,34.02923583984375,34.27452087402344,34.27452087402344,34.27452087402344,34.46987915039062,34.46987915039062,34.46987915039062,34.46987915039062,34.95438385009766,34.95438385009766,34.95438385009766,35.19362640380859,35.19362640380859,35.19362640380859,35.43331146240234,35.43331146240234,35.43331146240234,33.82682037353516,33.82682037353516,34.03798675537109,34.03798675537109,34.03798675537109,34.25258636474609,34.49063873291016,34.49063873291016,34.49063873291016,34.71682739257812,34.71682739257812,34.71682739257812,35.16494750976562,35.16494750976562,35.16494750976562,35.51596069335938,35.51596069335938,35.51596069335938,35.51596069335938,33.6591796875,33.91060638427734,33.91060638427734,33.91060638427734,34.1373291015625,34.1373291015625,34.1373291015625,34.50064086914062,34.50064086914062,34.50064086914062,34.71900177001953,34.71900177001953,34.71900177001953,35.08554840087891,35.34033203125,35.34033203125,35.34033203125,35.34033203125],"meminc":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.247650146484375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1846847534179688,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1726150512695312,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.1817855834960938,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.2196502685546875,0,0,0.4846115112304688,0.2369461059570312,0,0,0,-1.857933044433594,0,0,0,0.2809066772460938,0,0,0.265777587890625,0,0,0.2585983276367188,0,0,0.2363662719726562,0,0,0.2099838256835938,0,0,0,0.4654006958007812,0,0,0,0.2236480712890625,0,0,-1.608200073242188,0,0,0.2536773681640625,0.26171875,0,0,0.3305282592773438,0,0,0,0.224029541015625,0,0,0.2475814819335938,0,0,0,0.2405014038085938,0,0,0,-1.581695556640625,0,0,0.2068099975585938,0,0,0.2764739990234375,0,0,0,0.2665023803710938,0,0,0.3631973266601562,0,0,0.1779937744140625,0,0,0.2182998657226562,0,0,0.1910781860351562,0,0,0,-1.8079833984375,0,0.259552001953125,0,0,0.310699462890625,0,0,0.2389907836914062,0.4352035522460938,0,0,0.2254257202148438,0,0,0.3343505859375,0,0,0,-1.826454162597656,0,0,0.2250595092773438,0,0,0.255401611328125,0,0,0.2102203369140625,0,0.1946640014648438,0,0,0.198822021484375,0,0,0.2001800537109375,0,0,0.2244644165039062,0.2472076416015625,0,0,-1.713272094726562,0,0,0.2086715698242188,0,0,0.3361892700195312,0,0,0,0.211578369140625,0,0.2302474975585938,0,0,0,0.3423004150390625,0,0,0.3382720947265625,0,0,-1.850715637207031,0,0,0,0.2623672485351562,0,0,0.21832275390625,0,0,0.2897262573242188,0,0,0.37176513671875,0,0,0.33526611328125,0,0,0.2760772705078125,0,0,0.2124786376953125,0,0,-1.849349975585938,0,0,0,0.4517135620117188,0,0,0.215362548828125,0,0,0.2202682495117188,0,0,0.2707748413085938,0,0,0,0.244049072265625,0,0,0.2957916259765625,0,0,0.1776199340820312,0,0,-1.785301208496094,0.2723236083984375,0,0.2093734741210938,0,0,0.38348388671875,0,0,0.4246673583984375,0,0,0,0.4085159301757812,0,0,0,-1.740974426269531,0,0,0.2166519165039062,0,0,0.2784805297851562,0,0,0.2231826782226562,0,0,0.209442138671875,0,0,0.3346710205078125,0,0,0.2652359008789062,0,0,0.1944732666015625,0,0,0.1182403564453125,0,0,0,-1.794624328613281,0,0,0.2058944702148438,0,0,0.2406692504882812,0,0,0.1886444091796875,0,0,0.2313919067382812,0,0,0,0.2842559814453125,0,0,0.278717041015625,0,0,0.1501846313476562,0,0,0.2312240600585938,0,0,0,-1.732429504394531,0,0,0.2826766967773438,0,0,0.14154052734375,0,0,0.4784317016601562,0,0,0.2811737060546875,0,0,0.2321090698242188,0.215606689453125,0,0,-1.72247314453125,0,0,0.219757080078125,0,0.3498306274414062,0,0,0.2373199462890625,0,0,0.4620513916015625,0,0,0.34808349609375,0,0,0.208770751953125,0,0,0,0,-1.807777404785156,0,0,0.2850494384765625,0,0,0.3227767944335938,0,0,0.2795257568359375,0,0,0.3269882202148438,0,0,0,0.2781982421875,0,0,0,0.2634658813476562,0,0,-1.641120910644531,0,0,0.4225234985351562,0,0,0.2406768798828125,0,0.2320098876953125,0,0,0.2068557739257812,0,0,0.334075927734375,0,0,0.249481201171875,0,0,-1.871322631835938,0,0,0.4367599487304688,0,0,0.2139205932617188,0,0,0,0.2246627807617188,0,0,0,0.2438888549804688,0,0,0.2424697875976562,0,0,0.451416015625,0,0,-1.99627685546875,0,0,0,0.12725830078125,0,0,0.4066238403320312,0,0,0,0.422088623046875,0,0,0,0.1507568359375,0,0,0.1942367553710938,0,0,0.1693801879882812,0,0,0.174224853515625,0,0,0.2472991943359375,0,0,0,-1.836418151855469,0,0,0.2236404418945312,0.3975448608398438,0,0.2716217041015625,0,0,0.2431716918945312,0,0,0.334320068359375,0,0,0.2686386108398438,0,0,0.1381988525390625,0,0,-1.7784423828125,0,0,0.2716217041015625,0,0,0.2567291259765625,0,0,0.2927322387695312,0,0,0,0.2754974365234375,0,0,0.3401718139648438,0,0,0.2474594116210938,0.170806884765625,0,0,0,-1.728523254394531,0,0.4177780151367188,0,0,0.3995208740234375,0,0,0.191680908203125,0,0,0,0.2068634033203125,0,0,0.2208938598632812,0,0,0.2047653198242188,0,0,-1.64263916015625,0,0,0.2517471313476562,0,0,0.2452850341796875,0,0,0.1953582763671875,0,0,0,0.4845046997070312,0,0,0.2392425537109375,0,0,0.23968505859375,0,0,-1.606491088867188,0,0.2111663818359375,0,0,0.214599609375,0.2380523681640625,0,0,0.2261886596679688,0,0,0.4481201171875,0,0,0.35101318359375,0,0,0,-1.856781005859375,0.2514266967773438,0,0,0.2267227172851562,0,0,0.363311767578125,0,0,0.2183609008789062,0,0,0.366546630859375,0.2547836303710938,0,0,0],"filename":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,"<expr>","<expr>",null,"<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>",null,null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>","<expr>",null,"<expr>","<expr>","<expr>","<expr>","<expr>",null,null,null,"<expr>","<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"profvis({\nf <- function(x, par){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  return(gamma + (rho - gamma) / (1 + exp(beta * log(x) - alpha)))\n}\n\n\n\ngradient <- function(par, i, x, y,...){\n  alpha <- par[1]\n  beta <- par[2]\n  gamma <- par[3]\n  rho <- par[4]\n  \n  x_i <- x[i]\n  y_i <- y[i]\n  \n  expbetalogxalpha <- exp(beta * log(x_i) - alpha)\n  \n  identical_part <- - 2 * (y_i - f(x_i, par))\n  \n  grad_alpha <- mean(identical_part * (rho - gamma) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_beta <- - mean(identical_part * (rho - gamma) * log(x[i]) * expbetalogxalpha / (1 + expbetalogxalpha)^2)\n  grad_gamma <- mean(identical_part * (1 - 1 / (1 + expbetalogxalpha)))\n  grad_rho <- mean(identical_part / (1 + expbetalogxalpha))\n  \n  return(c(grad_alpha, grad_beta, grad_gamma, grad_rho))\n}\n\nsgd <- function(\n    par0,\n    grad,\n    N, # Sample size\n    gamma, # Decay schedule or a fixed learning rate\n    epoch = NULL,\n    maxiter = 100, # Max epoch iterations\n    sampler = sample, # How data is resampled. Default is a random permutation\n    cb = NULL,\n    ...) {\n  \n  if (is.function(gamma)) gamma <- gamma(1:maxiter)\n  gamma <- rep_len(gamma, maxiter)\n  \n  par <- par0\n  \n  for (n in 1:maxiter) {\n    if (!is.null(cb)) cb$tracer()\n    \n    samp <- sampler(N)\n    \n    if (is.null(epoch)){\n       for (j in 1:N) {\n         i <- samp[j]\n        par <- par - gamma[n] * grad(par, i, ...)\n       }\n    } else {\n      par <- epoch(par, samp, gamma[n], ...)\n    }\n  }\n  par\n}\n\nsgd(c(3,3,3,3), grad = gradient, N = 1000, gamma = 0.01, x = x, y = y)\n})","normpath":"<expr>"}],"prof_output":"/var/folders/kq/0f1vjrnj2xn0q3t1270kq05r0000gn/T//RtmpdEUMxi/file462d5420b43.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
---
###Implementing the gradient function in RCPP

```r
library(Rcpp)

cppFunction('
NumericVector gradient_rcpp(NumericVector par, NumericVector indices, 
NumericVector x, NumericVector y) {
  // Extract parameters
  double alpha = par[0];
  double beta = par[1];
  double gamma = par[2];
  double rho = par[3];

  // Initialize gradients
  double grad_alpha = 0.0;
  double grad_beta = 0.0;
  double grad_gamma = 0.0;
  double grad_rho = 0.0;

  int n = indices.size(); // Number of indices

  // Loop over the indices
  for (int idx = 0; idx &lt; n; ++idx) {
    int i = indices[idx] - 1;  // Convert from r to c++ indexing

    // Get individual data point
    double x_i = x[i];
    double y_i = y[i];

    // Calculating f(x_i, par)
    double f_x_i = gamma + (rho - gamma) / (1 + exp(beta * log(x_i) - alpha));

    // Exponential term
    double expbetalogxalpha = exp(beta * log(x_i) - alpha);

    // Identical part used in gradients
    double identical_part = -2 * (y_i - f_x_i);

    // Accumulate gradients for all indices
    grad_alpha += (identical_part * (rho - gamma) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_beta += -(identical_part * (rho - gamma) * log(x_i) * expbetalogxalpha) 
    / pow(1 + expbetalogxalpha, 2);
    grad_gamma += identical_part * (1 - 1 / (1 + expbetalogxalpha));
    grad_rho += identical_part / (1 + expbetalogxalpha);
  }

  // Return the mean of accumulated gradients
  return NumericVector::create(grad_alpha / n, grad_beta / n, grad_gamma / n, grad_rho / n);
}
')
```


```r
gradient(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

```r
gradient_rcpp(c(3,3,3,3), c(1,2), x, y)
```

```
## [1] 0.00000000 0.00000000 0.03792148 1.87818815
```

---
###Benchmarking
Benchmarking the RCPP implementation against the R implementation
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-16-1.png" width="864" style="display: block; margin: auto;" /&gt;
&lt;br&gt;

|expression |     min|  median|   itr.sec| mem_alloc|   gc.sec| n_itr| n_gc| total_time|
|:----------|-------:|-------:|---------:|---------:|--------:|-----:|----:|----------:|
|R          | 217.3ms| 254.3ms|  4.012227|   338.3KB|  9.87008|   100|  246|     24.92s|
|Rcpp       |  34.6ms|  44.2ms| 22.430658|    24.7MB| 13.45839|   100|   60|      4.46s|


---
### Decay Schedule
*Der går noget galt her, bliver fikset i morgen:)*



We implement a flexible three-parameter power law family of decay schedules

$$
\gamma_n = \frac{\gamma_0 K}{K + n^{a}}
$$

We try a decay schedules with `\(a = 1\)`, `\(K= 0.4\)` and `\(\gamma_0 = 1\)`. A different way to specify the decay schedule is to specify a desired learning rate `\(\gamma_1\)` which should be reached at iteration `\(n_1\)`. These specifications then determine the parameter `\(K\)`. We specify two decay schedules both with `\(\gamma_0 = 1\)`, `\(n_1 = 100\)` and `\(\gamma_1 = 0.01\)`. But one with `\(a = 1\)` and one with `\(a = 2\)`. 



&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-20-1.png" width="720" style="display: block; margin: auto;" /&gt;

---
### Decay Schedule

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-21-1.png" width="720" style="display: block; margin: auto;" /&gt;


---
### Gradient Descent



We have also implemented a gradient descent algorithm. We have created an equivalent class for this algorithm as well. As stopping criterion we use `\(|\theta_n - \theta_{n-1}| \leq \epsilon = 10^{-3}\)`. We use backtracking line search in the algorithm in order to find step length. We check that the algorithm works: 

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-23-1.png" width="720" style="display: block; margin: auto;" /&gt;

|Par   | (2.1, 5.1, 1.1, 2.1)| (1, 1, 1, 1)| Random vals|
|:-----|--------------------:|------------:|-----------:|
|α (2) |             2.085147|    0.8975873|   1.3695276|
|β (5) |             5.106368|    1.4161093|  -0.6854422|
|γ (1) |             1.024231|    0.6687783|   3.7305263|
|ρ (2) |             2.003127|    2.1524844|   1.0499579|


---
### Comparison



We compare the performance of the two algorithms. We use the same data as before and start the algorithms off in the points `\((2.2, 5.2, 1.2, 2.2)\)`. 




|   | True|       GD|       SGD|
|:--|----:|--------:|---------:|
|α  |    2| 2.167353| 2.0783116|
|β  |    5| 5.214136| 5.3463379|
|γ  |    1| 1.024089| 0.9837493|
|ρ  |    2| 1.992799| 1.9757947|

---
### Comparison

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-27-1.png" width="720" style="display: block; margin: auto;" /&gt;&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-27-2.png" width="720" style="display: block; margin: auto;" /&gt;




---


###Mini-batch stochastic gradient descent
The idea is to calculate the gradient in a _batch_ of data points and update the parameters:

+ Sample `\(m\)` indices, `\(I_n = \{i_1, ...,i_m \}\)` from `\(\{1, ..., N\}\)`.
+ Compute `\(\rho_n = \frac{1}{m} \sum_{i \in I_n} \nabla L_\theta(x_i, y_i, \theta_n)\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

We sample a partition of `\(I_1 \cup I_2 \cup ... \cup I_{M} \subseteq \{1, ..., N\}\)` for `\(M = \lfloor N/m \rfloor\)`.


```r
batch &lt;- function(
    par,           # Parameter estimates
    samp,          # Sample of N indices
    gamma,         # Learning rate
    grad,          # Gradient function
    m = 50,        # Mini-batch size
    ...
){
  M &lt;- floor(length(samp) / m) 
  for (j in 0:(M - 1)) {
    i &lt;- samp[(j * m + 1):(j * m + m)]        # Sample m indices
    par &lt;- par - gamma * grad(par, i, ...)    # Update parameter estimates
  } 
  return(par)
}
```












---

###Momentum
Version of batch gradient descent where we add _momemtum_ to the gradient through a convex combination of the current gradient and the previous gradient. Given `\(\theta_n\)` and a batch `\(I_n\)` with `\(|I_n| = m\)` we

+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta \rho_{n-1} + (1 - \beta) g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \rho_n\)`

The value of `\(\beta\)` determines the gradient memory and is a parameter that can be tuned. Default is set to `\(0.9\)`. Note that `\(\beta = 0\)` corresponds to batch stochastic gradient descent.


```r
momentum &lt;- function() {
  rho &lt;- 0        # Initialize rho outside the inner function to keep track of the previous gradient
  function(
    par,          # Parameter values
    samp,         # Sample of N indices
    gamma,        # Learning rate
    grad,         # Gradient function
    m = 50,       # Mini-batch size
    beta = 0.9,   # Momentum memory
    ...
  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      rho &lt;&lt;- beta * rho + (1 - beta) * grad(par, i, ...)   # Using '&lt;&lt;-' assigns the value to rho in the enclosing environment
      par &lt;- par - gamma * rho
    }
    par
  } 
}
```










---

###Adaptive learning rates
To mitigate tuning issues, we introduce the adam algorithm, an adaptive learning rate algorithm. The idea is to combine momemtum with a standardiziation of each coordinate direction of the descent direction. This is in practice done by dividing the learning rate by a running average of magnitude of previous gradients:
`$$v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n$$`
where we denote `\(\beta_2\)` the forgetting factor. The complete algorithm is as follows:

+ Initialize `\(\theta_0\)`, `\(\rho_0 = 0\)`, `\(v_0 = 0\)`
+ Compute `\(g_n = \frac{1}{m} \sum_{i \in I_n} \nabla_\theta L(y_i, x_i, \theta_n)\)`
+ Compute `\(\rho_n = \beta_1 \rho_{n-1} + (1 - \beta_1) g_n\)`
+ Compute `\(v_n = \beta_2 v_{n-1} + (1 - \beta_2) g_n \odot g_n\)`
+ Update `\(\theta_{n+1} = \theta_n - \gamma_n \frac{\rho_n}{\sqrt{v_n} + \epsilon}\)`

where we add `\(\epsilon\)` to avoid division by zero (default is `\(\epsilon = 10^{-8}\)`). The interpretation of `\(\beta_1\)` is the same as in the momentum algorithm.

---


```r
adam &lt;- function() {
  rho &lt;- v &lt;- 0     # Initialize rho and v outside the inner function to keep track of the previous gradients
  function(
    par,            # Initial parameter values
    samp,           # Sample of N indices
    gamma,          # Learning rate
    grad,           # Gradient function
    m = 50,         # Mini-batch size
    beta1 = 0.9,    # First-moment memory
    beta2 = 0.9,    # Second-moment memory
    ...

  ){
    M &lt;- floor(length(samp) / m) 
    for (j in 0:(M - 1)) {
      i &lt;- samp[(j * m + 1):(j * m + m)]
      gr &lt;- grad(par, i, ...)
      rho &lt;&lt;- beta1 * rho + (1 - beta1) * gr
      v &lt;&lt;- beta2 * v + (1 - beta2) * gr^2
      par &lt;- par - gamma * (rho / (sqrt(v) + 1e-8))
    }
    par
  } 
}
```












---

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-42-1.png" width="1152" style="display: block; margin: auto;" /&gt;

---

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-43-1.png" width="1152" style="display: block; margin: auto;" /&gt;


---
### Exploiting Grid Structure 



If we sample our `\(x\)`'s from a grid of `\(m\)` points we can exploit the fact that we only have `\(m\)` distinct `\(x\)`-values. We do not need to compute `\(f(x_i| \alpha, \beta,\gamma,\rho)\)` for each `\(i\)`. We can compute the values for each distinct `\(x\)`-value and then use these values to compute the loss function. In the same way we can save computations when computing the derivative of `\(f\)`. We have implemented a `gd_grid` function that exploits this structure. 


```r
gd_grid &lt;- function(
    par,
    t0 = 5e-2,
    maxit = 1200,
    cb = NULL,
    epsilon = 1e-3,
    beta = 0.8,
    alpha = 0.1,
    x,
    y,
    ...) {
  
  x_vals &lt;- unique(x)
  matches &lt;- match(x, x_vals)
  n &lt;- length(x)

  for (i in 1:maxit) {
    
    # Computing 
    fs &lt;- f(par, x_vals)[matches]
    nabla_fs &lt;- sapply(seq_along(x_vals), function(i) nabla_f(par, x_vals[i]))
    
    # Calculations of objective and gradient
    value &lt;- sum((y - fs)^2) 
    gr &lt;- - 2 / n * nabla_fs[,matches] %*% (y - fs)
    
    grad_norm &lt;- sum(gr^2)
    
    # Callback
    if (!is.null(cb)) cb()
    
    t &lt;- t0
    # Proposed descent step
    par_new &lt;- par - t * gr
    new_fs &lt;- f(par_new, x_vals)[matches]
    
    # Convergence criterion based on gradient norm
    if (all(abs(par_new - par) &lt;= epsilon)) break
    
    # Backtracking line search
    while (sum((y - new_fs)^2) &gt; value - alpha * t * grad_norm) {
      t &lt;- beta * t
      par_new &lt;- par - t * gr
      new_fs &lt;- f(par_new, x_vals)[matches]
    }
    par &lt;- par_new
  }
  
  if (i == maxit)  warning("Maximal number, ", maxit, ", of iterations reached")
  
  par
}
```

---
### Benchmarking the two GD versions

We sample data from a grid, we use the same true parametes `\((2,5,1,2)\)`, and sample the `\(x\)`'s from the grid. 



We check that the two algorithms return the same output:


```r
all(grad_desc(par = c(1,1,1,1), grad = grad_gd, H = H, x = x2, y = y2) -
  gd_grid(par = c(1,1,1,1), x = x2, y = y2)) &lt; 10^(-8)
```

```
## [1] TRUE
```

Comparing grid version with regular GD by benchmarking with 100 point

&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-48-1.png" width="1152" style="display: block; margin: auto;" /&gt;

---
###Comparison for N large

Convergence of the loss function against time














&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-54-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of `\(|H(\theta') - H(\theta)|\)` against time
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-55-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of the loss function against epochs
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-56-1.png" width="1152" /&gt;

---
###Comparison for N large

Convergence of `\(|H(\theta') - H(\theta)|\)` against epochs
&lt;img src="4---Presentation_files/figure-html/unnamed-chunk-57-1.png" width="1152" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"slideNumberFormat": "%current%",
"highlightStyle": "github",
"highlightLines": true,
"highlightSpans": true,
"ratio": "16:9",
"countIncrementalSlides": true,
"navigation": {
"scroll": false
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
