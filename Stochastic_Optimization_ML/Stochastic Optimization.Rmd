---
title: 'Stochastic Optimization A: '
author: "Michaela Lukacova"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  fig_height: 5
  fig_width: 5
  theme: flatly
  html_document: null
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r source, include=FALSE}
source("~/comp_stat/Stochastic_Optimization_ML/sampler.R")
source("~/comp_stat/Stochastic_Optimization_ML/SGD.R")
source("~/comp_stat/Stochastic_Optimization_ML/GD.R")
```

# Implement stochastic optimization algorithms for estimating the parameters of this model

The four-parameter log-logistic dose-response model is the nonlinear model of the mean of a real valued random variable $Y$ given $x$ defined as

$$
f(x|\alpha, \beta, \gamma, \rho) = \gamma + \frac{\rho - \gamma}{1 + e^{\beta \log(x) - \alpha}}
$$

$\theta = (\alpha, \beta, \gamma, \rho) \in \RR^4$. We wish to minimize

$$
H(\alpha, \beta, \gamma, \rho) = \frac{1}{N} \sum_{i=1}^{n} \left( y_i -  f(x_i|\alpha, \beta, \gamma, \rho ) \right)^2 
$$
By stochastic gradient descent. The optimization works by the update rule

$$\theta_{n+1} = \theta_n - \gamma_{n+1} \nabla H (\theta_{n+1})$$

where $\gamma_n$ is the learning rate. And $\nabla H (\theta_{n})$ is the gradient of the loss function at the $n$-th iteration. Since we are performing SGD we will find the gradient of a single random observation instead of the entire sum

$$
\nabla H (\theta_{n}) = - 2 \nabla_{\theta} f(x_i| \alpha_n, \beta_n, \gamma_n, \rho_n) (y_i - f(x_i| \alpha_n, \beta_n, \gamma_n, \rho_n))
$$

The gradient of the $f$ function can by differentiation be obtained as:

$$
\nabla f(x| \alpha, \beta, \gamma, \rho) = \begin{pmatrix} \frac{\rho- \gamma}{(1 + e^{\beta \log(x) - \alpha})^2} e^{\beta \log(x) - \alpha} \\
- \frac{\log(x) (\rho - \gamma)}{(1 + e^{\beta \log(x) - \alpha})^2} e^{\beta \log(x) - \alpha} \\
1 - \frac{1}{1 + e^{\beta \log(x) - \alpha}} \\ \frac{1}{1 + e^{\beta \log(x) - \alpha}} \end{pmatrix}

$$
The $\nabla H (\theta_{n})$ is implemented in the function grad. We have created a parameters class. The parameter class has a simulate method, which per default samples $\log(x)$'s from a normal distribution and calculates the $y$'s by $y_i = f(x_i|\alpha, \beta, \gamma, \rho)$. If you set grid = TRUE, the $x$'s are sampled from a grid. The grid is the grid from $e^1,..., e^{15}$

```{r}
# Sim 1
N <- 1000
param <- parameters(0.1,1,2.2,1)
data <- sim(param, N)
x <- data$x
y <- data$y
head(data)

# Sim 2
data2 <- sim(param, N, grid = TRUE)
x2 <- data2$x
y2 <- data2$y
head(data2)
```

We have furthermore implemented a SGD class
```{r}
set.seed(3099)
sgd0 <- SGD(par0 = c(1,1,1,1), grad = grad, gamma = 0.01, true_par = param$par,
             x = x, y = y)
```

With a print function
```{r}
sgd0
sgd0$true_par
```

A plot function
```{r}
plot(sgd0, 1)
```

For different starting values
```{r}
set.seed(34)
rand <- round(rnorm(4, 0, 5))
col_name <- sprintf("SV (%s)", paste(rand, collapse = ", "))

diff_start_vals_sgd <- tibble(
  "Par" = c("alpha", "beta", "gamma", "rho"),
  "True vals" = param$par,
  "Sv c(1,1,1,1)" = sgd0$est,
  "True sv" = SGD(par0 = param$par, grad = grad, gamma = 0.01, x = x, y = y,
                           true_par = param$par)$est,
   !!sym(col_name) := SGD(par0 = rand, grad = grad, gamma = 0.01, x = x, y = y,
                           true_par = param$par)$est)

diff_start_vals_sgd
```
It does not seem as if the SGD algorithm is very sensitive to the starting values.

# Mini batch implementation

Mini batches stabilizes the gradient, while maintaining the SGD speed. 

```{r}
set.seed(3099)
sgd_mini_batch <- SGD(par0 = c(1,1,1,1), grad = grad_mult, gamma = 0.01, x = x, y = y,
            true_par = param$par, m = 50)

sgd_mini_batch

grid.arrange(plot(sgd_mini_batch, 1),
             plot(sgd_mini_batch, 2), 
             plot(sgd_mini_batch, 3),
             nrow = 1)
```

```{r}
diff_start_vals_sgd_mb <- tibble(
  "Par" = c("alpha", "beta", "gamma", "rho"),
  "True vals" = param$par,
  "Sv c(1,1,1,1)" = sgd_mini_batch$est,
  "True sv" = SGD(par0 = param$par, grad = grad_mult, gamma = 0.01, x = x, y = y,
                           true_par = param$par, m = 50)$est,
   !!sym(col_name) := SGD(par0 = rand, grad = grad_mult, gamma = 0.01, x = x, y = y,
                           true_par = param$par, m = 50)$est)

diff_start_vals_sgd_mb
```


# Compare the resulting optimization algorithm(s) with non-stochastic algorithms e.g. gradient descent or the Newton algorithm


```{r}
gd0 <- GD(par = c(1,1,1,1), H = H, x = x, t0 = 0.05, y = y, true_par = param$par)

gd0
gd0$additional_args$true_par
```

```{r}
grid.arrange(
  plot(gd0, 1),
  plot(sgd0, 1),
  plot(gd0, 2),
  plot(sgd0, 2),
  plot(gd0, 3),
  plot(sgd0, 3),
  nrow = 3
)
```

```{r}
diff_start_vals_gd <- tibble(
  "Par" = c("alpha", "beta", "gamma", "rho"),
  "True vals" = param$par,
  "Sv c(1,1,1,1)" = gd0$est,
  "True sv" = GD(par = param$par, H = H, x = x, t0 = 0.05, y = y, 
                   true_par = param$par)$est,
  "Random sv" = GD(par = rand, H = H, x = x, t0 = 0.05, 
                             y = y, true_par = param$par)$est)
```
GD works very well if we start the algorithm out in the true values. 

# Data sampled from a grid

We see how well they do for the grid sampled data

```{r}
gd1 <- GD(par = c(1,1,1,1), grad = grad_gd, H = H, t0 = 1e-1, maxit = 1000,
    epsilon = 1e-4, beta = 0.5, alpha = 0.1,  x = x2,  y = y2, true_par = param$par)

sgd1 <- SGD(par0 = c(1,1,1,1), grad = grad, gamma = 0.01, x = x2, y = y2,
            true_par = param$par, maxiter = 100)

print(gd1$est); print(sgd1$est); param$par


```


