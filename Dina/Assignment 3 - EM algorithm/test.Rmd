---
title: "The Expectation-Maximization Algorithm"
author: "Dina Gyberg Jensen (vbz248)"
institute: "University of Copenhagen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content > h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
</style>
```{r, include=FALSE, eval = FALSE}
rmarkdown::render('presentation.rmd')
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(CSwR)
style_mono_accent(
 base_color = "#4f7942")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(bench)
library(tidyverse)
library(profvis)
library(bench)
theme_set(theme_bw() + theme(text = element_text(size = 13)))
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```



###Marginal Density of X

$Y = (X,W)\in\mathbb{R}\times(0,\infty)$ has joint density:
$$f(x,w)=\frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)} $$


The marginal density of $X$ is:
$$f(x)=\int_0^\infty f(x,w)dw=\frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)} \int_0^\infty w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)} dw\\
=\frac{1}{\sqrt{\pi \nu \sigma}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{-\frac{\nu+1}{2}}$$
Where we have used that the gamma density is given by $\Gamma(z)=\int_0^\infty t^{z-1}e^{-t}dt$ and used the substitution $t = \frac{w}{2}\left( 1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)$. The result can be easily recognized as a Student's t-distribution with $\nu$ degrees of freedom and location $\mu$ and scale $\sigma$.

---
###Maximizing Complete Data log-likelihood
Complete data log-likelihood for fixed $\nu$:
$$\sum_{i=1}^n\log f(x_i,w_i)=-\sum_{i=1}^n\log\left(\sigma\sqrt{\pi\nu}2^{(\nu+1)/2}\Gamma(\nu/2) \right)+\sum_{i=1}^n\log\left(w_i^{\frac{\nu-1}{2}} \right)-\sum_{i=1}^n\left(\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2} \right) \right)$$
We differentiate and set equal to 0 to find the MLEs of $\mu$ and $\sigma^2$ given that we observe the full dataset.
.pull-left[ 
$$\frac{\partial }{\partial\mu}=\frac{1}{\nu\sigma^2}\sum_{i=1}^nw_i(x_i-\mu)$$
]

.pull-right[ 
$$\frac{\partial }{\partial\sigma}=-\frac{n}{\sigma}+\sum_{i=1}^n w_i\frac{(x_i-\mu)^2}{\nu\sigma^3}$$
 ]
.pull-left[ 
$$\mu_{opt}=\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}$$
]

.pull-right[ 
$$\sigma^2_{opt} =\frac{1}{n\nu}\sum_{i=1}^nw_i(x_i-\mu)^2$$
 ]

---
###E-step
For the E-step we need to compute the $Q$-function
$$Q(\theta|\theta')=E_{\theta'}\left(\log( f(X,W \ | \ \theta) \ | \ X=x) \right)= \sum_{i=1}^nE_{\theta'}(\log( f(x_i,W_i \ | \ \theta) \ | \ X_i=x_i) )\\
=\sum_{i=1}^nE_{\theta'}\left( \log\left( \frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)}\right) \right)\\
=-\sum_{i=1}^n\log\left(\sigma\sqrt{\pi\nu}2^{(\nu+1)/2}\Gamma(\nu/2) \right)+\frac{\nu-1}{2}\sum_{i=1}^nE_{\theta'}( \log(W_i\ | \ X_i=x_i))\\
-\frac{1}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2} \right)\sum_{i=1}^n E_{\theta'}(W_i \ | \ X_i=x_i)$$

The conditional distribution of $W_i \ | X_i=x_i$ can be recognized as the gamma distribution with shape $k = \frac{\nu+1}{2}$ and scale $t_i = \frac{2}{1+\frac{(x_i-\mu')^2}{\nu\sigma'^2} }$. Hence we have:
.pull-left[
$$
E_{\theta'}(W_i \ | \ X_i=x_i)=k\cdot t_i
$$
]
.pull-right[
$$
E_{\theta'}( \log(W_i\ | \ X_i=x_i))=\psi(k)+\log(t_i)
$$
]

---
###M-step
For the M-step we need to maximize the $Q$-function with respect to $\mu$ and $\sigma^2$. This can be done analytically:
.pull-left[
$$\frac{\partial }{\partial\mu}Q(\theta|\theta')=-\sum_{i=0}^n\frac{(x_i-\mu)}{\nu\sigma^2}kt_i\\
\Rightarrow\hat{\mu}_{opt}=\frac{\sum_{i=1}^nx_it_i}{\sum_{i=1}^nt_i}$$
]

.pull-right[
$$\frac{\partial }{\partial\sigma}Q(\theta|\theta')=\frac{n}{\sigma}-3\sum_{i=1}^n\frac{(x_i-\mu)^2}{\nu\sigma^2}kt_i \\
\Rightarrow \hat{\sigma}^2_{opt}=\frac{1}{n\nu}k\sum_{i=1}^n(x_i-\mu)^2t_i$$
]


We note that this means we can actually optimize $Q$ wrt. $\mu$ and $\sigma^2$ without explicitly calculating $Q$.


---
###Implementation
```{r}
EM_alg <- function(x, param, max_iter = 20, epsilon = 1e-10, cb = NULL){
  mu_mark <- param$mu #Getting initial parameters
  sigma_mark <- param$sigma
  ny <- param$ny
  k <- (ny + 1) / 2   #Defining i and k, which only depends on ny:
  i <- 0
  
  while (i < max_iter) {
    mu_old <- mu_mark
    sigma_old <- sigma_mark
    t_old <- 2 / (1 + (x - mu_old)^2 / (ny * sigma_old))
    
    mu_mark <- sum(t_old * x) / sum(t_old)
    sigma_mark <- 1 / (length(x) * ny) * k * sum(t_old * (x - mu_mark)^2)
    #Calling  cb
    if(!is.null(cb)) cb()
    #Checking convergence
    if (sum((c(mu_mark, sigma_mark) - c(mu_old, sigma_old))^2) 
        <= epsilon * (sum(c(mu_mark, sigma_mark)^2) + epsilon)) break
    i <- i + 1
  }
  c(mu_mark, sigma_mark)
}
```



