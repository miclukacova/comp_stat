---
title: "The Expectation-Maximization Algorithm"
author: "Dina Gyberg Jensen (vbz248)"
institute: "University of Copenhagen"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: themer-new.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      ratio: 16:9
      countIncrementalSlides: true
      navigation:
        scroll: false
---
<style type="text/css">
.remark-slide-content {
    font-size: 18px;
        padding: 1em 4em 1em 4em;
    }
.remark-slide-content > h1 {
  font-size: 40px;
}
.remark-slide-scaler {
    overflow-y: auto;
    overflow-x: auto;
}
</style>
```{r, include=FALSE, eval = FALSE}
rmarkdown::render('presentation.rmd')
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(CSwR)
style_mono_accent(
 base_color = "#4f7942")
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(microbenchmark)
library(testthat)
library(bench)
library(tidyverse)
library(profvis)
library(bench)
library(knitr)
library(ggplot2)
library(gridExtra)
library(CSwR)
library(parallel)
library(foreach)
library(doParallel)
library(Rcpp)
library(RcppArmadillo)
knitr::opts_chunk$set(fig.retina = 2)
theme_set(
  theme_bw(base_size = 18) +  # Set base font size and family
    theme(
      text = element_text(size = 15),           # Adjust text size
      axis.line = element_line(size = 4),       # Set axis line thickness (use `size` not `linewidth`)
      panel.grid = element_line(size = 0.5)     # Set grid line thickness
    )
)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Themes and colors:
my_color_scale <- scale_color_manual(
   name = "Density estimator",
  values = c("R_density" = "coral4", "bin_kern_dens" = "#4f7942", "AMISE" = "hotpink4", "CV" = "#4f7942")
)

my_theme <-   theme(
    text = element_text(size = 16),  # Change the base text size
    plot.title = element_text(size = 18),  # Title size
    axis.title = element_text(size = 16),  # Axis titles size
    axis.text = element_text(size = 14),  # Axis text size
    legend.title = element_text(size = 18),  # Legend title size
    legend.text = element_text(size = 16),  # Legend text size
     line = element_line(linewidth = 4) +
      theme_bw()
  )
```

###Introduction
Let $Y=(X,W) \in \mathbb{R}\times [0,\infty)$ have density:
$$f(x,w)=\frac{1}{\sqrt{\pi\nu\sigma^2}2^{(\nu+1)/2}\Gamma(\nu/2)}w^{\frac{\nu-1}{2}}e^{\frac{w}{2}\left( \frac{(x-\mu)^2}{\nu\sigma^2} \right)} $$
Our goal is to implement an Expectation Maximization algorithm to estimate $\mu$ and $\sigma^2$ given that we observe $X$ but not $W$ and to compute the observed Fisher information.
<br>
Furthermore we wish to compare our implementation to a standard Gradient Descent algorithm.
---
###Marginal Density of X

$Y = (X,W)\in\mathbb{R}\times(0,\infty)$ has joint density:
$$f(x,w)=\frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)} $$


The marginal density of $X$ is:
$$f(x)=\int_0^\infty f(x,w)dw=\frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)} \int_0^\infty w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)} dw\\
=\frac{1}{\sqrt{\pi \nu \sigma}}\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2}\right)^{-\frac{\nu+1}{2}}$$
Where we have used that the gamma density is given by $\Gamma(z)=\int_0^\infty t^{z-1}e^{-t}dt$ and used the substitution $t = \frac{w}{2}\left( 1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)$. The result can be easily recognized as a Student's t-distribution with $\nu$ degrees of freedom and location $\mu$ and scale $\sigma$.

---
###Maximizing Complete Data log-likelihood
Complete data log-likelihood for fixed $\nu$:
$$\sum_{i=1}^n\log f(x_i,w_i)=-\sum_{i=1}^n\log\left(\sigma\sqrt{\pi\nu}2^{(\nu+1)/2}\Gamma(\nu/2) \right)+\sum_{i=1}^n\log\left(w_i^{\frac{\nu-1}{2}} \right)-\sum_{i=1}^n\left(\frac{w_i}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2} \right) \right)$$
We differentiate and set equal to 0 to find the MLEs of $\mu$ and $\sigma^2$ given that we observe the full dataset.
.pull-left[ 
$$\frac{\partial }{\partial\mu}=\frac{1}{\nu\sigma^2}\sum_{i=1}^nw_i(x_i-\mu)$$
]

.pull-right[ 
$$\frac{\partial }{\partial\sigma}=-\frac{n}{\sigma}+\sum_{i=1}^n w_i\frac{(x_i-\mu)^2}{\nu\sigma^3}$$
 ]
.pull-left[ 
$$\mu_{opt}=\frac{\sum_{i=1}^nw_ix_i}{\sum_{i=1}^nw_i}$$
]

.pull-right[ 
$$\sigma^2_{opt} =\frac{1}{n\nu}\sum_{i=1}^nw_i(x_i-\mu)^2$$
 ]

---
###E-step
For the E-step we need to compute the $Q$-function
$$Q(\theta|\theta')=E_{\theta'}\left(\log( f(X,W \ | \ \theta) \ | \ X=x) \right)= \sum_{i=1}^nE_{\theta'}(\log( f(x_i,W_i \ | \ \theta) \ | \ X_i=x_i) )\\
=\sum_{i=1}^nE_{\theta'}\left( \log\left( \frac{1}{\sqrt{\pi \sigma \nu}2^{(\nu+1)/2}\Gamma(\nu/2)}w^{\frac{\nu-1}{2}}e^{-\frac{w}{2}\left(1+\frac{(x-\mu)^2}{\nu\sigma^2} \right)}\right) \right)\\
=-\sum_{i=1}^n\log\left(\sigma\sqrt{\pi\nu}2^{(\nu+1)/2}\Gamma(\nu/2) \right)+\frac{\nu-1}{2}\sum_{i=1}^nE_{\theta'}( \log(W_i\ | \ X_i=x_i))-\frac{1}{2}\left(1+\frac{(x_i-\mu)^2}{\nu\sigma^2} \right)\sum_{i=1}^n E_{\theta'}(W_i \ | \ X_i=x_i)$$

The conditional distribution of $W_i \ | X_i=x_i$ can be recognized as the gamma distribution with shape $k = \frac{\nu+1}{2}$ and scale $t_i = \frac{2}{1+\frac{(x_i-\mu')^2}{\nu\sigma'^2} }$. Hence we have:
.pull-left[
$$
E_{\theta'}(W_i \ | \ X_i=x_i)=k\cdot t_i
$$
]
.pull-right[
$$
E_{\theta'}( \log(W_i\ | \ X_i=x_i))=\psi(k)+\log(t_i)
$$
]

---
###M-step
For the M-step we need to maximize the $Q$-function with respect to $\mu$ and $\sigma^2$. This can be done analytically:
.pull-left[
$$\frac{\partial }{\partial\mu}Q(\theta|\theta')=-\sum_{i=1}^n\frac{(x_i-\mu)}{\nu\sigma^2}kt_i\\
\Rightarrow\hat{\mu}_{opt}=\frac{\sum_{i=1}^nx_it_i}{\sum_{i=1}^nt_i}$$
]

.pull-right[
$$\frac{\partial }{\partial\sigma}Q(\theta|\theta')=-\frac{n}{\sigma}+\sum_{i=1}^n\frac{(x_i-\mu)^2}{\nu\sigma^3}kt_i \\
\Rightarrow \hat{\sigma}^2_{opt}=\frac{1}{n\nu}k\sum_{i=1}^n(x_i-\mu)^2t_i$$
]


We note that this means we can actually optimize $Q$ wrt. $\mu$ and $\sigma^2$ without explicitly calculating $Q$.


---
class: reduce-spacing

###Implementation
```{r}
EM_alg <- function(x, param, ny , max_iter = 20, epsilon = 1e-10, cb = NULL){
  #Getting initial parameters
  mu_mark <- param$mu 
  sigma_mark <- param$sigma
  #Defining i and k, which only depends on ny:
  ny <- ny
  k <- (ny + 1) / 2   
  i <- 0
  
  while (i < max_iter) {
    mu_old <- mu_mark
    sigma_old <- sigma_mark
    t_old <- 2 / (1 + (x - mu_old)^2 / (ny * sigma_old))
    
    mu_mark <- sum(t_old * x) / sum(t_old)
    sigma_mark <- 1 / (length(x) * ny) * k * sum(t_old * (x - mu_mark)^2)
    #Calling  cb
    if(!is.null(cb)) cb()
    #Checking convergence
    if (sum((c(mu_mark, sigma_mark) - c(mu_old, sigma_old))^2) 
        <= epsilon * (sum(c(mu_mark, sigma_mark)^2) + epsilon)) break
    i <- i + 1
  }
  c(mu_mark, sigma_mark)
}
```

---
###Implementation
```{r, echo=FALSE}
source("~/comp_stat/Dina/Assignment 3 - EM Algorithm/SO_source.R", local = FALSE)
set.seed(123)
par1 <- c(1,1,1)
sim1 <- sim(parameters(1,1,1), 1000)
```

S3 object for both simulation and the EM algorithm

- `parameters`-class for $\mu$, $\sigma^2$ and $\nu$ - and a `sim` function that simulates $W$ from a $\mathcal{X}_\nu^2$-distribution and then simulates $X$ from a normal distribution with mean $\mu$ and variance $\frac{\nu\sigma^2}{w}$ for $W =w$.

- `EM`-class for the EM algorithm, with corresponding `summary`, `print` and a `plot`-method. The `plot`-method can both plot the loglikelihood-convergence but also the suboptimality to the true MLE-estimates. We have the true MLE estimates as we simulate data ourselves.

- Stopping criterion is $||\theta' - \theta||^2\leq \varepsilon||\theta||^2 + \varepsilon$

```{r}
EM(sim1$x, parameters(1, 2, 3), 20, 1e-10, par_true = parameters(1, 1, 1))
```


---
class: reduce-spacing

###Testing
Testing stability. Simulating 4 different datasets, runnning for different starting points and compare to true MLE.
```{r, echo = FALSE}
set.seed(123)
par1 <- c(1,1,1)
sim1 <- sim(parameters(1,1,1), 1000)
par2 <- c(5,2,1)
sim2 <- sim(parameters(5, 2, 1), 1000)
par3 <- c(rexp(1),rexp(1),rexp(1))
sim3 <- sim(parameters(par3[1], par3[2], par3[3]), 1000)
par4 <- c(rexp(1,0.1),rexp(1,0.1),rexp(1,0.1))
sim4 <- sim(parameters(par4[1], par4[2], par4[3]), 1000)

start_values1 <- c(-2,5,1)
start_values2 <- c(rexp(1), rexp(1), rexp(1))

EM1_p1 <- EM_alg(sim1$x, parameters(start_values1[1], start_values1[2], start_values1[3]), ny = par1[3], max_iter = 1000, epsilon = 1e-10)
EM2_p1 <- EM_alg(sim2$x, parameters(start_values1[1], start_values1[2], start_values1[3]), ny = par2[3], max_iter = 1000, epsilon = 1e-10)
EM3_p1 <- EM_alg(sim3$x, parameters(start_values1[1], start_values1[2], start_values1[3]), ny = par3[3], max_iter = 1000, epsilon = 1e-10)
EM4_p1 <- EM_alg(sim4$x, parameters(start_values1[1], start_values1[2], start_values1[3]), ny = par4[3], max_iter = 1000, epsilon = 1e-10)
EM1_p2 <- EM_alg(sim1$x, parameters(start_values2[1], start_values2[2], start_values2[3]), ny = par1[3], max_iter = 1000, epsilon = 1e-10)
EM2_p2 <- EM_alg(sim2$x, parameters(start_values2[1], start_values2[2], start_values2[3]), ny = par2[3], max_iter = 1000, epsilon = 1e-10)
EM3_p2 <- EM_alg(sim3$x, parameters(start_values2[1], start_values2[2], start_values2[3]), ny = par3[3], max_iter = 1000, epsilon = 1e-10)
EM4_p2 <- EM_alg(sim4$x, parameters(start_values2[1], start_values2[2], start_values2[3]), ny = par4[3], max_iter = 1000, epsilon = 1e-10)
EM1_true <- EM_alg(sim1$x, parameters(par1[1], par1[2], par1[3]), ny = par1[3], max_iter = 1000, epsilon = 1e-10)
EM2_true <- EM_alg(sim2$x, parameters(par2[1], par2[2], par2[3]), ny = par2[3], max_iter = 1000, epsilon = 1e-10)
EM3_true <- EM_alg(sim3$x, parameters(par3[1], par3[2], par3[3]), ny = par3[3], max_iter = 1000, epsilon = 1e-10)
EM4_true <- EM_alg(sim4$x, parameters(par4[1], par4[2], par4[3]), ny = par4[3], max_iter = 1000, epsilon = 1e-10)

true1 <- theo_par(sim1$x, sim1$w, 1)
true2 <- theo_par(sim2$x, sim2$w, 1)
true3 <- theo_par(sim3$x, sim3$w, par3[3])
true4 <- theo_par(sim4$x, sim4$w, par4[3])

library(knitr)

# Function to create a table for each parameter set
create_table <- function(mu_values, sigma_values, title) {
  # Create data frame with parameter names and their corresponding values
  df <- data.frame(
    Starting_point = c("c(-2, 5)", "c(0.64, 0.247)", "Started in  true", "True"),
    mu = round(c(mu_values), 2),
    sigma = round(c(sigma_values), 2)
  )
}

# Create tables
table1 <- create_table(c(EM1_p1[1], EM1_p2[1], EM1_true[1], true1$mu[1]), c(EM1_p1[2], EM1_p2[2], EM1_true[2], true1$sigma[1]), "mu = 1, sigma = 1, nu = 1")

table2 <- create_table(c(EM2_p1[1], EM2_p2[1], EM2_true[1], true2$mu[1]), c(EM2_p1[2], EM2_p2[2], EM2_true[2], true2$sigma[1]), "mu = 1, sigma = 2, nu = 1")

table3 <- create_table(c(EM3_p1[1], EM3_p2[1], EM3_true[1], true3$mu[1]), c(EM3_p1[2], EM3_p2[2], EM3_true[2], true3$sigma[1]), "mu = 0.59, sigma = 0.36, nu = 5.79")

table4 <- create_table(c(EM4_p1[1], EM4_p2[1], EM4_true[1], true4$mu[1]), c(EM4_p1[2], EM4_p2[2], EM4_true[2], true4$sigma[1]), "mu = 6.46, sigma = 2.47, nu = 1.75")


tab1 <- knitr::kable(table1, caption = "Simulated data with c(1,1,1)")
tab2 <- knitr::kable(table2, caption = "Simulated data with c(5,2,1)")
tab3 <- knitr::kable(table3, caption = "Random parameters c(0.59, 0.36, 5.79)")
tab4 <- knitr::kable(table4, caption = "Random parameters c(6.46, 2.47, 1.75)")
```

.pull-left[
```{r,echo = FALSE}
tab1
tab2
```
]


.pull-right[
```{r,echo = FALSE}
tab3
tab4 
```
]


---
###Convergence
Looking at the convergence of the absolute distance to the true parameters:
<br>
<br>
```{r, echo=FALSE, warning = FALSE, fig.align='center', fig.width= 16, fig.height= 5}
EM1 <- EM(sim1$x, parameters(-2,5,1), 20, 1e-10, par_true = parameters(1, 1,1))
p1 <- plot(EM1, 1); p2 <- plot(EM1, 4)
grid.arrange(p1, p2, ncol = 2)
```


```{r, warning = FALSE, fig.align='center', fig.width= 16, fig.height= 5}
log_lm <- lm(log(par_norm_diff) ~ i, data = EM1$trace)
exp(coefficients(log_lm)["i"])
```


---
###Heatmaps
We now want to visualize the convergence further.
```{r, warning=FALSE, fig.align='center', fig.width= 16, fig.height= 6, message = FALSE}
p1 <- heatmap.My_EM(EM1, theo_par(sim1$x, sim1$w, 1), path = T) + ylim(0,5.5)
p2 <- heatmap.My_EM(EM1, theo_par(sim1$x, sim1$w, 1), gradient = T)
grid.arrange(p1, p2, ncol = 2)
```

---
###Convergence
Looking at the convergence of the negative loglikelihood and of the parameters:
<br>
<br>
```{r, echo=FALSE, warning=FALSE, fig.align='center', fig.width= 16, fig.height= 6}
p1 <- plot(EM1, 2) #S3-call
p2 <- plot(EM1, plot_no = 3, mle_est = theo_par(sim1$x, sim1$w, 1)) 
grid.arrange(p1, p2, ncol = 2)
```

---
### Gradiant Descent

As loss function we will use the negative loglikelihood divided by $\frac{1}{n}$

$$-\frac{1}{n}\mathcal{l}(\theta) = \frac{1}{n}\sum_{i=1}^n \log(\sigma) + \frac{\nu +1 }{2} \log \left( 1 + \frac{(x_i - \mu)^2}{\nu \sigma^2} \right)$$
The gradient is

$$\nabla - \mathcal{l}(\theta) =  \begin{pmatrix} \sum_{i=1}^n  \left(\nu +1 \right) \frac{(x_i - \mu)}{(x_i - \mu)^2 + \nu \sigma^2}  \\ \sum_{i=1}^n \frac{1}{\sigma} -  \frac{(\nu +1)}{  \nu \sigma^2 + (x_i - \mu)^2} \frac{(x_i - \mu)^2}{\sigma} \end{pmatrix}$$
 
We implement the gradient in the function `grad` and check that the gradient is close to $0$ in the full data MLE estimates.

```{r, message=FALSE, echo = FALSE, warning=FALSE, fig.width=10, fig.height=4, fig.align='center'}
grad_negloglik <- function(mu, sigma, nu, x){
  n <- length(x)
  d_mu <- - (nu + 1) * sum((x - mu) / (nu * sigma^2 + (x - mu)^2))
  d_sigma <- n/sigma - (nu + 1) / sigma * sum((x - mu)^2 / (nu * sigma^2 + (x - mu)^2))
  return(c(d_mu, d_sigma))
}
mle_est <- theo_par(sim1$x, sim1$w, 1)
grad_negloglik(mu = mle_est$mu, sigma = sqrt(mle_est$sigma), nu = 1, x = sim1$x) * 1 / length(sim1$x)
```

---
###Gradient Descent
Now implementing Gradient Descent to compare it to our EM. Wrapped in an S3-function similar to that of the EM. Initial stepsize $1$ and topping criterion is also $||\theta' - \theta||^2\leq \varepsilon||\theta||^2 + \varepsilon$.

We first look at the convergence of GD for data simulated from $parameters = (1,1,1)$ and with starting points $(-2,5)$ - same as for EM.  
<br>
```{r, echo=FALSE}
source("~/comp_stat/Dina/Assignment 3 - EM Algorithm/GD.R", local = FALSE)
```

```{r, echo = FALSE, fig.width = 16, fig.height = 5}
mle_est1 <- c(theo_par(sim1$x, sim1$w, 1)$mu, theo_par(sim1$x, sim1$w, 1)$sigma)
GD1 <- GD(par = c(-2, 5), x = sim1$x, nu = 1, epsilon = 1e-10)
p1 <- plot(GD1, 1) + my_theme
p2 <- plot(GD1, 2) + my_theme
p3 <- plot(GD1, 3, mle_est1) + my_theme
grid.arrange(p1, p2, p3, ncol = 3)
```

---
###Comparison
Looking at the convergence of GD compared to the EM.
<br>
<br>
```{r, echo=FALSE, fig.width = 16, fig.height = 6}
#Plotting EM and GD in the same plot
p1 <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = log(par_norm_diff), color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = log(par_norm_diff)), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = log(par_norm_diff), color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = log(par_norm_diff)), color = "coral4", size = 2.5) +
  my_theme +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4")) +
        labs(
        x = "Time",
        y = expression(log(bgroup("||", theta^{(n-1)} - theta^n, "||")))
        )

log_lik_new <- function(par, ny, x = samp$x){
  m <- par[[1]]
  s <- par[[2]]
  k <- (ny + 1) / 2
  return( - length(x) * log(sqrt(s)) - sum(k * log(1 + (x - m)^2 / (ny * s))))
}
log_lik1 <- log_lik(mle_est1[1], mle_est1[2], 1, sim1$x)
GD1$trace$abs_dist_from_par <- apply(GD1$trace[,1:2], 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
EM1$trace$abs_dist_from_par <- apply(EM1$trace[,1:2] , 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)

p2 <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = abs_dist_from_par, color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = abs_dist_from_par), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = abs_dist_from_par, color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = abs_dist_from_par), color = "coral4", size = 2.5) +
  my_theme +
  labs(
    x = "Time",
    y = "Suboptimality (ll) - logscale"
  ) +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4")) +
  scale_y_log10()

legend_plot <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = log(par_norm_diff), color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = log(par_norm_diff)), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = log(par_norm_diff), color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = log(par_norm_diff)), color = "coral4", size = 2.5) +
  my_theme +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4")) +
  labs(
    x = "Time",
    y = expression(log(bgroup("||", theta^{(n-1)} - theta^n, "||")))
  )
grid.arrange(p1, p2, ncol = 2)
```
---
#Optimizing GD
As GD is not competitive, we seek to improve it by giving it Polyak momentum. Now the update step is:
$$x_k = x_{k-1}-\gamma\nabla f(x_{k-1}) + \mu(x_{k-1}-x_{k-2}) \quad \text{for} \quad \mu\in[0,1), x_{-1}=x_0$$
```{r, echo=FALSE, warning=FALSE, fig.width=16, fig.height=6}
#Plotting EM, GD and optimized GD in the same plot
source("~/comp_stat/Dina/Assignment 3 - EM Algorithm/EM_mom.R", local = FALSE)
mle_est1 <- c(theo_par(sim1$x, sim1$w, 1)$mu, theo_par(sim1$x, sim1$w, 1)$sigma)
GD_mom <- GD_m(par = c(-2, 5), x = sim1$x, nu = 1, mu = 0.9)

p1 <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = log(par_norm_diff), color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = log(par_norm_diff)), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = log(par_norm_diff), color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = log(par_norm_diff)), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom$trace, aes(x = .time, y = log(par_norm_diff), color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom$trace, aes(x = .time, y = log(par_norm_diff)), color = "steelblue4", size = 2.5) +
  my_theme +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
        labs(
        x = "Time",
        y = expression(log(bgroup("||", theta^{(n-1)} - theta^n, "||")))
        ) + theme(legend.position = "none")

GD1$trace$abs_dist_from_par <- apply(GD1$trace[,1:2], 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
EM1$trace$abs_dist_from_par <- apply(EM1$trace[,1:2] , 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
GD_mom$trace$abs_dist_from_par <- apply(GD_mom$trace[,1:2], 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)

p2 <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = abs_dist_from_par, color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = abs_dist_from_par), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = abs_dist_from_par, color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = abs_dist_from_par), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom$trace, aes(x = .time, y = abs_dist_from_par, color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom$trace, aes(x = .time, y = abs_dist_from_par), color = "steelblue4", size = 2.5) +
  my_theme +
  labs(
    x = "Time",
    y = "Suboptimality - logscale"
  ) +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
  scale_y_log10() + theme(legend.position = "none")

legend_plot <- ggplot() +
  geom_line(data = GD1$trace, aes(x = .time, y = log(par_norm_diff), color = "GD"), size = 1.2) +
  geom_point(data = GD1$trace, aes(x = .time, y = log(par_norm_diff)), color = base, size = 2.5) +
  geom_line(data = EM1$trace, aes(x = .time, y = log(par_norm_diff), color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = log(par_norm_diff)), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom$trace, aes(x = .time, y = log(par_norm_diff), color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom$trace, aes(x = .time, y = log(par_norm_diff)), color = "steelblue4", size = 2.5) +
  my_theme +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
  labs(
    x = "Time",
    y = expression(log(bgroup("||", theta^{(n-1)} - theta^n, "||")))
  )

get_l <- cowplot::get_legend(legend_plot)

p3 <- ggplot() +
  geom_line(data = EM1$trace, aes(x = .time, y = abs_dist_from_par, color = "EM"), size = 1.2) +
  geom_point(data = EM1$trace, aes(x = .time, y = abs_dist_from_par), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom$trace, aes(x = .time, y = abs_dist_from_par, color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom$trace, aes(x = .time, y = abs_dist_from_par), color = "steelblue4", size = 2.5) +
  my_theme +
  labs(
    x = "Time",
    y = "Suboptimality - logscale"
  ) +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
  scale_y_log10()



grid.arrange(p1, p2, get_l, p3, ncol = 4, widths = c(2,2,1,3))
```

---
###Comparison for larger datasets
Increasing $n$, to see how that impacts performance of the two.
```{r, echo=FALSE, warning=FALSE, fig.width = 16, fig.height = 6}
set.seed(123)
sim1_large <- sim(parameters(1,1,1), 10000)
sim1_larger <- sim(parameters(1,1,1), 100000)

mle_est1 <- c(theo_par(sim1$x, sim1$w, 1)$mu, theo_par(sim1$x, sim1$w, 1)$sigma)
GD_mom_l <- GD_m(par = c(-2, 5), x = sim1_large$x, nu = 1, mu = 0.9, epsilon = 1e-10)
GD_mom_la <- GD_m(par = c(-2, 5), x = sim1_larger$x, nu = 1, mu = 0.9, epsilon = 1e-10)
EM1_l <- EM(sim1_large$x, parameters(-2,5,1), 20, 1e-10, par_true = parameters(1, 1,1))
EM1_la <- EM(sim1_larger$x, parameters(-2,5,1), 20, 1e-10, par_true = parameters(1, 1,1))

EM1_l$trace$abs_dist_from_par <- apply(EM1_l$trace[,1:2] , 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
GD_mom_l$trace$abs_dist_from_par <- apply(GD_mom_l$trace[,1:2], 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
EM1_la$trace$abs_dist_from_par <- apply(EM1_la$trace[,1:2] , 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)
GD_mom_la$trace$abs_dist_from_par <- apply(GD_mom_la$trace[,1:2], 1, FUN = function(par_est) sum(
  abs(log_lik1 - log_lik_new(par_est, ny = par1[3], x = sim1$x)))
)

p1 <- ggplot() +
  geom_line(data = EM1_l$trace, aes(x = .time, y = abs_dist_from_par, color = "EM"), size = 1.2) +
  geom_point(data = EM1_l$trace, aes(x = .time, y = abs_dist_from_par), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom_l$trace, aes(x = .time, y = abs_dist_from_par, color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom_l$trace, aes(x = .time, y = abs_dist_from_par), color = "steelblue4", size = 2.5) +
  my_theme +
  labs(title = "n = 10.000",
    x = "Time",
    y = "Suboptimality - logscale"
  ) +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
  scale_y_log10()

p2 <- ggplot() +
  geom_line(data = EM1_la$trace, aes(x = .time, y = abs_dist_from_par, color = "EM"), size = 1.2) +
  geom_point(data = EM1_la$trace, aes(x = .time, y = abs_dist_from_par), color = "coral4", size = 2.5) +
  geom_line(data = GD_mom_la$trace, aes(x = .time, y = abs_dist_from_par, color = "Momentum"), size = 1.2) +
  geom_point(data = GD_mom_la$trace, aes(x = .time, y = abs_dist_from_par), color = "steelblue4", size = 2.5) +
  my_theme +
  labs(title = "n = 100.000",
    x = "Time",
    y = "Suboptimality - logscale"
  ) +
  scale_color_manual(values = c("GD" = base, "EM" = "coral4", "Momentum" = "steelblue4")) +
  scale_y_log10()

grid.arrange(p1, p2, ncol = 2)
```


---
###Fischer Information
The empirical Fischer information matrix can be calculated as the empirical variance of the gradient of our $Q$ function. For a single observation we have
$$\nabla_\theta Q_i(\theta'|\theta')=\nabla_\theta \ell_i(\theta')$$
Where $\ell_i(\theta')$ is the loglikelihood for a single observation. By using the second Bartlett identity we have that the empirical Fischer information matrix can be calculated as
$$\mathcal{I}(\hat{\theta}) = \sum_{i=1}^n\left(\nabla_\theta Q_i(\hat{\theta}|\hat{\theta})- N^{-1}\nabla_\theta \ell(\hat{\theta}) \right)\left(\nabla_\theta Q_i(\hat{\theta}|\hat{\theta})- N^{-1}\nabla_\theta \ell(\hat{\theta}) \right)^T$$
We will calculate this both for $\nabla_\theta l(\hat{\theta})=0$ and without. To see if the fact that $\nabla_\theta l(\hat{\theta})$ is only approximately zero has any effect. We have already calculated the gradient of $Q$ wrt. $\mu$ and $\sigma^2$ for the M-step of our EM.


---
###Fischer information

We will now implement it and compare it to the numerical derivative obtained using `numDeriv`. We will calculate it in the optimal parameters from our EM-algorithm.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(numDeriv)
#Med sigma^2 som input!!!!
grad_sing_point <- function(par, ny, x){
  m <- par[1]
  s <- par[2]
  k <- (ny + 1) / 2
  
  # Using consistent variance treatment and k factor
  par_m <- - (k * 2 * (x - m)) / (ny * s + (x - m)^2)
  par_s <- -1 / (2 * s) + (k * (x - m)^2) / (ny * s^2 + (x - m)^2)
  return(c(par_m, par_s))
}


#Michaelas gradient
grad <- function(par, nu, x) {
  m <- par[1]
  s <- par[2]
  n <- length(x)
  k <- (nu + 1) / 2
  
  # Partial derivative with respect to m
  partial_m <- -sum((k * 2 * (x - m)) / (nu * s * (1 + (x - m)^2 / (nu * s))))
  
  # Partial derivative with respect to s
  partial_s <- -n / (2 * s) + sum(k * (x - m)^2 / (nu * s^2 * (1 + (x - m)^2 / (nu * s))))
  
  return(c(partial_m, partial_s))
}


#Centralized version - we need the gradient of the loglikelihood
grad_negloglik_old <- function(par, nu, data){
  mu <- par[1]
  sigma <- par[2]
  w <- 2 / (1 + (data - mu)^2 / (nu * sigma^2))
  grad_mu <- -sum(w * (data - mu)) / (nu * sigma^2)
  grad_sigma <- -length(data) / sigma + sum(w * (data - mu)^2) / (nu * sigma^2)
  return(c(grad_mu, grad_sigma))
}

grad_negloglik <- function(par, nu, data){
  mu <- par[1]
  sigma <- par[2]
  n <- length(data)
  d_mu <- - (nu + 1) * sum((data - mu) / (nu * sigma^2 + (data - mu)^2))
  d_sigma <- n/sigma - (nu + 1) / sigma * sum((data - mu)^2 / (nu * sigma^2 + (data - mu)^2))
  return(c(d_mu, d_sigma))
}

grad_loglik <- function(par, nu, data){
  mu <- par[1]
  sigma <- par[2]
  n <- length(data)
  d_mu <- (nu + 1) * sum((data - mu) / (nu * sigma^2 + (data - mu)^2))
  d_sigma <- - n/sigma + (nu + 1) / sigma * sum((data - mu)^2 / (nu * sigma^2 + (data - mu)^2))
  return(c(d_mu, d_sigma))
}


fisher_information_naive_cent <- function(gradient, par, nu, data){
  n <- length(data)
  mu <- par[1]
  sigma <- par[2]

  grad <- -gradient(par = par, nu = nu, data = data)/n
  
  fisher_information <- matrix(0, nrow = 2, ncol = 2)
  
  for (i in seq_along(data)){
    
    fisher_i <- gradient(par, 
                      nu = nu,
                      data = data[i])
    
    fisher_information <- fisher_information + (fisher_i - grad) %*% t(fisher_i - grad)
  }
  
  return(fisher_information)
}

fisher_information_naive <- function(gradient, par, nu, data){
  n <- length(data)
  mu <- par[1]
  sigma <- par[2]
  
  fisher_information <- matrix(0, nrow = 2, ncol = 2)
  
  for (i in seq_along(data)){
    
    fisher_i <- gradient(par, 
                      nu = nu,
                      data = data[i])
    
    fisher_information <- fisher_information + (fisher_i) %*% t(fisher_i)
  }
  
  return(fisher_information)
}
```


```{r, results='hide', message=FALSE, warning=FALSE}
mle_EM <- EM1$est
fisher_information_naive_cent(grad_loglik, c(mle_EM[1], mle_EM[2]), 1, sim1$x)
fisher_information_naive(grad_loglik, c(mle_EM[1], mle_EM[2]), 1, sim1$x)
-numDeriv::jacobian(grad_loglik, x = c(mle_EM[1], mle_EM[2]), nu = 1 , data = sim1$x)
```


.pull-left[
```{r, echo=FALSE}
print('Naive Fisherinformation')
fisher_information_naive(grad_loglik, c(mle_EM[1], mle_EM[2]), 1, sim1$x)
print('Centered Fisherinformation')
fisher_information_naive_cent(grad_loglik, c(mle_EM[1], mle_EM[2]), 1, sim1$x)
```
]
.pull-right[
```{r, echo = F}
print('numDeriv')
-numDeriv::jacobian(grad_loglik, x = c(mle_EM[1], mle_EM[2]), nu = 1 , data = sim1$x)
```
]

---
###Optimizing the EM Algorithm
Even though our EM is fast - we want to see if we can optimize it further. 
```{r, echo = FALSE}
sim1_large <- sim(parameters(1,1,1), 1000000)
profvis({
  EM_alg <- function(x, param, ny , max_iter = 20, epsilon = 1e-10, cb = NULL){
  mu_mark <- param$mu #Getting initial parameters
  sigma_mark <- param$sigma
  ny <- ny
  k <- (ny + 1) / 2   #Defining i and k, which only depends on ny:
  i <- 0
  
  while (i < max_iter) {
    mu_old <- mu_mark
    sigma_old <- sigma_mark
    t_old <- 2 / (1 + (x - mu_old)^2 / (ny * sigma_old))
    
    mu_mark <- sum(t_old * x) / sum(t_old)
    sigma_mark <- 1 / (length(x) * ny) * k * sum(t_old * (x - mu_mark)^2)
    #Calling  cb
    if(!is.null(cb)) cb()
    #Checking convergence
    if (sum((c(mu_mark, sigma_mark) - c(mu_old, sigma_old))^2) 
        <= epsilon * (sum(c(mu_mark, sigma_mark)^2) + epsilon)) break
    i <- i + 1
  }
  c(mu_mark, sigma_mark)
}
  EM_alg(sim1_large$x, parameters(-2,5,1), ny = 1, 100, 1e-10)
})
```

---
###Optimizing the EM Algorithm
Implementing in `Rcpp` and in `RcppArmadillo` to see if it is possible to optimize.

```{r, echo=FALSE, warning=FALSE, message=FALSE}
sourceCpp("~/comp_stat/Dina/Assignment 3 - EM Algorithm/EM_cpp.cpp")
sourceCpp("~/comp_stat/Dina/Assignment 3 - EM Algorithm/EM_arma.cpp")
```


```{r, echo=FALSE, fig.align='center', fig.width = 16, fig.height = 6, warning=FALSE, message=FALSE}
#Another simple one just using bench::mark
bm_s <- bench::mark(
  Original = EM_alg(sim1$x, parameters(-2,5,1), ny = 1, 100, 1e-10),
  Rcpp = EM_alg_cpp(sim1$x, parameters(-2,5,1), ny = 1, 100, 1e-10),
  Armadillo = EM_alg_arma(sim1$x, parameters(-2,5,1), ny = 1, 100, 1e-10)
  , iterations = 50
  ,check = FALSE
)

bm_l <- bench::mark(
  Original = EM_alg(sim1_large$x, parameters(-2,5,1), ny = 1, 100, 1e-10),
  Rcpp = EM_alg_cpp(sim1_large$x, parameters(-2,5,1), ny = 1, 100, 1e-10),
  Armadillo = EM_alg_arma(sim1_large$x, parameters(-2,5,1), ny = 1, 100, 1e-10)
  , iterations = 50
  ,check = FALSE
)

p1 <- plot(bm_l) + scale_color_manual(values = c("none" = "#4f7942", "level0" = "coral2", 
                                "level1" = "steelblue", "level2" = "hotpink3")) +
  my_theme + ggtitle("n = 1e6") + theme(legend.position = "none")

p3 <- plot(bm_l) + scale_color_manual(values = c("none" = "#4f7942", "level0" = "coral2", 
                                "level1" = "steelblue", "level2" = "hotpink3")) +
  my_theme + ggtitle("n = 1000")

legend_l <- cowplot::get_legend(p3)

p2 <- plot(bm_s) + scale_color_manual(values = c("none" = "#4f7942", "level0" = "coral2", 
                                "level1" = "steelblue", "level2" = "hotpink3")) +
  my_theme + ggtitle("n = 1000") + theme(legend.position = "none")

grid.arrange(p2, p1, legend_l, ncol = 3, widths = c(2,2,1))
```



